

@article{Coughlan-et-al-JABS-2023,
author = {Paul Coughlan and Roberta Bellini and Aisha Bello-Dambatta and Richard Dallison and Katrin Dreyer-Gibney and John Gallagher and Ian Harris and Aonghus McNabola and Djordje Mitrovic and Madhu Murali and Daniele Novara and Sopan Patil and Alex Rigby and Panagiotis Ritsos and Isabel Schestak and Ajeet Singh and Nathan Walker and Prysor Williams},
title ={Researching Green Process Innovation Across Borders and Boundaries Through Collaborative Inquiry},
journal = {The Journal of Applied Behavioral Science},
month=aug,
year = {2023},
doi = {10.1177/00218863231194655},
volume = {59},
number = {4},
pages = {556--584},
abstract = {Research involving multistakeholder collaborative partnerships is growing, as both academia and funding agencies align their objectives with societal challenges and undertake research in the context of application. In particular, the UN sustainable development goals mandate green process innovation research that transcends disciplinary boundaries. Responding to this opportunity, this article explores the question: how can researchers, as societal stakeholders, collaborate in the design and implementation of a green process innovation research initiative and produce actionable research-based contributions to knowledge? Drawing upon our shared experience of realizing green process innovation, we describe and conceptualize the collaborative inquiry process, reflecting on the interplay of modes of knowledge production and the complementarity of researchers' roles. We conclude by noting how researchers collaborating in a green process innovation initiative can shape the environment in which Transdisciplinary research (TDR) develops and play different roles enabling breadth and diversity of interaction, depth of disciplinary integration, and production of different types of knowledge.}
}




@article{Batch-et-al-TVCG-2024,
	title = {{Wizualization: A 'Hard Magic' Visualization System for Immersive and Ubiquitous Analytics}},
	author={Batch, Andrea and Butcher, Peter W. S. and Ritsos, Panagiotis D. and Elmqvist, Niklas},
	journal = {IEEE Transactions on Visualization and Computer Graphics (to appear)},
	year = {2024},
	abstract = {What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (SpellBook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space.},
	teaser={wiz.png},
  doi={10.1109/TVCG.2023.3326580},
	github = {https://github.com/wizualization/}, 
  ytpreview={a5cKFPINHXs},
  info={Presented at IEEE VIS 2023},
}





@article{Shin-et-al-TVCG-2023,
title = {{The Reality of the Situation: A Survey of Situated Analytics}},
author={Shin, Sungbok and Batch, Andrea and Butcher, Peter W. S. and Ritsos, Panagiotis D. and Elmqvist, Niklas},
journal = {IEEE Transactions on Visualization and Computer Graphics (to appear)},
year = {2023},
abstract = { The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user's physical location. In this work, we identify prior literature in this emerging field with a focus on situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. We also assess the level which these systems support the sensemaking process. Finally, we discuss insights and design guidelines that we learned from our analysis.},
teaser={situated_survey.png},
osf = {https://osf.io/3wxv2/}, 
doi={10.1109/TVCG.2023.3285546},
ytpreview={6A-Uk8h9AO0},
info={Presented at IEEE VIS 2023},
}







@article{Batch-et-al-CGF-2023,
title = {{Evaluating View Management for Situated Visualization in Web-based Handheld AR}},
author={Batch, Andrea and Shin, Sungbok and Liu, Julia and Butcher, Peter W. S. and Ritsos, Panagiotis D. and Elmqvist, Niklas},
journal = {Computer Graphics Forum},
year = {2023},
month = jun,
abstract = {As visualization makes the leap to mobile and situated settings, where data is increasingly integrated with the physical world using mixed reality, there is a corresponding need for effectively managing the immersed user's view of situated visualizations. In this paper we present an analysis of view management techniques for situated 3D visualizations in handheld augmented reality: a shadowbox, a world-in-miniature metaphor, and an interactive tour. We validate these view management solutions through a concrete implementation of all techniques within a situated visualization framework built using a web-based augmented reality visualization toolkit, and present results from a user study in augmented reality accessed using handheld mobile devices.},
volume = {42},
pages = {349-360},
number = {3},
teaser={handheld_AR_view_management.png},
osf = {https://osf.io/ma72f/},
editors={Archambault, Daniel and Bujack, Roxana and Schreck, Tobias},
publisher = {The Eurographics Association and John Wiley & Sons Ltd.},
issn = {1467-8659},
doi = {10.1111/cgf.14835},
info={Presented at EG EuroVis 2023},
}










@article{Rigby-et-al-EnvModSoft-2022,
title = {LUCST: A novel toolkit for Land Use Land Cover change assessment in SWAT+ to support flood management decisions},
journal = {Environmental Modelling \& Software},
number = {105469},
year = {2022},
issn = {1364-8152},
volume = {156},
publisher = {Elsevier},
month = aug,
teaser={LUCSTc.png},
doi = {10.1016/j.envsoft.2022.105469},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001736},
author = {Rigby, Alexander M.F. and Butcher, Peter W.S. and Ritsos, Panagiotis D. and Patil, Sopan D.},
abstract = {Land Use Land Cover (LULC) change is widely recognised as one of the most important factors impacting the hydrological response of river basins. SWAT +, the latest version of the Soil and Water Assessment Tool, has been used extensively to assess the hydrological impacts of LULC change. However, the process of making and assessing such changes in SWAT+ is often cumbersome and non-intuitive, thereby reducing its usability amongst a wider pool of applied users. We address this issue by developing a user-friendly toolkit, Land Use Change SWAT+ Toolkit (LUCST), that will: (1) allow the end-user to define various LULC change scenarios in their study catchment, (2) run the SWAT+ model with the specified LULC changes, and (3) enable interactive visualisation of the different SWAT+ output variables. A good System Usability Score (79.8) and positive feedback from end-users promises the potential for adopting LUCST in future LULC change studies.},
ytpreview = {QygBidYr4cQ},
github = {https://github.com/alexrigby/LUCST}
}


@article{Dykes-et-al-PhilTransA-2022,
  author = {Dykes, Jason and Abdul-Rahman, Alfie and Archambault, Daniel and Bach, Benjamin and Borgo, Rita and Chen, Min and Enright, Jessica and Fang, Hui and Firat, Elif E. and Freeman, Euan and G\"{o}nen, Tuna and  Harris, Claire and Jianu, Radu and John, Nigel W. and Khan, Saiful and Lahiff, Andrew and Laramee, Robert S. and Matthews, Louise and Mohr, Sibylle and Nguyen, Phong H. and Rahat, Alma A. M. and Reeve, Richard and Ritsos, Panagiotis D. and Roberts, Jonathan C. and Slingsby, Aidan and Swallow, Ben and Torsney-Weir, Thomas and Turkay, Cagatay and Turner, Robert and Vidal, Franck P. and Wang, Qiru and Wood, Jo and Xu, Kai},
  title = {Visualization for Epidemiological Modelling: Challenges, Solutions, Reflections \& Recommendations},
  journal = {Philosophical Transactions of the Royal Society A (Special issue on 'Technical challenges of modelling real-life epidemics and examples of overcoming these') },
  year = {2022},
  volume = {380},
  number = {2233},
  pages = {20210299},
  month = aug,
  abstract = {We report on an ongoing collaboration between epidemiological modellers and visualization researchers by documenting and reflecting upon knowledge constructs - a series of ideas, approaches and methods taken from existing visualization research and practice – deployed and developed to support modelling of the COVID-19 pandemic. Structured independent commentary on these efforts is synthesized through iterative reflection to develop: evidence of the effectiveness and value of visualization in this context; open problems upon which the research communities may focus; guidance for future activity of this type; and recommendations to safeguard the achievements and promote, advance, secure and prepare for future collaborations of this kind. In describing and comparing a series of related projects that were undertaken in unprecedented conditions, our hope is that this unique report, and its rich interactive supplementary materials, will guide the scientific community in embracing visualization in its observation, analysis and modelling of data as well as in disseminating findings. Equally we hope to encourage the visualization community to engage with impactful science in addressing its emerging data challenges. If we are successful, this showcase of activity may stimulate mutually beneficial engagement between communities with complementary expertise to address problems of significance in epidemiology and beyond.},
  doi = {10.1098/rsta.2021.0299},
  teaser={ramvisII_teaser.png},
  preprint = {arXiv},
  url    ={2204.06946},
  github={https://ramp-vis.github.io/RAMPVIS-PhilTransA-Supplement/},
}



@article{Chen-et-al-Epidemics-2022,
  author = {Chen, Min, and Abdul-Rahman, Alfie and Archambault, Daniel and Dykes, Jason and Slingsby, Aidan and Ritsos, Panagiotis D. and Torsney-Weir, Thomas and Turkay, Cagatay and Bach, Benjamin and Borgo, Rita and Brett, Alys and Fang, Hui and Jianu, Radu and Khan, Shaiful and Laramee, Robert S. and Nguyen, Phong H. and Reeve, Richard and Robert, Jonathan C. and Vidal, Franck and Wang, Qiru and Wood, Jo and Xu, Kai},
  title = {RAMPVIS: Answering the Challenges of Building Visualisation Capabilities for Large-scale Emergency Responses},
  journal = {Epidemics},
  year = {2022},
  volume = {39},
  month = jun,
  number = {100569},
  url={2012.04757},
  preprint={arXiv},
  doi = {10.1016/j.epidem.2022.100569},
  abstract = {The effort for combating the COVID-19 pandemic around the world has resulted in a huge amount of data, e.g., from testing, contact tracing, modelling, treatment, vaccine trials, and more. In addition to numerous challenges in epidemiology, healthcare, biosciences, and social sciences, there has been an urgent need to develop and provide visualisation and visual analytics (VIS) capacities to support emergency responses under difficult operational conditions. In this paper, we report the experience of a group of VIS volunteers who have been working in a large research and development consortium and providing VIS support to various observational, analytical, model-developmental, and disseminative tasks. In particular, we describe our approaches to the challenges that we have encountered in requirements analysis, data acquisition, visual design, software design, system development, team organisation, and resource planning. By reflecting on our experience, we propose a set of recommendations as the first step towards a methodology for developing and providing rapid VIS capacities to support emergency responses.},
  teaser={RAMPv4.png},
}





@article{Roberts-et-al-MDPI-2022,
  author = {Roberts, Jonathan C. and Butcher, Peter WS and Ritsos, Panagiotis D.},
  title = {One View Is Not Enough: Review of and Encouragement for Multiple and Alternative Representations in 3D and Immersive Visualisation},
  journal = {Computers},
  volume = {11},
  year = {2022},
  issue = {2},
  number = {20},
  month = feb,
  url = {https://www.mdpi.com/2073-431X/11/2/20},
  abstract = {The opportunities for 3D visualisations are huge. People can be immersed inside their data, interface with it in natural ways, and see it in ways that are not possible on a traditional desktop screen. Indeed, 3D visualisations, especially those that are immersed inside head-mounted displays are becoming popular. Much of this growth is driven by the availability, popularity and falling cost of head-mounted displays and other immersive technologies. However, there are also challenges. For example, data visualisation objects can be obscured, important facets missed (perhaps behind the viewer), and the interfaces may be unfamiliar. Some of these challenges are not unique to 3D immersive technologies. Indeed, developers of traditional 2D exploratory visualisation tools would use alternative views, across a multiple coordinated view (MCV) system. Coordinated view interfaces help users explore the richness of the data. For instance, an alphabetical list of people in one view shows everyone in the database, while a map view depicts where they live. Each view provides a different task or purpose. While it is possible to translate some desktop interface techniques into the 3D immersive world, it is not always clear what equivalences would be. In this paper, using several case studies, we discuss the challenges and opportunities for using multiple views in immersive visualisation. Our aim is to provide a set of concepts that will enable developers to perform critical thinking, creative thinking and push the boundaries of what is possible with 3D and immersive visualisation. In summary developers should consider how to integrate many views, techniques and presentation styles, and one view is not enough when using 3D and immersive visualisations.},
  doi = {10.3390/computers11020020},
  teaser={oneViewNotEnough_teaser.png}
}



@article{Butcher-et-al-TVCG-2021,
  author={Butcher, Peter W.S. and John, Nigel W. and Ritsos, Panagiotis D.},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  title={{VRIA: A Web-based Framework for Creating Immersive Analytics Experiences}},
  year={2021},
  volume={27},
  number={07},
  pages={3213-3225},
  month=jul,
  doi={10.1109/TVCG.2020.2965109},
  github={https://github.com/vriajs},
  ISSN={1077-2626},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
  abstract={We present &#60;VRIA&#62;, a Web-based framework for creating Immersive Analytics (IA) experiences in Virtual Reality. &#60;VRIA&#62; is built upon WebVR, A-Frame, React and D3.js, and offers a visualization creation workflow which enables users, of different levels of
  expertise, to rapidly develop Immersive Analytics experiences for the Web. The use of these open-standards Web-based technologies
  allows us to implement VR experiences in a browser and offers strong synergies with popular visualization libraries, through the HTML
  Document Object Model (DOM). This makes &#60;VRIA&#62; ubiquitous and platform-independent. Moreover, by using WebVR’s progressive
  enhancement, the experiences &#60;VRIA&#62; creates are accessible on a plethora of devices. We elaborate on our motivation for focusing on
  open-standards Web technologies, present the &#60;VRIA&#62; creation workflow and detail the underlying mechanics of our framework. We also
  report on techniques and optimizations necessary for implementing Immersive Analytics experiences on the Web, discuss scalability
  implications of our framework, and present a series of use case applications to demonstrate the various features of &#60;VRIA&#62;. Finally, we
  discuss current limitations of our framework, the lessons learned from its development, and outline further extensions.},
  teaser={vria_teaser.png},
  ytpreview={YlIRzpN2i-E},
  tags={XR,IA},
  info={Presented at IEEE VIS 2020},
}



@article{Williams-et-al-MDPI-2020,
  author={Williams, Benjamin and Ritsos, Panagiotis D. and Headleand, Christopher},
  journal={Computers},
  title={Virtual Forestry Generation: Evaluating Models for Tree Placement in Games},
  year={2020},
  publisher={MDPI},
  volume={9},
  issue={1},
  number={20},
  month=mar,
  doi={10.3390/computers9010020},
  teaser={virtual_forest_s.png},
  abstract={A handful of approaches have been previously proposed to generate procedurally virtual forestry for virtual worlds and computer games, including plant growthmodels and point distribution methods. However, there has been no evaluation to date which assesses how effective these algorithms are at modelling real-world phenomena. In this paper we tackle this issue by evaluating three algorithms used in the generation of virtual forests – a randomly uniform point distribution method (control), a plant competition model, and an iterative random point distribution technique.Our results show that a plant competition model generated more believable content when viewed from an aerial perspective. Interestingly however, we also found that a randomly uniform point distribution method produced forestry which was rated higher in playability and photorealism, when viewed from a first-person perspective. We conclude that the objective of the game designer is important to consider when selecting an algorithm to generate forestry, as the algorithms produce forestry which is perceived differently.},
}






@article{Gray-et-al-CAEH-2020,
author={Gray, Cameron C. and Perkins, David and Ritsos, Panagiotis D.},
journal={Assessment \& Evaluation in Higher Education},
title={{Degree Pictures: Visualizing the university student journey}},
year={2020},
doi={10.1080/02602938.2019.1676397},
volume = {20},
number = {4},
pages = {568-578},
month=aug,
publisher = {Routledge},
teaser={degreepics.png},
abstract={The field of learning analytics is progressing at a rapid rate. New tools, with ever-increasing number of features and a plethora of datasets that are increasingly utilized demonstrate the evolution and multifaceted nature of the field. In particular, the depth and scope of insight that can be gleaned from analysing related datasets can have a significant, and positive, effect in educational practices. We introduce the concept of degree pictures, a symbolic overview of students’ achievement. Degree pictures are small visualizations that depict graphically 16 categories of overall student achievement, over the duration of a higher education course. They offer a quick summary of students’ achievement and are intended to initiate appropriate responses, such as teaching and pastoral interventions. This can address the subjective nature of assessment, by providing a method for educators to calibrate their own marking practices by showing an overview of any cohort. We present a prototype implementation of degree pictures, which was evaluated within our School of Computer Science, with favourable results.},
}


@article{John-et-al-TVCG-2018,
author={John, Nigel W. and Pop, Serban R. and Day, Thomas W. Day and Ritsos, Panagiotis D. and Headleand, C. J.},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={{The Implementation and Validation of a Virtual Environment for Training Powered Wheelchair Manoeuvres}},
year={2018},
volume={24},
number={5},
pages={1867-1878},
doi={10.1109/TVCG.2017.2700273},
ISSN={1077-2626},
month=may,
abstract={Navigating a powered wheelchair and avoiding collisions is often a daunting task for new wheelchair users. It takes time and practice to gain the coordination needed to become a competent driver and this can be even more of a challenge for someone with a disability. We present a cost-effective virtual reality (VR) application that takes advantage of consumer level VR hardware. The system can be easily deployed in an assessment centre or for home use, and does not depend on a specialized high-end virtual environment such as a Powerwall or CAVE. This paper reviews previous work that has used virtual environments technology for training tasks, particularly wheelchair simulation. We then describe the implementation of our own system and the first validation study carried out using thirty three able bodied volunteers. The study results indicate that at a significance level of 5% then there is an improvement in driving skills from the use of our VR system. We thus have the potential to develop the competency of a wheelchair user whilst avoiding the risks inherent to training in the real world. However, the occurrence of cybersickness is a particular problem in this application that will need to be addressed.},
info={Presented at IEEE VR 2018},
teaser={WheelBanner.png},
}

@article{Roberts-et-al-TVCG-2018,
  author={Roberts, Jonathan C. and Ritsos, Panagiotis D. and Jackson, James and Headleand, Chris},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  title={The explanatory visualization framework: {A}n active learning framework for teaching creative computing using explanatory visualizations},
  year={2018},
  volume={24},
  number={1},
  pages={791-801},
  month=jan,
  doi={  10.1109/TVCG.2017.2745878},
  vmpreview={230840888},
  teaser={evf3.png},
  abstract={Visualizations are nowadays appearing in popular media and are used everyday in the workplace. This democratisation of visualization challenges educators to develop effective learning strategies, in order to train the next generation of creative visualization specialists. There is high demand for skilled individuals who can analyse a problem, consider alternative designs, develop new visualizations, and be creative and innovative. Our three-stage framework, leads the learner through a series of tasks, each designed to develop different skills necessary for coming up with creative, innovative, effective, and purposeful visualizations. For that, we get the learners to create an explanatory visualization of an algorithm of their choice. By making an algorithm choice, and by following an active-learning and project-based strategy, the learners take ownership of a particular visualization challenge. They become enthusiastic to develop good results and learn different creative skills on their learning journey.},
  info={Presented at IEEE VIS 2017},
  
}



@article{Miles-et-al-JOCCH-2016,
 author = {Miles, Helen C. and Wilson, Andrew T. and Labrosse, Fr{\'e}d{\'e}ric and Tiddeman, Bernard and Griffiths, Seren and Edwards, Ben and Ritsos, Panagiotis D. and Mearman, Joseph W. and M\"{o}ller, Katharina and Karl, Raimund and Roberts, Jonathan C.},
 title = {{Alternative Representations of 3D-Reconstructed Heritage Data}},
 journal = {ACM Journal on Computing and Cultural Heritage (JOCCH)},
 issue_date = {November 2015},
 volume = {9},
 number = {1},
 month = nov,
 year = {2015},
 issn = {1556-4673},
 pages = {4:1--4:18},
 articleno = {4},
 numpages = {18},
 doi={10.1145/2795233},
 teaser={heritagetogether.png},
 abstract={By collecting images of heritage assets from members of the public and processing them to create 3D-reconstructed models, the HeritageTogether project has accomplished the digital recording of nearly 80 sites across Wales, UK. A large amount of data has been collected and produced in the form of photographs, 3D models, maps, condition reports, and more. Here we discuss some of the different methods used to realize the potential of this data in different formats and for different purposes. The data are explored in both virtual and tangible settings, and—with the use of a touch table—a combination of both. We examine some alternative representations of this community-produced heritage data for educational, research, and public engagement applications.},
}



@article{Roberts-et-al-TVCG-2016,
  author={Roberts, Jonathan C. and Headleand, Chris and Ritsos, Panagiotis D.},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  title={{Sketching Designs Using the Five Design-Sheet Methodology}},
  year={2016},
  volume={22},
  number={1},
  pages={419-428},
  month=jan,
  doi={10.1109/TVCG.2015.2467271},
  vmpreview={136205787},
  teaser={fdsbanner.png},
  abstract={Sketching designs has been shown to be a useful way of planning and considering alternative solutions. The use of lo-fidelity prototyping, especially paper-based sketching, can save time, money and converge to better solutions more quickly. However, this design process is often viewed to be too informal. Consequently users do not know how to manage their thoughts and ideas (to first think divergently, to then finally converge on a suitable solution). We present the Five Design Sheet (FdS) methodology. The methodology enables users to create information visualization interfaces through lo-fidelity methods. Users sketch and plan their ideas, helping them express different possibilities, think through these ideas to consider their potential effectiveness as solutions to the task (sheet 1); they create three principle designs (sheets 2,3 and 4); before converging on a final realization design that can then be implemented (sheet 5). In this article, we present (i) a review of the use of sketching as a planning method for visualization and the benefits of sketching, (ii) a detailed description of the Five Design Sheet (FdS) methodology, and (iii) an evaluation of the FdS using the System Usability Scale, along with a case-study of its use in industry and experience of its use in teaching.},
  info={Presented at IEEE VIS 2015},
}

@article{Roberts-et-al-CGA2014,
	author ={Roberts, Jonathan C. and Ritsos, Panagiotis D. and Badam, Sriram Karthik and Brodbeck, Dominique and Kennedy, Jessie and Elmqvist, Niklas},
	journal ={IEEE Computer Graphics and Applications},
	title ={{Visualization Beyond the Desktop - the next big thing}},
	year ={2014},
	month =nov,
	volume ={34},
	number ={6},
	pages ={26-34},
	doi={10.1109/MCG.2014.82},
  vmpreview={136248781},
  teaser={vis_perception.png},
  abstract={Visualization is coming of age. With visual depictions being seamlessly integrated into documents, and data visualization techniques being used to understand increasingly large and complex datasets, the term "visualization"' is becoming used in everyday conversations. But we are on a cusp; visualization researchers need to develop and adapt to today's new devices and tomorrow's technology. Today, people interact with visual depictions through a mouse. Tomorrow, they'll be touching, swiping, grasping, feeling, hearing, smelling, and even tasting data. The next big thing is multisensory visualization that goes beyond the desktop.},
  info={Presented at IEEE VIS 2015}
}

@article{George-et-al-Evn2014,
	author ={George, Richard L.S.F. and Robins, Peter E. and Davies, Alan G. and Ritsos, Panagiotis D. and Roberts, Jonathan C.},
	title ={Interactive visual analytics of hydrodynamic flux for the coastal zone},
	journal ={Environmental Earth Sciences},
	year ={2014},
  month=nov,
  volume={72},
  number={10},
  pages={3753--3766},
	publisher ={Springer Berlin Heidelberg},
  doi={10.1007/s12665-014-3283-9},
  teaser={vincabanner.png},
  abstract={Researchers wish to study the potential impact of sea level rise from climate change, and visual analytic tools can allow scientists to visually examine and explore different possible scenarios from simulation runs. In particular, hydrodynamic flux is calculated to understand the net movement of water; but typically this calculation is tedious and is not easily achieved with traditional visualization and analytic tools. We present a visual analytic method that incorporates a transect profiler and flux calculator. The analytic software is incorporated into our visual analytics tool Vinca, and generates multiple transects, which can be visualized and analysed in several alternative visualizations; users can choose specific transects to compare against real-world data; users can explore how flux changes within a domain. In addition, we report how ocean scientists have used our tool to display multiple-view views of their data and analyse hydrodynamic flux for the coastal zone.},
}


@incollection{Ritsos-et-al-Lncs2013,
	author ={Ritsos, Panagiotis D. and Gittins, Robert and Braun, Sabine and Slater, Catherine and Roberts, Jonathan C.},
	title ={{Training Interpreters using Virtual Worlds}},
	booktitle ={Transactions on Computational Science XVIII},
	volume ={7848},
	pages ={21--40},
	year ={2013},
	month =may,
	series ={Lecture Notes in Computer Science},
  doi={10.1007/978-3-642-38803-3_2},
	publisher ={Springer Berlin Heidelberg},
    teaser={ivyscenario.png},
  abstract={With the rise in population migration there has been an increased need for professional interpreters who can bridge language barriers and operate in a variety of fields such as business, legal, social and medical. Interpreters require specialized training to cope with the idiosyncrasies of each field and their potential clients need to be aware of professional parlance. We present ‘Project IVY’. In IVY, users can make a selection from over 30 interpreter training scenarios situated in the 3D virtual world. Users then interpret the oral interaction of two avatar actors. In addition to creating different 3D scenarios, we have developed an asset management system for the oral files and permit users (mentors of the training interpreters) to easily upload and customize the 3D environment and observe which scenario is being used by a student. In this article we present the design and development of the IVY Virtual Environment and the asset management system. Finally we make discussion over our plans for further development.},
}

@article{Paneels-et-al-CG2013,
	title ={{Prototyping 3D haptic data visualizations}},
	author ={Pan\"eels, Sabrina A. and Ritsos, Panagiotis D. and Rodgers, Peter J. and Roberts, Jonathan C.},
	journal ={Computers and Graphics},
	volume ={37},
	number ={3},
	pages ={179 - 192},
	year ={2013},
  month=may,
  doi={10.1016/j.cag.2013.01.009},
      teaser={hitproto.png},
  abstract={Haptic devices are becoming more widely used as hardware becomes available and the cost of both low and high fidelity haptic devices decreases. One of the application areas of haptics is haptic data visualization (HDV). HDV provides functionality by which users can feel and touch data. Blind and partially sighted users can benefit from HDV, as it helps them manipulate and understand information. However, developing any 3D haptic world is difficult, time-consuming and requires skilled programmers. Therefore, systems that enable haptic worlds to be rapidly developed in a simple environment could enable non-computer skilled users to create haptic 3D interactions. In this article we present HITPROTO: a system that enables users, such as mentors or support workers, to quickly create haptic interactions (with an emphasis on HDVs) through a visual programming interface. We describe HITPROTO and include details of the design and implementation. We present the results of a detailed study using postgraduate students as potential mentors, which provides evidence of the usability of HITPROTO. We also present a pilot study of HITPROTO with a blind user. It can be difficult to create prototyping tools and support 3D interactions, therefore we present a detailed list of ‘lessons learnt’ that provides a set of guidelines for developers of other 3D haptic prototyping tools.},
}
