<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Human-Centred Computing Research Group, Bangor University</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2025-11-04T15:01:52+01:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name></name>
   <email></email>
 </author>

 
 <entry>
   <title>Papers for presentation at IEEE VIS2025</title>
   <link href="http://localhost:4000/news/2025/08/11/publications/"/>
   <updated>2025-08-11T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2025/08/11/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/vis2025_logo.png&quot; class=&quot;prjImgFlt&quot; alt=&quot;IEEE VIS2025 Banner and Logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We are participating, with several works, at &lt;a href=&quot;https://ieeevis.org/year/2025/welcome&quot;&gt;IEEE VIS2025&lt;/a&gt;, to be held in Vienna, Austria.&lt;/p&gt;

&lt;p&gt;We will be presenting our VIS paper &lt;a href=&quot;/publicationsYear#Roberts-et-al-TVCG-2026&quot;&gt;“Critical Thinking in Data Visualisation: A Framework for Contextual Scenarios”&lt;/a&gt;, which introduces the Critical Design Strategy (CDS)—a structured heuristic evaluation method designed to facilitate the examination of visualisation designs through reflection and critical thought.&lt;/p&gt;

&lt;p&gt;We will also be presenting our IEEE TVCG Paper &lt;a href=&quot;/publicationsYear#Borowski-et-al-TVCG-2025&quot;&gt;“DashSpace: A Cross-Reality Toolkit for Immersive Analytics”&lt;/a&gt;, which introduces DashSpace, a cross-reality toolkit designed to support immersive analytics. This toolkit allows users to create and manipulate visualizations in both 2D and 3D environments, facilitating a more integrated approach to data analysis across different platforms. DashSpace is part of our ongoing collaboration with Aarhus Unniversity on anywhere, anytime analytics toolkits, along with our recent UIST’25 paper &lt;a href=&quot;/publicationsYear#Borowski-et-al-UIST-2025&quot;&gt;“Spatialstrates: Cross-Reality Collaboration through Spatial Hypermedia”&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Related to our DashSpace project, we will be presenting a tutorial on &lt;a href=&quot;/publicationsYear#Borowski-et-al-IEEEVIS205-Tutorial&quot;&gt;“Live Collaborative Immersive Analytics Development with DashSpace”&lt;/a&gt;. This tutorial will cover the DashSpace/SpatialStrate platform’s architecture, its use of web technologies, and how it supports cross-reality collaboration.&lt;/p&gt;

&lt;p&gt;We are also presenting a workshop paper titled &lt;a href=&quot;/publicationsYear#Roberts-et-al-EduVis-2025&quot;&gt;“From Data to Insight: Using Contextual Scenarios to Teach Critical Thinking in Data Visualisation”&lt;/a&gt;, which discusses the use of contextual scenarios to enhance data visualization education.&lt;/p&gt;

&lt;p&gt;You can find more details on these papers following the reference links below.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-TVCG-2026&quot;&gt;J. C. Roberts, A. Hanan, O. A. E., and P. D. Ritsos, “Critical Design Strategy: a Method for Heuristically Evaluating Visualisation Designs,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/i&gt;, 2026.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We present the Critical Design Strategy (CDS)—a structured method designed to facilitate the examination of visualisation designs through reflection and critical thought. The CDS helps designers think critically and make informed improvements using heuristic evaluation. When developing a visual tool or pioneering a novel visualisation approach, identifying areas for enhancement can be challenging. Critical thinking is particularly crucial for visualisation designers and tool developers, especially those new to the field, such as studying visualisation in higher education. The CDS consists of three stages across six perspectives: Stage 1 captures the essence of the idea by assigning an indicative title and selecting five adjectives (from twenty options) to form initial impressions of the design. Stage 2 involves an in-depth critique using 30 heuristic questions spanning six key perspectives—user, environment, interface, components, design, and visual marks. Stage 3 focuses on synthesising insights, reflecting on design decisions, and determining the next steps forward. We introduce the CDS and explore its use across three visualisation modules in both undergraduate and postgraduate courses. Our longstanding experience with the CDS has allowed us to refine and develop it over time: from its initial creation through workshops in 2017/18 to improvements in wording and the development of two applications by 2020, followed by the expansion of support notes and refinement of heuristics through 2023; while using it in our teaching each year. This sustained use allows us to reflect on its practical application and offer guidance on how others can incorporate it into their own work.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-TVCG-2026/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-TVCG-2026.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
   [&lt;a href=&quot;https://arxiv.org/abs/2508.05325&quot; class=&quot;doilink&quot;&gt;Preprint&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
   [&lt;span class=&quot;addedInfo&quot;&gt;To be presented at IEEE VIS 2025&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-TVCG-2026/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Borowski-et-al-TVCG-2025&quot;&gt;M. Borowski, P. W. S. Butcher, J. B. Kristensen, J. O. Petersen, P. D. Ritsos, C. N. Klokmose, and N. Elmqvist, “DashSpace: A Live Collaborative Platform for Immersive and Ubiquitous Analytics,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/i&gt;, vol. 31, no. 10, pp. 7034–7047, 2025.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We introduce DashSpace, a live collaborative immersive and ubiquitous analytics (IA/UA) platform designed for handheld and head-mounted Augmented/Extended Reality (AR/XR) implemented using WebXR and open standards. To bridge the gap between existing web-based visualizations and the immersive analytics setting, DashSpace supports visualizing both legacy D3 and Vega-Lite visualizations on 2D planes, and extruding Vega-Lite specifications into 2.5D. It also supports fully 3D visual representations using the Optomancy grammar. To facilitate authoring new visualizations in immersive XR, the platform provides a visual authoring mechanism where the user groups specification snippets to construct visualizations dynamically. The approach is fully persistent and collaborative, allowing multiple participants—whose presence is shown using 3D avatars and webcam feeds—to interact with the shared space synchronously, both co-located and remotely. We present three examples of DashSpace in action: immersive data analysis in 3D space, synchronous collaboration, and immersive data presentations.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Borowski-et-al-TVCG-2025/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Borowski-et-al-TVCG-2025.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/TVCG.2025.3537679&quot; class=&quot;doilink&quot;&gt;10.1109/TVCG.2025.3537679&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;To be presented at IEEE VIS 2025&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Borowski-et-al-TVCG-2025/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Borowski-et-al-IEEEVIS205-Tutorial&quot;&gt;M. Borowski, P. W. S. Butcher, S. J. Jones, P. D. Ritsos, C. N. Klokmose, and N. Elmqvist, “Tutorial: Live Collaborative Immersive Analytics Development with DashSpace,” in &lt;i&gt;Tutorials of at the IEEE VIS: Visualization &amp;amp; Visual Analytics Conference, Vienna, Austria&lt;/i&gt;, 2025.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Immersive, situated, and ubiquitous analytics (IA/SA/UA) envision scenarios where heterogeneous devices can be used in combination and multiple users can work in collaboration around data. While there are many research prototypes in this domain, there is a lack of a common platform to build upon. Furthermore, many prototypes build on proprietary game engines such as Unreal or Unity, which make distribution and reuse more difficult, require niche knowledge which is not often found in enterprise environments, as well as being prone to changing license agreements. We therefore advocate for the use of open standards, such as WebXR in order to overcome such restrictions. During the last year we created two iterations of an open, malleable, and collaborative platform for IA/SA/UA: DashSpace. We published both DashSpace as well as a its core WebXR framework Spatialstrates on GitHub. DashSpace and Spatialstrates build on the know-how and technical synergies between Web-based IA/SA frameworks, such as VRIA and Wizualization, UA frameworks such as Vistrates, and dynamic-media environments such as Webstrates, all converging together since 2019, through the binding affordances that the reliance on Web technologies enables.

	We deem the tutorial framing particularly well suited to introduce DashSpace in depth to a broader audience of IA/SA/UA researchers, with the goal of creating a growing and interoperable platform for these interaction flavors. We envision to grow the platform and create a “component store” or package repository with self-contained components and packages that can be mixed and matched within the same platform without the need to install new applications on multiple devices or re-implementing common features.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Borowski-et-al-IEEEVIS205-Tutorial/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Borowski-et-al-IEEEVIS205-Tutorial.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Borowski-et-al-IEEEVIS205-Tutorial/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-EduVis-2025&quot;&gt;J. C. Roberts, P. W. S. Butcher, and P. D. Ritsos, “From Data to Insight: Using Contextual Scenarios to Teach Critical Thinking in Data Visualisation,” in &lt;i&gt;IEEE VIS Workshop on Visualization Education, Literacy, and Activities&lt;/i&gt;, 2025.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    This paper explores the use of scenario-based visualisation examples as a pedagogical strategy for teaching students the complexities of data insight, representation, and interpretation. Teaching data visualisation often involves explaining intricate issues related to data management and the challenges of presenting data meaningfully. In this work, we present a series of data-driven scenarios. These concise stories depict specific situations, and are created to help the educators highlight key concerns in data communication, such as chart selection, temporal versus categorical comparison, visual bias, and narrative framing. By grounding these examples in real-world contexts, students are encouraged to critically assess not only what the data shows, but how and why it is shown that way. This approach aims to deepen students’ data literacy and awareness of the ethical, analytical, and rhetorical choices that shape effective visual storytelling. The paper presents a collection of example scenarios, that educators can use for their own lessons; the work fits with a larger project on looking at critical thinking in the classroom, and developing appropriate tools. We also start to abstract principles, from our approach, so that others can develop their own scenarios for their teaching. Our approach aligns with principles of authentic and scenario-based learning, using real-world contexts to foster critical engagement with data.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-EduVis-2025/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-EduVis-2025.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
   [&lt;a href=&quot;https://arxiv.org/abs/2508.08737&quot; class=&quot;doilink&quot;&gt;Preprint&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-EduVis-2025/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper accepted by UIST'25</title>
   <link href="http://localhost:4000/news/2025/07/24/publications/"/>
   <updated>2025-07-24T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2025/07/24/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/Spatialstrates.png&quot; class=&quot;prjImgFlt&quot; alt=&quot;Spatialstrates&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our paper &lt;a href=&quot;/publicationsYear#Borowski-et-al-UIST-2025&quot;&gt;“Spatialstrates: Cross-Reality Collaboration through Spatial Hypermedia&lt;/a&gt; has been accepted to the &lt;a href=&quot;https://uist.acm.org/2025/&quot;&gt;38th Annual ACM Symposium on User Interface Software and Technology (UIST 2025)&lt;/a&gt;, to be held in Busan, South Korea, in September 2025.&lt;/p&gt;

&lt;p&gt;This article continues the work on immersive toolkits, explored initially with &lt;a href=&quot;/bibliography/Borowski-et-al-TVCG-2025/&quot;&gt;DashSpace&lt;/a&gt;, and extends it towards the theme of cross-reality collaboration through spatial hypermedia.&lt;/p&gt;

&lt;p&gt;Contemporary consumer-level XR hardware enables immersive spatial computing, yet most knowledge work remains confined to traditional 2D desktop environments. These worlds exist in isolation: writing emails or editing presentations favors desktop interfaces, while viewing 3D simulations or architectural models benefits from immersive environments.&lt;/p&gt;

&lt;p&gt;We address this fragmentation by combining spatial hypermedia, shareable dynamic media, and cross-reality computing to provide (1) composability of heterogeneous content and of nested information spaces through spatial transclusion,(2) pervasive cooperation across heterogeneous devices and platforms, and (3) congruent spatial representations despite underlying environmental differences.&lt;/p&gt;

&lt;p&gt;Our implementation, the Spatialstrates platform, embodies these principles using standard web technologies to bridge 2D desktop and 3D immersive environments.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Borowski-et-al-UIST-2025&quot;&gt;M. Borowski, J. E. Grønbæk, P. W. S. Butcher, P. D. Ritsos, C. N. Klokmose, and N. Elmqvist, “Spatialstrates: Cross-Reality Collaboration through Spatial Hypermedia,” in &lt;i&gt;Proceedings of the The 38th Annual ACM Symposium on User Interface Software and Technology (UIST 2025)&lt;/i&gt;, New York, NY, USA, 2025.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Consumer-level XR hardware now enables immersive spatial computing, yet most knowledge work remains confined to traditional 2D desktop environments. These worlds exist in isolation: writing emails or editing presentations favors desktop interfaces, while viewing 3D simulations or architectural models benefits from immersive environments. We address this fragmentation by combining spatial hypermedia, shareable dynamic media, and cross-reality computing to provide (1) composability of heterogeneous content and of nested information spaces through spatial transclusion,(2) pervasive cooperation across heterogeneous devices and platforms, and (3) congruent spatial representations despite underlying environmental differences. Our implementation, the Spatialstrates platform, embodies these principles using standard web technologies to bridge 2D desktop and 3D immersive environments. Through four scenarios—collaborative brainstorming, architectural design, molecular science visualization, and immersive analytics—we demonstrate how Spatialstrates enables collaboration between desktop 2D and immersive 3D contexts, allowing users to select the most appropriate interface for each task while maintaining collaborative capabilities.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Borowski-et-al-UIST-2025/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Borowski-et-al-UIST-2025.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1145/3746059.3747708&quot; class=&quot;doilink&quot;&gt;10.1145/3746059.3747708&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Borowski-et-al-UIST-2025/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Paper published in IEEE CG&A</title>
   <link href="http://localhost:4000/news/2025/04/11/publications/"/>
   <updated>2025-04-11T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2025/04/11/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/prepareVisPandemic.png&quot; class=&quot;prjImgFlt&quot; alt=&quot;Dashspace&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our paper &lt;a href=&quot;/publicationsYear#McGee-et-al-CGA-2025&quot;&gt;Preparedness for visualization in the next pandemic&lt;/a&gt; has been published in &lt;a href=&quot;https://www.computer.org/csdl/magazine/cg&quot;&gt;IEEE Computer Graphics and Applications&lt;/a&gt;. This article discusses considerations on how visualization can be best positioned to help response to future pandemics. The work is a product of the Dagstuhl Seminar 23482 on “Visualization for Pandemic Preparedness” held in January 2024. You can find more info on the seminar &lt;a href=&quot;https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=23482&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;McGee-et-al-CGA-2025&quot;&gt;F. McGee, M. Abu Sin, M. Chen, D. Ebert, K. Misue, P. D. Ritsos, and A. Wulff, “Preparedness for visualization in the next pandemic,” &lt;i&gt;IEEE Computer Graphics and Applications&lt;/i&gt;, vol. 45, no. 3, pp. 95–103, 2025.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    This article discusses considerations on how visualization can be best positioned to help response to future pandemics. We examine visualization, along with the corresponding and necessary enabling technologies and platforms, as a tool to facilitate a rapid and effective response to a forthcoming pandemic. We consider challenges in terms of an infrastructure supporting world-wide response, corresponding training and stakeholder engagement, integration of future technologies, and appraisal of such systems, Finally, we discuss how addressing these challenges also helps emergency response beyond infectious diseases.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/McGee-et-al-CGA-2025/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/McGee-et-al-CGA-2025.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/MCG.2025.3561619&quot; class=&quot;doilink&quot;&gt;10.1109/MCG.2025.3561619&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/McGee-et-al-CGA-2025/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Paper published in IEEE TVCG, to be presented at IEEE VIS 2025</title>
   <link href="http://localhost:4000/news/2025/01/22/publications/"/>
   <updated>2025-01-22T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2025/01/22/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/dashspace.png&quot; class=&quot;prjImgFlt&quot; alt=&quot;Dashspace&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our paper &lt;a href=&quot;/publicationsYear#Borowsky-et-al-TVCG-2025&quot;&gt;DashSpace: A Live Collaborative Platform for Immersive and Ubiquitous Analytics&lt;/a&gt; has been published in &lt;a href=&quot;https://www.computer.org/csdl/journal/tg&quot;&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/a&gt; and will be presented in &lt;a href=&quot;https://ieeevis.org/year/2025/welcome&quot;&gt;IEEE VIS 2025&lt;/a&gt; in Vienna, Austria.&lt;/p&gt;

&lt;p&gt;DashSpace is a live collaborative immersive and ubiquitous analytics (IA/UA) platform designed for handheld and head-mounted Augmented/Extended Reality (AR/XR) implemented using WebXR and open standards. To bridge the gap between existing web-based visualizations and the immersive analytics setting, DashSpace supports visualizing both legacy D3 and Vega-Lite visualizations on 2D planes, and extruding Vega-Lite specifications into 2.5D. It also supports fully 3D visual representations using the Optomancy grammar.&lt;/p&gt;

&lt;p&gt;To facilitate authoring new visualizations in immersive XR, the platform provides a visual authoring mechanism where the user groups specification snippets to construct visualizations dynamically. The approach is fully persistent and collaborative, allowing multiple participants—whose presence is shown using 3D avatars and webcam feeds—to interact with the shared space synchronously, both co-located and remotely. In our paper, we present three examples of DashSpace in action: immersive data analysis in 3D space, synchronous collaboration, and immersive data presentations.&lt;/p&gt;

&lt;p&gt;You can find DashSpace on &lt;a href=&quot;https://github.com/Webstrates/DashSpace&quot;&gt;Girhub&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Presented at DAFNI Wales Roadshow 2024</title>
   <link href="http://localhost:4000/news/2024/09/20/workshop/"/>
   <updated>2024-09-20T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2024/09/20/workshop</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/DAFNI-logo.jpg&quot; class=&quot;prjImgFlt&quot; alt=&quot;DAFNI_banner&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I presented a lightning talk on our &lt;a href=&quot;/projects/ia/&quot;&gt;Immersive and Situated Analytics&lt;/a&gt; systems at the DAFNI Wales Roadshow 2024 event, held at the &lt;a href=&quot;https://www.swansea.ac.uk/computational-foundry/&quot;&gt;Computational Foundry&lt;/a&gt;, &lt;a href=&quot;https://www.swansea.ac.uk/&quot;&gt;Swansea University&lt;/a&gt;.  We discussed research into infrastructure systems, synergies with visualization, and the challenges and opportunities of data sharing.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Papers accepted by IEEE TVCG/ VIS2024</title>
   <link href="http://localhost:4000/news/2024/08/09/publications/"/>
   <updated>2024-08-09T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2024/08/09/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/vi2024_logo.png&quot; class=&quot;prjImgFlt&quot; alt=&quot;IEEE VIS2024 Banner and Logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We presented two papers in &lt;a href=&quot;https://ieeevis.org/year/2024/welcome&quot;&gt;IEEE VIS2024&lt;/a&gt;, published in &lt;a href=&quot;https://www.computer.org/csdl/journal/tg&quot;&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/a&gt;, in 2025.&lt;/p&gt;

&lt;p&gt;In our paper &lt;a href=&quot;/publicationsYear#Srinivasan-et-al-TVCG-2025&quot;&gt;Attention-Aware Visualization: Tracking and Responding to User Perception Over Time&lt;/a&gt;, we propose the notion of Attention-Aware Visualizations (AAVs) that track the user’s perception of a visual representation over time and feed this information back to the visualization. We present two separate implementations of AAV: a 2D data-agnostic method for web-based visualizations that can use an embodied eyetracker to capture the user’s gaze, and a 3D data-aware one that uses the stencil buffer to track the visibility of each individual mark in a visualization.&lt;/p&gt;

&lt;p&gt;In our paper &lt;a href=&quot;/publicationsYear#Jackson-et-al-TVCG-2025&quot;&gt;Path-based Design Model for Constructing and Exploring Alternative Visualisations&lt;/a&gt;, we present a path-based design model and system for designing and creating visualisations. Our model represents a systematic approach to constructing visual representations of data or concepts following a predefined sequence of steps. Through our implementation we showcase the model in action. We (1) introduce, define and explain the path model and discuss possibilities for its use, (2) present our implementation, results, and evaluation, and (3) demonstrate and evaluate an application of its use on a mobile watch.&lt;/p&gt;

&lt;p&gt;You can find more details on these papers following the reference links below.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Srinivasan-et-al-TVCG-2025&quot;&gt;A. Srinivasan, J. Ellemose, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “Attention-Aware Visualization: Tracking and Responding to User Perception Over Time,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/i&gt;, vol. 31, no. 1, pp. 1017–1027, 2025.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We propose the notion of Attention-Aware Visualizations (AAVs) that track the user’s perception of a visual representation over time and feed this information back to the visualization. Such context awareness is particularly useful for ubiquitous and immersive analytics where knowing which embedded visualizations the user is looking at can be used to make visualizations react appropriately to the user’s attention: for example, by highlighting data the user has not yet seen. We can separate the approach into three components: (1) measuring the user’s gaze on a visualization and its parts; (2) tracking the user’s attention over time; and (3) reactively modifying the visual representation based on the current attention metric. In this paper, we present two separate implementations of AAV: a 2D data-agnostic method for web-based visualizations that can use an embodied eyetracker to capture the user’s gaze, and a 3D data-aware one that uses the stencil buffer to track the visibility of each individual mark in a visualization. Both methods provide similar mechanisms for accumulating attention over time and changing the appearance of marks in response. We also present results from a qualitative evaluation studying visual feedback and triggering mechanisms for capturing and revisualizing attention.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Srinivasan-et-al-TVCG-2025/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Srinivasan-et-al-TVCG-2025.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
   [&lt;a href=&quot;https://arxiv.org/abs/2404.10732&quot; class=&quot;doilink&quot;&gt;Preprint&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/TVCG.2024.3456300&quot; class=&quot;doilink&quot;&gt;10.1109/TVCG.2024.3456300&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2024&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Srinivasan-et-al-TVCG-2025/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Jackson-et-al-TVCG-2025&quot;&gt;J. Jackson, P. D. Ritsos, P. W. S. Butcher, and J. C. Roberts, “Path-based Design Model for Constructing and Exploring Alternative Visualisations,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/i&gt;, vol. 31, no. 1, pp. 1158–1168, 2025.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We present a path-based design model and system for designing and creating visualisations. Our model represents a systematic approach to constructing visual representations of data or concepts following a predefined sequence of steps. The initial step involves outlining the overall appearance of the visualisation by creating a skeleton structure, referred to as a flowpath. Subsequently, we specify objects, visual marks, properties, and appearance, storing them in a gene. Lastly, we map data onto the flowpath, ensuring suitable morphisms. Alternative designs are created by exchanging values in the gene. For example, designs that share similar traits, are created by making small incremental changes to the gene. Our design methodology fosters the generatiion of diverse creative concepts, space-filling visualisations, and traditional formats like bar charts, circular plots and pie charts. Through our implementation we showcase the model in action. As an example application, we integrate the output visualisations onto a smartwatch and visualisation dashboards. In this article we (1) introduce, define and explain the path model and discuss possibilities for its use, (2) present our implementation, results, and evaluation, and (3) demonstrate and evaluate an application of its use on a mobile watch.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Jackson-et-al-TVCG-2025/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Jackson-et-al-TVCG-2025.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
   [&lt;a href=&quot;https://arxiv.org/abs/2408.03681&quot; class=&quot;doilink&quot;&gt;Preprint&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/TVCG.2024.3456323&quot; class=&quot;doilink&quot;&gt;10.1109/TVCG.2024.3456323&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2024&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Jackson-et-al-TVCG-2025/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Participated in Dagstuhl Seminar - Reflections on Pandemic Visualization</title>
   <link href="http://localhost:4000/news/2024/03/01/travel/"/>
   <updated>2024-03-01T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2024/03/01/travel</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/dagstuhl.jpg&quot; class=&quot;prjImgFlt&quot; alt=&quot;Image of Schloss Dagstuhl&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I attended the Reflections on &lt;a href=&quot;https://www.dagstuhl.de/en/seminars/seminar-calendar/seminar-details/24091&quot;&gt;Reflections on Pandemic Visualization Dagstuhl Seminar&lt;/a&gt;, held in the &lt;a href=&quot;https://www.dagstuhl.de/en/&quot;&gt;Computer Science Centre, Schloss Dagstuhl, Germany&lt;/a&gt;, between 24/2 and 1/3.&lt;/p&gt;

&lt;p&gt;The purpose of this Dagstuhl Seminar was to bring the visualization community, modelling experts, epidemiologists, infection control experts, and end users together with other application domain experts to reflect on research work done in response to the COVID-19 pandemic.&lt;/p&gt;

&lt;p&gt;Dagstuhl Seminars are the flagship of the event program at Schloss Dagstuhl. They are often praised by participants as the most productive academic events they have ever experienced. Consequently participation is highly coveted. Their topic can be anything that is computer science related. These seminars focus on the exchange and development of ideas and not so much on the presentation of already achieved results. They profit from Dagstuhl’s ambience and setup that promote personal interaction and open discussion (&lt;a href=&quot;https://www.dagstuhl.de/en/seminars/dagstuhl-seminars&quot;&gt;source&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;You can find the outputs produced below.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;DagRep_14_2_191&quot;&gt;D. Archambault, F. McGee, N. Reinoso-Schiller, T. von Landesberger, and S. Scheithauer, “Reflections on Pandemic Visualization (Dagstuhl Seminar 24091),” &lt;i&gt;Dagstuhl Reports&lt;/i&gt;, vol. 14, no. 2, pp. 191–205, 2024.&lt;/span&gt;

&lt;br /&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/DagRep_14_2_191/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.4230/DagRep.14.2.191&quot; class=&quot;doilink&quot;&gt;10.4230/DagRep.14.2.191&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/DagRep_14_2_191/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper published in IEEE CG&A Vis Viewpoints</title>
   <link href="http://localhost:4000/news/2024/02/15/publications/"/>
   <updated>2024-02-15T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2024/02/15/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/unity-webxr-benchmarks.png&quot; class=&quot;prjImgFlt&quot; alt=&quot;WebXR vs Game Engine Bnechmarks&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our paper &lt;a href=&quot;/publicationsYear#Butcher-et-al-CGA2024&quot;&gt;“Is Native Naïve? Comparing Native Game Engines and WebXR as Immersive Analytics Development Platforms,”&lt;/a&gt; has been published in &lt;a href=&quot;https://tc.computer.org/vgtc/conferences/currently-sponsored-events/cga-viewpoints/&quot;&gt;IEEE Computer Graphics and Applications Visualization Viewpoints&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this paper, we investigate common assumptions about native game engines vs. WebXR and find that while native engines still have an advantage in many areas, WebXR is rapidly catching up and is superior for many immersive analytics applications.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Butcher-et-al-CGA2024&quot;&gt;P. W. S. Butcher, A. Batch, D. Saffo, B. MacIntyre, N. Elmqvist, and P. D. Ritsos, “Is Native Naïve? Comparing Native Game Engines and WebXR as Immersive Analytics Development Platforms,” &lt;i&gt;IEEE Computer Graphics and Applications&lt;/i&gt;, vol. 44, no. 3, pp. 91–98, May 2024.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Native game engines have long been the 3D development platform of choice for research in mixed and augmented reality. For this reason they have also been adopted in many immersive visualization and immersive analytics systems and toolkits. However, with the rapid improvements of WebXR and related open technologies, this choice may not always be optimal for future visualization research. In this paper, we investigate common assumptions about native game engines vs. WebXR and find that while native engines still have an advantage in many areas, WebXR is rapidly catching up and is superior for many immersive analytics applications.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Butcher-et-al-CGA2024/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Butcher-et-al-CGA2024.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/MCG.2024.3367422&quot; class=&quot;doilink&quot;&gt;10.1109/MCG.2024.3367422&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Butcher-et-al-CGA2024/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>XRVA at IEEE VIS 2023</title>
   <link href="http://localhost:4000/news/2023/10/21/publications/"/>
   <updated>2023-10-21T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2023/10/21/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/vis2023-banner.png&quot; alt=&quot;VIS2023 Header&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We visited Melbourne, Australia, for &lt;a href=&quot;https://ieeevis.org/year/2023/welcome&quot;&gt;&lt;em&gt;IEEE VIS2023&lt;/em&gt;&lt;/a&gt;, the premier forum for advances in scientific and information visualization. This week-long event convenes an international community of researchers and practitioners from academia, government, and industry to explore their shared interests in tools, techniques, and technology.&lt;/p&gt;

&lt;p&gt;We presented the following journals, workshop papers, and posters:&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Batch-et-al-TVCG-2024&quot;&gt;A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “Wizualization: A ’Hard Magic’ Visualization System for Immersive and Ubiquitous Analytics,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics (to appear)&lt;/i&gt;, 2024.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (SpellBook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Batch-et-al-TVCG-2024/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Batch-et-al-TVCG-2024.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/TVCG.2023.3326580&quot; class=&quot;doilink&quot;&gt;10.1109/TVCG.2023.3326580&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2023&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Batch-et-al-TVCG-2024/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Shin-et-al-TVCG-2023&quot;&gt;S. Shin, A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “The Reality of the Situation: A Survey of Situated Analytics,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics (to appear)&lt;/i&gt;, 2023.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
     The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user’s physical location. In this work, we identify prior literature in this emerging field with a focus on situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. We also assess the level which these systems support the sensemaking process. Finally, we discuss insights and design guidelines that we learned from our analysis.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Shin-et-al-TVCG-2023/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Shin-et-al-TVCG-2023.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/TVCG.2023.3285546&quot; class=&quot;doilink&quot;&gt;10.1109/TVCG.2023.3285546&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2023&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Shin-et-al-TVCG-2023/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Chen-et-al-Bulletin-Vis4PandemRes-VIS2023&quot;&gt;M. Chen, A. Abdul-Rahman, D. Archambault, J. Dykes, P. D. Ritsos, A. Slingsby, T. Torsney-Weir, C. Turkay, B. Bach, R. Borgo, A. Brett, H. Fang, R. Jianu, S. Khan, R. S. Laramee, L. Matthews, P. Nguyen, R. Reeve, J. C. Roberts, F. P. Vidal, Q. Wang, J. Wood, and K. Xu, “RAMPVIS: Answering the Challenges of Building Visualization Capabilities for Large-scale Emergency Responses,” in &lt;i&gt;Bulletins presented at the IEEE VIS Workshop on Visualization for Pandemic and Emergency Responses 2023 (Vis4PandEmRes), IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia&lt;/i&gt;, 2023.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    In this bulletin video, we summarize the volunteering activities of  a group of visualization researchers who provided support to  epidemiological modeling during the COVID-19 pandemic. Epidemiological modeling during a pandemic is a complex and continuous process. The intraoperative workflow entails different visualization tasks at four different levels, i.e., disseminative, observational, analytical, and model-developmental visualization. The visualization volunteers were organized into seven teams, including a generic support team, an analytical support team, a disseminative visualization team, and four modeling support teams. During the volunteering activities, we encountered a few major challenges. We made an effort to address these challenges and gained useful experience.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Chen-et-al-Bulletin-Vis4PandemRes-VIS2023/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Chen-et-al-Bulletin-Vis4PandemRes-VIS2023.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Chen-et-al-Bulletin-Vis4PandemRes-VIS2023/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Ritsos-et-al-Bulletin-Vis4PandemRes-VIS2023&quot;&gt;P. D. Ritsos, S. Khan, S. Jones, B. Bach, J. Meikle, J. C. Roberts, J. Wood, and M. Chen, “Creating storytelling visualizations for the Covid-19 pandemic using Feature-Action Design Patterns,” in &lt;i&gt;Bulletins presented at the IEEE VIS Workshop on Visualization for Pandemic and Emergency Responses 2023 (Vis4PandEmRes), IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia&lt;/i&gt;, 2023.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    In this bulletin video, we summarize a novel technique for authoring storytelling visualization. The technique was developed by one of the teams in the RAMPVIS project, which provided visualization support to epidemiological modeling during the COVID-19 pandemic. The team explored the prevailing approaches, in the UK and internationally, for creating public-facing visualizations related to the pandemic. This ranged from those produced by a number of governments (e.g., the four home nations in the UK), organizations (e.g., WHO, UK ONS), universities (e.g., Johns Hopkins dashboards), media outlets (e.g., FT Coronavirus tracker), and non-commercial web services (e.g., Worldometers). The team concluded that we should complement, but not duplicate, the existing effort, and defined our goal as to inform the public through advanced storytelling visualization.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Ritsos-et-al-Bulletin-Vis4PandemRes-VIS2023/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Ritsos-et-al-Bulletin-Vis4PandemRes-VIS2023.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Ritsos-et-al-Bulletin-Vis4PandemRes-VIS2023/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Rigby-et-al-Poster-VIS2023&quot;&gt;A. Rigby, P. W. S. Butcher, R. Bellini, P. Coughlan, A. Mc Nabola, and P. D. Ritsos, “DUVis: A visual analytics tool for supporting a trans-disciplinary project,” in &lt;i&gt;Posters presented at the IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia&lt;/i&gt;, 2023.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We present DUVis, a visual analytics application developed to support the analysis and appraisal, of the transdiciplinary project Dŵr Uisce, from internal project managers and external stakeholders. DUVis provides a number of visualizations and additional features to facilitate data exploration of a project’s progress. It presents a map of stakeholders’ activities, and their engagement with each other, as well as outputs, workpackages, their completion status and potential impact. We present our preliminary design and provide a blueprint for further development.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Rigby-et-al-Poster-VIS2023/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Rigby-et-al-Poster-VIS2023.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Rigby-et-al-Poster-VIS2023/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-Poster-VIS2023&quot;&gt;J. C. Roberts, H. Alnjar, A. E. Owen, and P. D. Ritsos, “A method for Critical and Creative Visualisation Design-Thinking,” in &lt;i&gt;Posters presented at the IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia&lt;/i&gt;, 2023.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Visualisation design requires critical thought: to understand important facets, investigate design suitability and explore alternatives. But, especially for learners, it can be difficult to structure a critical reflection of creative solutions. We introduce the Critical Design Survey (CDS): structured method that facilitates visualisation design analysis through reflective and critical thought. Applying the CDS helps someone to structure critical thought, provides a unified method that can be readily taught, learners can actively engage with the process and directly use it to write a critical-thinking report of their design ideas. The CDS contains three steps: Step 1, summarise and write down the essence of the idea. Step 2, perform an in-depth critique (we define 30 questions structured in six perspectives). Step 3, synthesise the ideas, implications, and decide on the next steps. We present the CDS, describe our design process (critical thinking workshops, talk aloud, and student use), and describe our use in teaching visualisation to undergraduate and postgraduate students.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-Poster-VIS2023/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-Poster-VIS2023.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-Poster-VIS2023/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;You can find more information on the conference at &lt;a href=&quot;http://ieeevis.org/&quot;&gt;IEEE VIS2023&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Workshop paper accepted in HUI / ISMAR2023</title>
   <link href="http://localhost:4000/news/2023/08/07/publications/"/>
   <updated>2023-08-07T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2023/08/07/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/wiz_hui_ismar.png&quot; class=&quot;prjImgFlt&quot; alt=&quot;Wizualization System Overview&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our paper &lt;a href=&quot;/publications#Butcher-et-al-HUI-ISMAR2023&quot;&gt;“Don’t Pull the Balrog — Lessons Learned from Designing Wizualization: a Magic-inspired Data Analytics System in XR”&lt;/a&gt; has been accepted for publication in &lt;a href=&quot;https://hybrid-ui-workshop.io/&quot;&gt;HybridUI: 1st Workshop on Hybrid User Interfaces: Complementary Interfaces for Mixed Reality Interaction&lt;/a&gt;, taking place during &lt;a href=&quot;https://ismar23.org/&quot;&gt;ISMAR 2023&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this paper, we review lessons learned during the development of &lt;a href=&quot;/bibliography/Batch-et-al-TVCG-2024/&quot;&gt;Wizualization&lt;/a&gt;, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using a ‘hard-magic’ metaphor. Wizualization is based on a fundamentally hybrid and multimodal approach utilizing AR/XR, gestures, sound, and speech to support the mobile setting.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Butcher-et-al-HUI-ISMAR2023&quot;&gt;P. W. S. Butcher, A. Batch, P. D. Ritsos, and N. Elmqvist, “Don’t Pull the Balrog — Lessons Learned from Designing Wizualization: a Magic-inspired Data Analytics System in XR,” in &lt;i&gt;HybridUI: 1st Workshop on Hybrid User Interfaces: Complementary Interfaces for Mixed Reality Interaction&lt;/i&gt;, 2023.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    This paper presents lessons learned in the design and development of Wizualization, a ubiquitous analytics system for authoring visualizations in WebXR using a magic metaphor. The system is based on a fundamentally hybrid and multimodal approach utilizing AR/XR, gestures, sound, and speech to support the mobile setting. Our lessons include how to overcome mostly technical challenges, such as view management and combining multiple sessions in the same analytical 3D space, but also user-based, design-oriented, and even social ones. Our intention in sharing these teachings is to help fellow travellers navigate the same troubled waters we have traversed.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Butcher-et-al-HUI-ISMAR2023/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Butcher-et-al-HUI-ISMAR2023.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Butcher-et-al-HUI-ISMAR2023/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Journal paper accepted in IEEE TVCG / VIS2023</title>
   <link href="http://localhost:4000/news/2023/07/17/publications/"/>
   <updated>2023-07-17T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2023/07/17/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/wiz.png&quot; class=&quot;prjImgFlt&quot; alt=&quot;Motivating scenario for Wizualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our paper &lt;a href=&quot;/publications#Batch-et-al-TVCG-2024&quot;&gt;“Wizualization: A ``Hard Magic’’ Visualization System for Immersive and Ubiquitous Analytics”&lt;/a&gt; has been accepted for publication at &lt;a href=&quot;https://ieeevis.org/year/2023/welcome&quot;&gt;IEEE VIS 2023&lt;/a&gt; and will appear in &lt;a href=&quot;https://www.computer.org/csdl/journal/tg&quot;&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using a system based on a ‘hard-magic’ metaphor, through gestures, speech commands, and touch interaction. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data, across time and space. We describe our system using a viginette-based scenario, the actors of which are shown in our teaser, above.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Batch-et-al-TVCG-2024&quot;&gt;A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “Wizualization: A ’Hard Magic’ Visualization System for Immersive and Ubiquitous Analytics,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics (to appear)&lt;/i&gt;, 2024.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (SpellBook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Batch-et-al-TVCG-2024/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Batch-et-al-TVCG-2024.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/TVCG.2023.3326580&quot; class=&quot;doilink&quot;&gt;10.1109/TVCG.2023.3326580&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2023&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Batch-et-al-TVCG-2024/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Journal paper accepted in IEEE TVCG</title>
   <link href="http://localhost:4000/news/2023/06/08/publications/"/>
   <updated>2023-06-08T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2023/06/08/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/situated_survey.png&quot; class=&quot;prjImgFlt&quot; alt=&quot;Example of a situated analytics tool that uses Augmented Reality to enable sensemaking in-situ&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our paper &lt;a href=&quot;/publications#Shin-et-al-TVCG-2023&quot;&gt;“The Reality of the Situation: A Survey of Situated Analytics”&lt;/a&gt; has been accepted for publication in &lt;a href=&quot;https://www.computer.org/csdl/journal/tg&quot;&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/a&gt;, and will be presented, later this year, at &lt;a href=&quot;https://ieeevis.org/year/2023/welcome&quot;&gt;IEEE VIS 2023&lt;/a&gt;. In this work, we identify prior literature in this emerging field with a focus on the technologies enabling such situated analytics. We identify archetypical patterns in our classification using an ensemble cluster analysis, gauge the level of support for sensemaking and discuss several insights and design guidelines that we learned from our analysis.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Shin-et-al-TVCG-2023&quot;&gt;S. Shin, A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “The Reality of the Situation: A Survey of Situated Analytics,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics (to appear)&lt;/i&gt;, 2023.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
     The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user’s physical location. In this work, we identify prior literature in this emerging field with a focus on situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. We also assess the level which these systems support the sensemaking process. Finally, we discuss insights and design guidelines that we learned from our analysis.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Shin-et-al-TVCG-2023/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Shin-et-al-TVCG-2023.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/TVCG.2023.3285546&quot; class=&quot;doilink&quot;&gt;10.1109/TVCG.2023.3285546&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2023&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Shin-et-al-TVCG-2023/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Journal paper accepted in Computer Graphics Forum</title>
   <link href="http://localhost:4000/news/2023/03/30/publications/"/>
   <updated>2023-03-30T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2023/03/30/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/holodeck-teaser.png&quot; class=&quot;prjImgFlt&quot; alt=&quot;Situated Analytics with Handheld AR&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our paper &lt;a href=&quot;/publications#Batch-et-al-EuroVis-2023&quot;&gt;“Evaluating View Management for Situated Visualization in Web-based Handheld AR”&lt;/a&gt; has been accepted for publication in &lt;a href=&quot;https://onlinelibrary.wiley.com/journal/14678659&quot;&gt;Computer Graphics Forum&lt;/a&gt;, and was presented at &lt;a href=&quot;https://www.eurovis.org/&quot;&gt;EuroVis 2023&lt;/a&gt;. In this paper, we (1) present an analysis of the properties and challenges of view management for situated visualizations in Augmented Reality (AR), (2) revisit existing techniques from the domains of computer graphics, Virtual Reality (VR), and visualization, and (3) evaluate implementations of interaction, layout, and presentation of some of those techniques, accessible in web-based handheld AR.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Batch-et-al-CGF-2023&quot;&gt;A. Batch, S. Shin, J. Liu, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “Evaluating View Management for Situated Visualization in Web-based Handheld AR,” &lt;i&gt;Computer Graphics Forum&lt;/i&gt;, vol. 42, no. 3, pp. 349–360, Jun. 2023.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    As visualization makes the leap to mobile and situated settings, where data is increasingly integrated with the physical world using mixed reality, there is a corresponding need for effectively managing the immersed user’s view of situated visualizations. In this paper we present an analysis of view management techniques for situated 3D visualizations in handheld augmented reality: a shadowbox, a world-in-miniature metaphor, and an interactive tour. We validate these view management solutions through a concrete implementation of all techniques within a situated visualization framework built using a web-based augmented reality visualization toolkit, and present results from a user study in augmented reality accessed using handheld mobile devices.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Batch-et-al-CGF-2023/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Batch-et-al-CGF-2023.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1111/cgf.14835&quot; class=&quot;doilink&quot;&gt;10.1111/cgf.14835&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at EG EuroVis 2023&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Batch-et-al-CGF-2023/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Journal paper accepted in Environmental Modelling & Software</title>
   <link href="http://localhost:4000/news/2022/07/21/publications/"/>
   <updated>2022-07-21T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2022/07/21/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/LUCSTc.png&quot; class=&quot;prjImgFlt&quot; alt=&quot;LUCST&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our paper &lt;a href=&quot;/publications#Rigby-et-al-EnvModSoft-2022&quot;&gt;“LUCST: A novel toolkit for Land Use Land Cover change assessment in SWAT+ to support flood management decisions”&lt;/a&gt; has been accepted for publication in Elsevier’s &lt;a href=&quot;https://www.sciencedirect.com/journal/environmental-modelling-and-software&quot;&gt;Environmental Modelling &amp;amp; Software&lt;/a&gt;. LUCST is an open-source, locally hosted, JavaScript application that allows for editing and interpreting &lt;a href=&quot;https://swat.tamu.edu/&quot;&gt;SWAT&lt;/a&gt;+ files relevant to land-use land-cover changes. LUCST employs visual analytics, to facilitate easier change detection in catchments, and enhances SWAT+ accessibility by reducing scenario modelling complexity.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Rigby-et-al-EnvModSoft-2022&quot;&gt;A. M. F. Rigby, P. W. S. Butcher, P. D. Ritsos, and S. D. Patil, “LUCST: A novel toolkit for Land Use Land Cover change assessment in SWAT+ to support flood management decisions,” &lt;i&gt;Environmental Modelling &amp;amp; Software&lt;/i&gt;, vol. 156, no. 105469, Aug. 2022.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Land Use Land Cover (LULC) change is widely recognised as one of the most important factors impacting the hydrological response of river basins. SWAT +, the latest version of the Soil and Water Assessment Tool, has been used extensively to assess the hydrological impacts of LULC change. However, the process of making and assessing such changes in SWAT+ is often cumbersome and non-intuitive, thereby reducing its usability amongst a wider pool of applied users. We address this issue by developing a user-friendly toolkit, Land Use Change SWAT+ Toolkit (LUCST), that will: (1) allow the end-user to define various LULC change scenarios in their study catchment, (2) run the SWAT+ model with the specified LULC changes, and (3) enable interactive visualisation of the different SWAT+ output variables. A good System Usability Score (79.8) and positive feedback from end-users promises the potential for adopting LUCST in future LULC change studies.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Rigby-et-al-EnvModSoft-2022/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Rigby-et-al-EnvModSoft-2022.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1016/j.envsoft.2022.105469&quot; class=&quot;doilink&quot;&gt;10.1016/j.envsoft.2022.105469&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Rigby-et-al-EnvModSoft-2022/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Journal papers accepted in Epidemics and in Phil. Trans. R. Soc. A.</title>
   <link href="http://localhost:4000/news/2022/04/25/publications/"/>
   <updated>2022-04-25T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2022/04/25/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/ramvisII_teaser.png&quot; alt=&quot;RAMPVIS Project pipeline&quot; style=&quot;margin: 0 auto; width: 8S00px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Two journal papers from the &lt;a href=&quot;/projects/RAMPVIS/&quot;&gt;RAMPVIS project&lt;/a&gt; have been accepted for publication in Elsevier’s &lt;a href=&quot;https://www.journals.elsevier.com/epidemics&quot;&gt;Epidemics&lt;/a&gt; and in the &lt;a href=&quot;https://royalsocietypublishing.org/journal/rsta&quot;&gt;Philosophical Transactions of the Royal Society A&lt;/a&gt; (Special issue on ’Technical challenges of modelling real-life epidemics and examples of overcoming these’). Bangor’s contribution in both works revolves around public engagement and disseminative visualisation for the Covid-19 pandemic.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Dykes-et-al-PhilTransA-2022&quot;&gt;J. Dykes, A. Abdul-Rahman, D. Archambault, B. Bach, R. Borgo, M. Chen, J. Enright, H. Fang, E. E. Firat, E. Freeman, T. Gönen, C. Harris, R. Jianu, N. W. John, S. Khan, A. Lahiff, R. S. Laramee, L. Matthews, S. Mohr, P. H. Nguyen, A. A. M. Rahat, R. Reeve, P. D. Ritsos, J. C. Roberts, A. Slingsby, B. Swallow, T. Torsney-Weir, C. Turkay, R. Turner, F. P. Vidal, Q. Wang, J. Wood, and K. Xu, “Visualization for Epidemiological Modelling: Challenges, Solutions, Reflections &amp;amp; Recommendations,” &lt;i&gt;Philosophical Transactions of the Royal Society A (Special issue on ’Technical challenges of modelling real-life epidemics and examples of overcoming these’) &lt;/i&gt;, vol. 380, no. 2233, p. 20210299, Aug. 2022.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We report on an ongoing collaboration between epidemiological modellers and visualization researchers by documenting and reflecting upon knowledge constructs - a series of ideas, approaches and methods taken from existing visualization research and practice – deployed and developed to support modelling of the COVID-19 pandemic. Structured independent commentary on these efforts is synthesized through iterative reflection to develop: evidence of the effectiveness and value of visualization in this context; open problems upon which the research communities may focus; guidance for future activity of this type; and recommendations to safeguard the achievements and promote, advance, secure and prepare for future collaborations of this kind. In describing and comparing a series of related projects that were undertaken in unprecedented conditions, our hope is that this unique report, and its rich interactive supplementary materials, will guide the scientific community in embracing visualization in its observation, analysis and modelling of data as well as in disseminating findings. Equally we hope to encourage the visualization community to engage with impactful science in addressing its emerging data challenges. If we are successful, this showcase of activity may stimulate mutually beneficial engagement between communities with complementary expertise to address problems of significance in epidemiology and beyond.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Dykes-et-al-PhilTransA-2022/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Dykes-et-al-PhilTransA-2022.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
   [&lt;a href=&quot;https://arxiv.org/abs/2204.06946&quot; class=&quot;doilink&quot;&gt;Preprint&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1098/rsta.2021.0299&quot; class=&quot;doilink&quot;&gt;10.1098/rsta.2021.0299&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Dykes-et-al-PhilTransA-2022/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Chen-et-al-Epidemics-2022&quot;&gt;M. Chen, A. Abdul-Rahman, D. Archambault, J. Dykes, A. Slingsby, P. D. Ritsos, T. Torsney-Weir, C. Turkay, B. Bach, R. Borgo, A. Brett, H. Fang, R. Jianu, S. Khan, R. S. Laramee, P. H. Nguyen, R. Reeve, J. C. Robert, F. Vidal, Q. Wang, J. Wood, and K. Xu, “RAMPVIS: Answering the Challenges of Building Visualisation Capabilities for Large-scale Emergency Responses,” &lt;i&gt;Epidemics&lt;/i&gt;, vol. 39, no. 100569, Jun. 2022.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    The effort for combating the COVID-19 pandemic around the world has resulted in a huge amount of data, e.g., from testing, contact tracing, modelling, treatment, vaccine trials, and more. In addition to numerous challenges in epidemiology, healthcare, biosciences, and social sciences, there has been an urgent need to develop and provide visualisation and visual analytics (VIS) capacities to support emergency responses under difficult operational conditions. In this paper, we report the experience of a group of VIS volunteers who have been working in a large research and development consortium and providing VIS support to various observational, analytical, model-developmental, and disseminative tasks. In particular, we describe our approaches to the challenges that we have encountered in requirements analysis, data acquisition, visual design, software design, system development, team organisation, and resource planning. By reflecting on our experience, we propose a set of recommendations as the first step towards a methodology for developing and providing rapid VIS capacities to support emergency responses.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Chen-et-al-Epidemics-2022/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Chen-et-al-Epidemics-2022.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
   [&lt;a href=&quot;https://arxiv.org/abs/2012.04757&quot; class=&quot;doilink&quot;&gt;Preprint&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1016/j.epidem.2022.100569&quot; class=&quot;doilink&quot;&gt;10.1016/j.epidem.2022.100569&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Chen-et-al-Epidemics-2022/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Delivered an XReality course at Airbus</title>
   <link href="http://localhost:4000/news/2022/03/08/teaching/"/>
   <updated>2022-03-08T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2022/03/08/teaching</id>
   <content type="html">&lt;p&gt;Between 15th of February and the 8th of March, we delivered, along with &lt;a href=&quot;https://www.bangor.ac.uk/staff/computer-science-electronic-engineering/peter-butcher-080986/en&quot;&gt;Dr Peter Butcher&lt;/a&gt;, a course on XReality, to &lt;a href=&quot;https://www.airbus.com/en&quot;&gt;Airbus&lt;/a&gt; employees. The course is part of the &lt;a href=&quot;https://dsp-centre.bangor.ac.uk/&quot;&gt;DSP Centre’s&lt;/a&gt; Educational College and was delivered within the &lt;a href=&quot;https://www.bangor.ac.uk/news/bangor-university-and-coleg-cambria-announce-a-new-collaborative-relationship&quot;&gt;Skills Factory programme&lt;/a&gt;, for which the &lt;a href=&quot;https://www.bangor.ac.uk/computer-science-and-electronic-engineering/index.php.en&quot;&gt;School of Computer Science and Electronic Engineering&lt;/a&gt;, &lt;a href=&quot;https://www.bangor.ac.uk/&quot;&gt;Bangor University&lt;/a&gt; is a primary contributor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/XR_Airbus.jpg&quot; alt=&quot;XReality course delivered at Airbus&quot; style=&quot;margin: 10 auto; width: 800px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The course provided and introduction to Virtual, Augmented and Mixed Reality, discussed some of the challenges of these paradigms, and presented some of our demos in &lt;a href=&quot;/projects/ia/&quot;&gt;Immersive Analytics&lt;/a&gt; (see image above, of the VRIA framework in MR, through the Microsoft HoloLens 2). The course was delivered over four sessions.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Sprint Project on Visual Analytics for Flood Modelling funded by WDNA</title>
   <link href="http://localhost:4000/news/2022/02/09/funding-updates/"/>
   <updated>2022-02-09T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2022/02/09/funding-updates</id>
   <content type="html">&lt;p&gt;We have received funding to extend our work on the development of a new decision support visual analytics toolkit, called LUCST (&lt;a href=&quot;/projects/LUCST/&quot;&gt;project page&lt;/a&gt;), as a Sprint Project from the &lt;a href=&quot;https://www.cardiff.ac.uk/__data/assets/pdf_file/0006/2520393/Wales-Data-Nation-Accelerator.pdf&quot;&gt;Wales Data Nation Accelerator&lt;/a&gt;. The toolkit is to help improve the planning and management of flood alleviation schemes in Gwynedd, and the funded extension is to integrate AI capabilities.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/LUCSTc.png&quot; alt=&quot;The LUCST toolkit developed by Bangor University and YCG&quot; style=&quot;margin: 0 auto; width: 500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This builds on work done in collaboration with the &lt;a href=&quot;https://www.bangor.ac.uk/natural-sciences/index.php.en&quot;&gt;School of Natural Sciences&lt;/a&gt; and Gwynedd Council’s consultancy arm &lt;a href=&quot;https://ygc.cymru/&quot;&gt;Ymgynghoriaeth Gwynedd Consultancy (YGC)&lt;/a&gt;. The work was funded by YGC and &lt;a href=&quot;http://kess2.ac.uk/&quot;&gt;Knowledge Economy Skills Scholarships (KESS 2)&lt;/a&gt;, a pan-Wales higher level skills initiative led by Bangor University on behalf of the HE sector in Wales. It is part funded by the Welsh Government’s European Social Fund (ESF) convergence programme for West Wales and the Valleys.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Journal paper in MDPI Computers</title>
   <link href="http://localhost:4000/news/2022/02/03/publications/"/>
   <updated>2022-02-03T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2022/02/03/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/oneViewNotEnough_teaser.png&quot; alt=&quot;Visualisation Data Flow for 3D&quot; style=&quot;margin: 0 auto; width: 8S00px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our journal article &lt;a href=&quot;/publications#Roberts-et-al-MDPI-2022&quot;&gt;“One View Is Not Enough: Review of and Encouragement for Multiple and Alternative Representations in 3D and Immersive Visualisation”&lt;/a&gt; has been accepted for publication in &lt;a href=&quot;https://www.mdpi.com/journal/computers/special_issues/CGVC_2021&quot;&gt;&lt;em&gt;MDPI Computers&lt;/em&gt;&lt;/a&gt;, in a special issue with invited extended publications from EG CGVC 2021.&lt;/p&gt;

&lt;p&gt;In this paper, using several case studies, we discuss the challenges and opportunities for using multiple views in immersive visualisation. Our aim is to provide a set of concepts that will enable developers to perform critical thinking, creative thinking and push the boundaries of what is possible with 3D and immersive visualisation.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-MDPI-2022&quot;&gt;J. C. Roberts, P. W. S. Butcher, and P. D. Ritsos, “One View Is Not Enough: Review of and Encouragement for Multiple and Alternative Representations in 3D and Immersive Visualisation,” &lt;i&gt;Computers&lt;/i&gt;, vol. 11, no. 2, Feb. 2022.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    The opportunities for 3D visualisations are huge. People can be immersed inside their data, interface with it in natural ways, and see it in ways that are not possible on a traditional desktop screen. Indeed, 3D visualisations, especially those that are immersed inside head-mounted displays are becoming popular. Much of this growth is driven by the availability, popularity and falling cost of head-mounted displays and other immersive technologies. However, there are also challenges. For example, data visualisation objects can be obscured, important facets missed (perhaps behind the viewer), and the interfaces may be unfamiliar. Some of these challenges are not unique to 3D immersive technologies. Indeed, developers of traditional 2D exploratory visualisation tools would use alternative views, across a multiple coordinated view (MCV) system. Coordinated view interfaces help users explore the richness of the data. For instance, an alphabetical list of people in one view shows everyone in the database, while a map view depicts where they live. Each view provides a different task or purpose. While it is possible to translate some desktop interface techniques into the 3D immersive world, it is not always clear what equivalences would be. In this paper, using several case studies, we discuss the challenges and opportunities for using multiple views in immersive visualisation. Our aim is to provide a set of concepts that will enable developers to perform critical thinking, creative thinking and push the boundaries of what is possible with 3D and immersive visualisation. In summary developers should consider how to integrate many views, techniques and presentation styles, and one view is not enough when using 3D and immersive visualisations.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-MDPI-2022/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-MDPI-2022.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.3390/computers11020020&quot; class=&quot;doilink&quot;&gt;10.3390/computers11020020&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-MDPI-2022/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Visual Analytics research on local press</title>
   <link href="http://localhost:4000/news/2021/12/22/media/"/>
   <updated>2021-12-22T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2021/12/22/media</id>
   <content type="html">&lt;p&gt;Our collaboration with the &lt;a href=&quot;https://www.bangor.ac.uk/natural-sciences/index.php.en&quot;&gt;School of Natural Sciences&lt;/a&gt; and Gwynedd Council’s consultancy arm &lt;a href=&quot;https://ygc.cymru/&quot;&gt;Ymgynghoriaeth Gwynedd Consultancy (YGC)&lt;/a&gt; on the development of a new decision support visual analytics toolkit (&lt;a href=&quot;/projects/LUCST/&quot;&gt;project page&lt;/a&gt;), that facilitates modelling the impact of land use change on river hydrology and provides valuable insights, has appeared in the &lt;a href=&quot;https://www.northwaleschronicle.co.uk/news/19802617.bangor-researchers-gwynedd-council-manage-flood-risk-using-visual-analytics/&quot;&gt;North Wales Chronicle&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The toolkit is to help improve the planning and management of flood alleviation schemes in Gwynedd.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/LUCSTc.png&quot; alt=&quot;The LUCST toolkit developed by Bangor University and YCG&quot; style=&quot;margin: 0 auto; width: 500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This research was funded by YGC and &lt;a href=&quot;http://kess2.ac.uk/&quot;&gt;Knowledge Economy Skills Scholarships (KESS 2)&lt;/a&gt;, a pan-Wales higher level skills initiative led by Bangor University on behalf of the HE sector in Wales. It is part funded by the Welsh Government’s European Social Fund (ESF) convergence programme for West Wales and the Valleys.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>DSP Centre receives 3M funding</title>
   <link href="http://localhost:4000/news/2021/12/15/funding-updates/"/>
   <updated>2021-12-15T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2021/12/15/funding-updates</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/dspBanner.jpg&quot; alt=&quot;Research at the DSP Centre of Excellence, in Bangor University&quot; style=&quot;margin: 0 auto; width: 960px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bangor University’s &lt;a href=&quot;http://dsp-centre.bangor.ac.uk/&quot;&gt;Digital Signal Processing (DSP) Centre of Excellence&lt;/a&gt; has secured £3 million to invest in cutting-edge equipment, which will develop the facility as well as create up to 40 new jobs. The DSP Centre is the first project funded from the North Wales Growth Deal, a £1 billion investment to the region’s economy, £240m of which is funded by the Welsh Government and UK Government. The project is recognised by both governments as being critical to the development and future of North Wales and the wider economy. More information can be found &lt;a href=&quot;https://www.bangor.ac.uk/news/major-milestone-for-north-wales-growth-deal-and-bangor-universitys-dsp-centre&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One aspect of the DSP Centre is the development of novel applications that make use of these ‘future networks’, and technologies such as edge computing, IoT, and XR.
This theme has strong synergies with the themes of &lt;a href=&quot;/projects/ia/&quot;&gt;Immersive Analytics&lt;/a&gt; and broader work on XR, such as medical training simulations.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>RAMPVIS project funded by EPSRC</title>
   <link href="http://localhost:4000/news/2021/01/01/funding-updates/"/>
   <updated>2021-01-01T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2021/01/01/funding-updates</id>
   <content type="html">&lt;p&gt;Our project_RAMP VIS: Making Visual Analytics an Integral Part of the Technological Infrastructure for Combating COVID-19_ (01/2020 - 12/2020) has been funderd by the &lt;a href=&quot;https://gtr.ukri.org/projects?ref=EP%2FV054236%2F1&quot;&gt;EPSRC funded project (EP/V054236/1)&lt;/a&gt;. This work is continuation of our volunteering work in the &lt;a href=&quot;https://epcced.github.io/ramp/&quot;&gt;Rapid Assistance in Modelling the Pandemic (RAMP)&lt;/a&gt; initiative. Bangor VIS experts, &lt;a href=&quot;https://sites.google.com/view/rampvis/home&quot;&gt;along with some twenty VIS scientists&lt;/a&gt;, researchers, and developers in the UK answered a call in May 2020 for VIS volunteers to support modelling scientists and epidemiologists in the &lt;a href=&quot;https://www.gla.ac.uk/research/az/scrc/&quot;&gt;Scottish COVID-19 Response Consortium (SCRC)&lt;/a&gt;, which is one of the three consortia in the Royal Society’s &lt;a href=&quot;https://epcced.github.io/ramp/&quot;&gt;RAMP)&lt;/a&gt; programme.&lt;/p&gt;

&lt;p&gt;The Bangor team is involved in efforts of visual analyics for modellers and epidemiologists (F. Vidal), public engagement through visualisation, data-driven storytelling and interfaces suitable for informing the public of the outputs of the modelling stages (J.C. Roberts, P.D. Ritsos), as well as more general data-driven guidance on COVID-19.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title> PhDs in AI, Immersive Analytics and Edge Computing, and Hydrology (UKRI CDT),</title>
   <link href="http://localhost:4000/news/2020/12/15/postgrad-opportunities/"/>
   <updated>2020-12-15T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2020/12/15/postgrad-opportunities</id>
   <content type="html">&lt;p&gt;We have some exciting PhD opprtunities, funded from our &lt;a href=&quot;http://cdt-aimlac.org/cdt-research.html&quot; target=&quot;\_blank&quot;&gt;UKRI Centre for Doctoral Training in Artificial Intelligence, Machine Learning &amp;amp; Advanced Computing&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Project title: &lt;em&gt;Edge-based object recognition for immersive analytics in Web-based XR&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Supervisors:&lt;a href=&quot;https://sopanpatil.weebly.com/&quot;&gt;Dr Panagiotis D. Ritsos&lt;/a&gt; and &lt;a href=&quot;https://www.bangor.ac.uk/computer-science-and-electronic-engineering/staff/jonathan-roberts/en&quot;&gt;Prof Jonathan C. Roberts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Department/Institution: &lt;a href=&quot;https://www.bangor.ac.uk/computer-science-and-electronic-engineering/&quot; target=&quot;\_blank&quot;&gt;School of Computer Science and Electronic Engineering, Bangor University&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are increasingly being immersed in a technology-mediated world, where the omni-presence of data introduces increased needs in mechanisms facilitating in-situ cognition, reasoning and sensemaking. In parallel, edge computing, facilitated by future networks, such as 5G, is transforming the way data is being processed and delivered from millions of devices around the world, bringing computing and analytics close to where the data is created. Building on these synergies, this project will investigate the use of edge-based object recognition using distributed neural networks (DNN), as a mechanism for in-situ registration and data processing for mobile, Web-based Immersive Analytics (IA) in Extended Reality (XR). Object-recognition can provide accurate and real-time registration, yet its practical application still faces important challenges. Current object-recognition systems are either self-contained, or cloud-based, yet face low latency and poor user experience respectively. Deep Learning, and DNNs, can provide effective solutions for object detection, and ameliorate these challenges. In addition, they have the potential to provide adaptive MR interfaces, and multimodal sensing capabilities useful for advanced IA experiences.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Project title: &lt;em&gt;Developing Artificial Intelligence Techniques to Improve Hydrological Model Regionalisation&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Supervisors:&lt;a href=&quot;https://sopanpatil.weebly.com/&quot;&gt;Dr Sopan Patil&lt;/a&gt; and Dr Panagiotis D. Ritsos&lt;/li&gt;
  &lt;li&gt;Department/Institution: &lt;a href=&quot;https://www.bangor.ac.uk/computer-science-and-electronic-engineering/&quot; target=&quot;\_blank&quot;&gt;School of Computer Science and Electronic Engineering, Bangor University&lt;/a&gt; and &lt;a href=&quot;https://www.bangor.ac.uk/natural-sciences/&quot; target=&quot;\_blank&quot;&gt;School of Natural Sciences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;project-description&quot;&gt;Project description:&lt;/h3&gt;

&lt;p&gt;Hydrological models are essential tools for simulating streamflow in river basins and are widely used for understanding, and forecasting, a riverâ€™s flood response to storm events. However, appropriate application of hydrological models requires a priori calibration of parameters using historical measured streamflow data. Previous research has shown that the relationship between hydrological model parameters and physical river basin properties (e.g., topography, soils, land use) is too complex to characterize using traditional statistical models. This limits the ability to determine how parameter values will modify if land use change alters the physical structure of a river basin. Recent advances in Artificial Intelligence (AI), specifically in Deep Learning (DL), have resulted in the ability to provide efficient high-dimensional interpolators that can handle data of multiple dimensions and heterogeneous information, such as those encountered in hydrological modelling. In this project, our goal is to develop AI techniques that can help improve the ability of hydrological models to predict the impact of land use change on river flood risk. Specifically, we propose a novel use of AI and information visualization to interactively relate hydrological model parameters to the physical properties of river basins. Our approach will involve development of DL techniques to extract high level abstractions in the hydrological model and physical river basin data, which can be used to test the impact of land management decisions on river flood risk. This abstraction will be made available to end-users via an interactive visualization interface to facilitate the flood risk investigation of multiple scenarios of land management changes (e.g., increase in urbanization by 10%). Our training dataset will include data from &amp;gt;1000 river basins across the UK, and the coupled AI-hydrological modelling workflow will be streamlined to operate on Supercomputing Wales High-Performance Computing (HPC) framework.&lt;/p&gt;

&lt;h3 id=&quot;more-info&quot;&gt;More Info:&lt;/h3&gt;

&lt;p&gt;More information on this exciting research can be found at &lt;a href=&quot;http://cdt-aimlac.org/cdt-research.html&quot; target=&quot;\_blank&quot;&gt;http://cdt-aimlac.org/cdt-research.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The deadline for applications is 12th February 2020&lt;/strong&gt;; however applications will be accepted until all positions are filled.&lt;/p&gt;

&lt;p&gt;For questions, please get in touch with &lt;a href=&quot;mailto:p.ritsos@bangor.ac.uk&quot;&gt;me&lt;/a&gt; or &lt;a href=&quot;mailto:s.d.patil@bangor.ac.uk&quot;&gt;Sopan&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>IEEE TVCG journal paper & a poster to be presented in IEEE VIS 2020</title>
   <link href="http://localhost:4000/news/2020/09/08/publications/"/>
   <updated>2020-09-08T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2020/09/08/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/vria_out_s.png&quot; alt=&quot;WebVR-based visualizations built with VRIA&quot; style=&quot;margin: 0 auto; width: 800px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our IEEE TVCG journal article &lt;a href=&quot;/publications#Butcher-et-al-TVCG-2020&quot;&gt;“VRIA: A Web-based Framework for Creating Immersive Analytics Experiences”&lt;/a&gt; will be presented at the virtual IEEE VIS 2020 &lt;a href=&quot;http://ieeevis.org/year/2020/welcome&quot;&gt;&lt;em&gt;IEEE VIS 2020&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&amp;#60;VRIA&amp;#62; is a Web-based framework, built for creating Immersive Analytics (IA) experiences in Virtual Reality. &amp;#60;VRIA&amp;#62; is built upon WebVR, A-Frame, React and D3.js, and offers a visualization creation workflow which enables users, of different levels of expertise, to rapidly develop Immersive Analytics experiences for the Web. More info on this paper can be found &lt;a href=&quot;/news/2020/01/04/publications/&quot;&gt;here&lt;/a&gt;, whereas the &amp;#60;VRIA&amp;#62; framework can be found on &lt;a href=&quot;https://github.com/vriajs&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/covidIA.png&quot; alt=&quot;WebVR-based Hapiness Data from Hedonometer.org&quot; style=&quot;margin: 0 auto; width: 800px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We will also be presenting a poster, titled &lt;a href=&quot;/publications#Williams-et-al-VIS-2020&quot;&gt;“Immersive visualisation of COVID-19 UK travel and US happiness data”&lt;/a&gt;, written with our undergraduate interns, Rhys Williams and Dan Farmer, along with Prof J.C. Roberts. The work was supported by the &lt;a href=&quot;https://www.bangor.ac.uk/skills-and-employability/index.php.en&quot;&gt;Bangor University Undergraduate Internship Scheme&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Butcher-et-al-TVCG-2021&quot;&gt;P. W. S. Butcher, N. W. John, and P. D. Ritsos, “VRIA: A Web-based Framework for Creating Immersive Analytics Experiences,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/i&gt;, vol. 27, no. 07, pp. 3213–3225, Jul. 2021.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We present &amp;#60;VRIA&amp;#62;, a Web-based framework for creating Immersive Analytics (IA) experiences in Virtual Reality. &amp;#60;VRIA&amp;#62; is built upon WebVR, A-Frame, React and D3.js, and offers a visualization creation workflow which enables users, of different levels of
  expertise, to rapidly develop Immersive Analytics experiences for the Web. The use of these open-standards Web-based technologies
  allows us to implement VR experiences in a browser and offers strong synergies with popular visualization libraries, through the HTML
  Document Object Model (DOM). This makes &amp;#60;VRIA&amp;#62; ubiquitous and platform-independent. Moreover, by using WebVR’s progressive
  enhancement, the experiences &amp;#60;VRIA&amp;#62; creates are accessible on a plethora of devices. We elaborate on our motivation for focusing on
  open-standards Web technologies, present the &amp;#60;VRIA&amp;#62; creation workflow and detail the underlying mechanics of our framework. We also
  report on techniques and optimizations necessary for implementing Immersive Analytics experiences on the Web, discuss scalability
  implications of our framework, and present a series of use case applications to demonstrate the various features of &amp;#60;VRIA&amp;#62;. Finally, we
  discuss current limitations of our framework, the lessons learned from its development, and outline further extensions.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Butcher-et-al-TVCG-2021/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Butcher-et-al-TVCG-2021.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/TVCG.2020.2965109&quot; class=&quot;doilink&quot;&gt;10.1109/TVCG.2020.2965109&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2020&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Butcher-et-al-TVCG-2021/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Williams-et-al-VIS-2020&quot;&gt;R. L. Williams, D. Farmer, J. C. Roberts, and P. D. Ritsos, “Immersive visualisation of COVID-19 UK travel and US happiness data,” in &lt;i&gt;Posters presented at the IEEE Conference on Visualization (IEEE VIS 2020), Virtual Event&lt;/i&gt;, 2020.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    The global COVID-19 pandemic has had great affect on the lives of everyone, from changing how children are educated to how or whether at all, we travel, go to work or do our shopping. Consequently, not only has people’s happiness changed throughout the pandemic, but there has been less vehicles on the roads. We present work to visualise both US happiness and UK travel data, as examples, in immersive environments. These impromptu visualisations encourage discussion and engagement with these topics, and can help people see the data in an alternative way.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Williams-et-al-VIS-2020/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Williams-et-al-VIS-2020.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Williams-et-al-VIS-2020/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Journal paper in MDPI Computers</title>
   <link href="http://localhost:4000/news/2020/03/06/publications/"/>
   <updated>2020-03-06T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2020/03/06/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/virtual_forest_s.png&quot; alt=&quot;Virtual Forests, created procedurally&quot; style=&quot;margin: 0 auto; width: 8S00px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our journal article &lt;a href=&quot;/publications#Williams-et-al-MDPI-2020&quot;&gt;“Virtual Forestry Generation: Evaluating Models for Tree Placement in Games”&lt;/a&gt; has been accepted for publication in &lt;a href=&quot;https://www.mdpi.com/journal/computers/special_issues/CGVC_2019&quot;&gt;&lt;em&gt;MDPI Computers&lt;/em&gt;&lt;/a&gt;, in a special issue with invited extended publications from EG CGVC 2019.&lt;/p&gt;

&lt;p&gt;In this article, we present a user study into virtual forests, using three different approaches of spatially distributing trees to approximate a plant community. These three approaches consisted of a random uniform distribution algorithm, a asymmetric plant competition model, and an iterative random distribution algorithm for creating clusters of trees. Our results demonstrate that the asymmetric plant competition model (the ‘Propagation’ algorithm) produces forests which were rated the highest in terms of playability and believability, for both 2D and 3D aerial perspectives.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Williams-et-al-MDPI-2020&quot;&gt;B. Williams, P. D. Ritsos, and C. Headleand, “Virtual Forestry Generation: Evaluating Models for Tree Placement in Games,” &lt;i&gt;Computers&lt;/i&gt;, vol. 9, no. 1, Mar. 2020.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    A handful of approaches have been previously proposed to generate procedurally virtual forestry for virtual worlds and computer games, including plant growthmodels and point distribution methods. However, there has been no evaluation to date which assesses how effective these algorithms are at modelling real-world phenomena. In this paper we tackle this issue by evaluating three algorithms used in the generation of virtual forests – a randomly uniform point distribution method (control), a plant competition model, and an iterative random point distribution technique.Our results show that a plant competition model generated more believable content when viewed from an aerial perspective. Interestingly however, we also found that a randomly uniform point distribution method produced forestry which was rated higher in playability and photorealism, when viewed from a first-person perspective. We conclude that the objective of the game designer is important to consider when selecting an algorithm to generate forestry, as the algorithms produce forestry which is perceived differently.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Williams-et-al-MDPI-2020/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Williams-et-al-MDPI-2020.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.3390/computers9010020&quot; class=&quot;doilink&quot;&gt;10.3390/computers9010020&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Williams-et-al-MDPI-2020/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Journal paper in IEEE TVCG</title>
   <link href="http://localhost:4000/news/2020/01/04/publications/"/>
   <updated>2020-01-04T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2020/01/04/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/vria_out_s.png&quot; alt=&quot;WebVR-based visualizations built with VRIA&quot; style=&quot;margin: 0 auto; width: 800px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our journal article &lt;a href=&quot;/publications#Butcher-et-al-TVCG-2020&quot;&gt;“VRIA: A Web-based Framework for Creating Immersive Analytics Experiences”&lt;/a&gt; has been accepted in &lt;a href=&quot;https://www.computer.org/web/tvcg&quot;&gt;&lt;em&gt;IEEE Transactions of Visualization and Computer Graphics (TVCG)&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&amp;#60;VRIA&amp;#62; is a framework for building Immersive Analytics solutions in VR, using standards-based Web-technologies. It is built using WebVR, A-Frame and React and the resulting VR solutions can be experienced through a WebVR-compliant browser on a variety of devices, ranging from smartphones to HMD-equipped desktop computers. &amp;#60;VRIA&amp;#62; uses a declarative format for specifying visualization types through simple configuration files, simplifying visualization prototyping, data binding and interaction configuration.&lt;/p&gt;

&lt;p&gt;&amp;#60;VRIA&amp;#62;’s visualization creation workflow (below, Figure 1.) provides different development paths for novice, intermediate and expert developers, and makes (optional) use of a dedicated visualization builder (show at the top), a Web-based interface that enables developers to easily prototype IA experiences and export their visualization configurations. These configurations can be further customized via the &amp;#60;VRIA&amp;#62; API to create new immersive depictions. Our paper presents a series of use cases that demonstrate the functionality and versatility of &amp;#60;VRIA&amp;#62;, including early extensions to MR space.&lt;/p&gt;

&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/assets/figures/process_v2-final.png&quot;&gt;&lt;img src=&quot;/assets/figures/process_v2-final.png&quot; alt=&quot;Visualization creation workflow demonstrating how &amp;#60;VRIA&amp;#62;  is suitable for users with different levels of programming experience. Novice users can upload datasets to the &amp;#60;VRIA&amp;#62;  builder tool and quickly produce immersive visualizations without coding. Intermediate users can use the builder to create a visualization config file or write one from scratch to use in their application. Finally, advanced users can make use of &amp;#60;VRIA&amp;#62; ’s API to develop additional features.&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: Visualization creation workflow demonstrating how &amp;#60;VRIA&amp;#62;  is suitable for users with different levels of programming experience. Novice users can upload datasets to the &amp;#60;VRIA&amp;#62;  builder tool and quickly produce immersive visualizations without coding. Intermediate users can use the builder to create a visualization config file or write one from scratch to use in their application. Finally, advanced users can make use of &amp;#60;VRIA&amp;#62; ’s API to develop additional features. [&lt;a href=&quot;/assets/figures/process_v2-final.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;More information on the VRIA framework can be found &lt;a href=&quot;https://github.com/vriajs&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Butcher-et-al-TVCG-2021&quot;&gt;P. W. S. Butcher, N. W. John, and P. D. Ritsos, “VRIA: A Web-based Framework for Creating Immersive Analytics Experiences,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/i&gt;, vol. 27, no. 07, pp. 3213–3225, Jul. 2021.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We present &amp;#60;VRIA&amp;#62;, a Web-based framework for creating Immersive Analytics (IA) experiences in Virtual Reality. &amp;#60;VRIA&amp;#62; is built upon WebVR, A-Frame, React and D3.js, and offers a visualization creation workflow which enables users, of different levels of
  expertise, to rapidly develop Immersive Analytics experiences for the Web. The use of these open-standards Web-based technologies
  allows us to implement VR experiences in a browser and offers strong synergies with popular visualization libraries, through the HTML
  Document Object Model (DOM). This makes &amp;#60;VRIA&amp;#62; ubiquitous and platform-independent. Moreover, by using WebVR’s progressive
  enhancement, the experiences &amp;#60;VRIA&amp;#62; creates are accessible on a plethora of devices. We elaborate on our motivation for focusing on
  open-standards Web technologies, present the &amp;#60;VRIA&amp;#62; creation workflow and detail the underlying mechanics of our framework. We also
  report on techniques and optimizations necessary for implementing Immersive Analytics experiences on the Web, discuss scalability
  implications of our framework, and present a series of use case applications to demonstrate the various features of &amp;#60;VRIA&amp;#62;. Finally, we
  discuss current limitations of our framework, the lessons learned from its development, and outline further extensions.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Butcher-et-al-TVCG-2021/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Butcher-et-al-TVCG-2021.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/TVCG.2020.2965109&quot; class=&quot;doilink&quot;&gt;10.1109/TVCG.2020.2965109&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2020&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Butcher-et-al-TVCG-2021/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>PhD opportunity in AI and Hydrology (UKRI CDT)</title>
   <link href="http://localhost:4000/news/2020/01/03/postgrad-opportunities-copy/"/>
   <updated>2020-01-03T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2020/01/03/postgrad-opportunities copy</id>
   <content type="html">&lt;p&gt;We have an exciting PhD opprtunity, funded from our &lt;a href=&quot;http://cdt-aimlac.org/cdt-research.html&quot; target=&quot;_blank&quot;&gt;UKRI Centre for Doctoral Training in Artificial Intelligence, Machine Learning &amp;amp; Advanced Computing&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Project title: Developing Artificial Intelligence Techniques to Improve Hydrological Model Regionalisation&lt;/li&gt;
  &lt;li&gt;Supervisors:&lt;a href=&quot;https://sopanpatil.weebly.com/&quot;&gt;Dr Sopan Patil&lt;/a&gt; and Dr Panagiotis Ritsos&lt;/li&gt;
  &lt;li&gt;Department/Institution: &lt;a href=&quot;https://www.bangor.ac.uk/computer-science-and-electronic-engineering/&quot; target=&quot;_blank&quot;&gt;School of Computer Science and Electronic Engineering, Bangor University&lt;/a&gt; and &lt;a href=&quot;https://www.bangor.ac.uk/natural-sciences/&quot; target=&quot;_blank&quot;&gt;School of Natural Sciences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;project-description&quot;&gt;Project description:&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;The focus of this PhD is to develop AI techniques that can help improve hydrological model regionalisation. Specifically, the research will investigate novel use of AI and information visualization to interactively relate hydrological model parameters to the physical properties of river basins. Hydrological models are essential tools for simulating streamflow in river basins and are widely used for forecasting floods and droughts. However, appropriate application of hydrological models requires a priori calibration of parameters using historical measured streamflow data. To make matters worse, previous research has shown that hydrological model parameters are not strongly correlated to the physical properties of river basins (e.g., topography, soils, land use). This limits the ability to regionalise hydrological models, i.e. estimate model parameters at ungauged river basins or modify parameter values if land use changes in a river basin. Recent advances in Artificial Intelligence (AI), specifically in Deep Learning, have resulted in the ability to provide efficient high-dimensional interpolators that can handle data of multiple dimensions and heterogeneous information, such as those encountered in hydrological modelling. Our approach will involve development of Deep Learning techniques to extract high level abstractions in hydrological models and physical river basin data, which can be used to test the impact of land management decisions on river basin hydrology. This abstraction will be made available to relevant stakeholders via an interactive visualization interface to facilitate the investigation of multiple hydrological and land-use change scenarios using interpolation, classification and, where possible, generalization. Our training dataset will include data from &amp;gt;1000 river basins across the UK, and the coupled AI-hydrological modelling workflow will be streamlined to operate on HPC framework. This research will advance the field of AI through application of novel techniques for hydrological model regionalisation and help improve the assessment of land management decisions on flood/drought risk.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;entry-requirements&quot;&gt;Entry Requirements:&lt;/h3&gt;

&lt;p&gt;Applicants should have at least a 2:1 degree in computer science, mathematics or electronic engineering (with substantial programming), or closely related discipline. Excellent programming skills and interest in AI, machine learning and advanced computing and one of the topics, above. Applicants should have an aptitude and ability in computational thinking and methods (as evidenced by your degree). Interviews will be conducted March 2020.&lt;/p&gt;

&lt;p&gt;We welcome applications by UK/Home and EU nationals. To qualify as a UK/Home applicant, you must have been resident in the UK for three years immediately prior to the start of the award, with no restrictions on how long you can remain in the UK. Residence in the UK that is solely for the purpose of education will only count towards these three years if the candidate is an EU national.&lt;/p&gt;

&lt;p&gt;More information on this exciting research can be found at &lt;a href=&quot;http://cdt-aimlac.org/cdt-research.html&quot; target=&quot;_blank&quot;&gt;http://cdt-aimlac.org/cdt-research.html&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;apply&quot;&gt;Apply:&lt;/h3&gt;

&lt;p&gt;Applications should be made through Bangor’s electronic application process at &lt;a href=&quot;https://apps.bangor.ac.uk/applicant/&quot;&gt;https://apps.bangor.ac.uk/applicant/&lt;/a&gt;. Your application must include the following attachments in pdf form:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Your CV&lt;/li&gt;
  &lt;li&gt;Degree certificates and transcripts (if you are still an undergraduate, provide a transcript of results known to date)&lt;/li&gt;
  &lt;li&gt;A statement no longer than 1000 words that explains why you want to join our Centre, and your preferred topic/supervisor.&lt;/li&gt;
  &lt;li&gt;Academic references - all scholarship applications require two supporting references to be submitted. Please ensure that your chosen referees are aware of the funding deadline, as their references form a vital part of the evaluation process. Please include these with your scholarship application.&lt;/li&gt;
  &lt;li&gt;In addition, email the pdf(s) of your application to &lt;a href=&quot;mailto:cdt-aimlac@swansea.ac.uk&quot;&gt;cdt-aimlac@swansea.ac.uk&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;The deadline for applications is 31 January 2020&lt;/strong&gt;; however applications will be accepted until all positions are filled.&lt;/p&gt;

&lt;p&gt;For more information, please get in touch with &lt;a href=&quot;mailto:p.ritsos@bangor.ac.uk&quot;&gt;me&lt;/a&gt; or &lt;a href=&quot;mailto:s.d.patil@bangor.ac.uk&quot;&gt;Sopan&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Journal paper in T&F Assessment & Evaluation in Higher Education</title>
   <link href="http://localhost:4000/news/2019/10/02/publications/"/>
   <updated>2019-10-02T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2019/10/02/publications</id>
   <content type="html">&lt;p&gt;Our publication &lt;a href=&quot;https://doi.org/10.1080/02602938.2019.1676397&quot;&gt;Visualizing the University Student Journey&lt;/a&gt; is published the journal of &lt;a href=&quot;https://www.tandfonline.com/toc/caeh20/current&quot;&gt;Assessment &amp;amp; Evaluation in Higher Education&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our article reports on the work that we have been involved in which combines the concept of Learning Pictures (Lindsley 1977, All 1977) with contemporary InfoVis techniques, such as Small Multiples, in order to provide enhanced, objective guidance to examiners when making adjustment decisions. Our goal is to improve the sense making process of examiners, during situations such as annual board of studies, where dozens, if not hundreds of students need to be evaluated in terms of their overall performance.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Gray-et-al-CAEH-2020&quot;&gt;C. C. Gray, D. Perkins, and P. D. Ritsos, “Degree Pictures: Visualizing the university student journey,” &lt;i&gt;Assessment &amp;amp; Evaluation in Higher Education&lt;/i&gt;, vol. 20, no. 4, pp. 568–578, Aug. 2020.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    The field of learning analytics is progressing at a rapid rate. New tools, with ever-increasing number of features and a plethora of datasets that are increasingly utilized demonstrate the evolution and multifaceted nature of the field. In particular, the depth and scope of insight that can be gleaned from analysing related datasets can have a significant, and positive, effect in educational practices. We introduce the concept of degree pictures, a symbolic overview of students’ achievement. Degree pictures are small visualizations that depict graphically 16 categories of overall student achievement, over the duration of a higher education course. They offer a quick summary of students’ achievement and are intended to initiate appropriate responses, such as teaching and pastoral interventions. This can address the subjective nature of assessment, by providing a method for educators to calibrate their own marking practices by showing an overview of any cohort. We present a prototype implementation of degree pictures, which was evaluated within our School of Computer Science, with favourable results.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Gray-et-al-CAEH-2020/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Gray-et-al-CAEH-2020.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1080/02602938.2019.1676397&quot; class=&quot;doilink&quot;&gt;10.1080/02602938.2019.1676397&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Gray-et-al-CAEH-2020/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Papers at EG CGVC 2019</title>
   <link href="http://localhost:4000/news/2019/07/31/publications/"/>
   <updated>2019-07-31T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2019/07/31/publications</id>
   <content type="html">&lt;p&gt;We presented one full paper and one short paper in the   &lt;a href=&quot;http://eguk.bangor.ac.uk/CGVC2019/&quot;&gt;&lt;em&gt;Eurographics Conference on Computer Graphics and Visual Computing 2019 (CGVC 2019)&lt;/em&gt;&lt;/a&gt;, held in Bangor, UK, on the 12th - 13th of September, 2019. The full paper received the Rob Fletcher Prize for Best Student Application Paper.&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Williams-et-al-CGVC-2019&quot;&gt;B. R. Williams, P. D. Ritsos, and C. Headleand, “Evaluating Models for Virtual Forestry Generation and Tree Placement in Games,” in &lt;i&gt;Proceedings of the Eurographics Conference in Computer Graphics and Visual Computing (CGVC) 2019, Bangor, UK&lt;/i&gt;, 2019.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    A handful of approaches have been previously proposed to generate procedurally virtual forestry for virtual worlds and computer games, including plant growth models and point distribution methods. However, there has been no evaluation to date which assesses how effective these algorithms are at modelling real-world phenomena. In this paper we tackle this issue by evaluating three algorithms used in the generation of virtual forests – a randomly uniform point distribution method (control), a plant competition model, and an iterative random point distribution technique. Our results show that a plant competition model generated more believable content when viewed from an aerial perspective. We also found that a randomly uniform point distribution method produced forest visualisations which were rated highest in playability and photorealism, when viewed from a first-person perspective. Our results indicate that when it comes to believability, the relationship between viewing perspective and procedural generation algorithm is more important than previously thought.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Williams-et-al-CGVC-2019/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Williams-et-al-CGVC-2019.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.2312/cgvc.20191259&quot; class=&quot;doilink&quot;&gt;10.2312/cgvc.20191259&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;&lt;i class=&quot;fa-solid fa-award&quot;&gt;&lt;/i&gt; &amp;nbsp; Best Student Paper&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Williams-et-al-CGVC-2019/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Jackson-et-al-CGVC-2019&quot;&gt;J. R. Jackson, P. D. Ritsos, and J. C. Roberts, “Towards a tool for the creation of micro-visualisations,” in &lt;i&gt;Proceedings of the Eurographics Conference in Computer Graphics and Visual Computing (CGVC) 2019, Bangor, UK&lt;/i&gt;, 2019.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    As the every day use of mobile and small screen devices becomes more common, it is necessary to explore how we can visualise data effectively in small design spaces. These screens are often used in situations where it is necessary to convey information in a concise, readable, reliable and visually appealing way. Our work focuses on the design and development of a tool to facilitate the creation and manipulation of new micro-visualisations. The results show that the tool is suitable for creating large number of outputs quickly and efficiently.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Jackson-et-al-CGVC-2019/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Jackson-et-al-CGVC-2019.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.2312/cgvc.20191270&quot; class=&quot;doilink&quot;&gt;10.2312/cgvc.20191270&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Jackson-et-al-CGVC-2019/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Extended Abstract at ACM CHI 2019 Late-Breaking Work</title>
   <link href="http://localhost:4000/news/2019/04/30/publications/"/>
   <updated>2019-04-30T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2019/04/30/publications</id>
   <content type="html">&lt;p&gt;We presented our work on the VRIA framework, as a Late Breaking work extended abstract in the &lt;a href=&quot;http://chi2019.acm.org/&quot;&gt;&lt;em&gt;ACM Conference on Human Factors in Computing Systems (CHI 2019)&lt;/em&gt;&lt;/a&gt;, held in Glasgow, UK, on the 4th - 9th of May, 2019.&lt;/p&gt;

&lt;p&gt;The work was a preamble, for our more recent journal work available &lt;a href=&quot;/news/2020/01/04/publications/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;More information on our work on Immersive Analytics can be found &lt;a href=&quot;/projects/ia/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;div class=&quot;bibliography&quot;&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Workshop papers at ACM CHI 2019</title>
   <link href="http://localhost:4000/news/2019/04/25/publications/"/>
   <updated>2019-04-25T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2019/04/25/publications</id>
   <content type="html">&lt;p&gt;We presented three workshop papers during the &lt;a href=&quot;http://chi2019.acm.org/&quot;&gt;&lt;em&gt;ACM Conference on Human Factors in Computing Systems (CHI 2019)&lt;/em&gt;&lt;/a&gt;, held in Glasgow, UK, on the 4th - 9th of May, 2019.&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Edwards-Ritsos-CMHCI-2019&quot;&gt;S. C. Edwards and P. D. Ritsos, “A Framework for Modelling Human Emotion,” in &lt;i&gt;Workshop on Computational Modeling in Human-Computer Interaction, CHI Conference on Human Factors in Computing Systems (ACM CHI 2019), Glasgow, UK&lt;/i&gt;, 2019.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    This paper describes the design of a modular framework, for constructing models of interacting systems. In particular, systems that can adapt and have different objectives; we also consider that these objectives could be of an emotional/hedonistic form. To that end, we introduce Pask’s conversation theory, and Boyd’s thoughts on decision making under uncertainty. In conclusion we describe modes of studying interacting systems.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Edwards-Ritsos-CMHCI-2019/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Edwards-Ritsos-CMHCI-2019.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Edwards-Ritsos-CMHCI-2019/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Mearman-et-al-CHI-2019&quot;&gt;J. W. Mearman, P. W. S. Butcher, P. D. Ritsos, and J. C. Roberts, “Tangible papercraft visualisations for education,” in &lt;i&gt;Workshop on Troubling Innovation: Craft and Computing Across Boundaries Workshop, CHI Conference on Human Factors in Computing Systems (ACM CHI 2019), Glasgow, UK&lt;/i&gt;, 2019.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We have been exploring how papercraft can be used to create ‘data physicalisations’ of student data, which act as physical artefacts and data sculptures that can be used in discussions. Papercrafting is cheap and quick to produce, and easily disposed of. Papercrafting student data is powerful as it acts as a focal point for discussions about the progression of their students and the effects of any extenuating circumstances. During such meetings teachers often reference spreadsheets and dashboard visualisations to explore the data. They focus and shift their attention to individual students, often commenting on individual performance and circumstances in turn. Tangible depictions, such as the ones we present, can be passed around, facilitating discussions, and can act as a focal-point for conversation. We present several prototypes and discuss our design process.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Mearman-et-al-CHI-2019/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Mearman-et-al-CHI-2019.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Mearman-et-al-CHI-2019/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-Ritsos-EduCHI-2019&quot;&gt;J. C. Roberts and P. D. Ritsos, “Critical Thinking Sheets: Encouraging critical thought and sketched implementation design,” in &lt;i&gt;EduCHI 2019 Symposium: Global Perspectives on HCI Education, CHI Conference on Human Factors in Computing Systems (ACM CHI 2019), Glasgow, UK&lt;/i&gt;, 2019.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Learners are often asked to create an interface as part of their course. For example, they could be asked to “create a calculator”, “develop a stopwatch” or “develop an image processing app”. But students often struggle to know how to start. At the same time, teachers want their students to think critically about their assignments and plan how they will build an interface. We have developed, and used for two academic years, a structured “critical thinking sheet (CTS)”. It is a method to help students consider a problem from different views, and help them critically consider different aspects of the task. The sheet gets the learners to (1) sketch the solution, (2) explain the challenge, (3) detail system components, (4) list algorithmic steps, and (5) explain next steps and issues of implementation. In this paper we introduce the sheet, explain how we have used it, and discuss learner experience.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-Ritsos-EduCHI-2019/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-Ritsos-EduCHI-2019.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-Ritsos-EduCHI-2019/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Posters and workshop paper at IEEE VIS 2018</title>
   <link href="http://localhost:4000/news/2018/08/23/publications/"/>
   <updated>2018-08-23T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2018/08/23/publications</id>
   <content type="html">&lt;p&gt;We presented the following two posters and a workshop paper, in the &lt;a href=&quot;http://ieeevis.org/year/2018/welcome&quot;&gt;&lt;em&gt;International Conference on Visualization (IEEE VIS 2018)&lt;/em&gt;&lt;/a&gt;, held in Berlin, Germany, on the 21th - 26th of October, 2018.&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Butcher-et-al-Poster-VIS2018&quot;&gt;P. W. S. Butcher, N. W. John, and P. D. Ritsos, “Towards a Framework for Immersive Analytics on the Web,” in &lt;i&gt;Posters presented at the IEEE Conference on Visualization (IEEE VIS 2018), Berlin, Germany&lt;/i&gt;, 2018.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We present work-in-progress on the design and implementation of a Web framework for building Immersive Analytics (IA) solutions in Virtual Reality (VR). We outline the design of our prototype framework, VRIA, which facilitates the development of VR spaces for IA solutions, which can be accessed via a Web browser. VRIA is built on emerging open-standards Web technologies such as WebVR, A-Frame and React, and supports a variety of interaction devices (e.g., smartphones, head-mounted displays etc.). We elaborate on our motivation for focusing on open-standards Web technologies and provide an overview of our framework. We also present two early visualization components. Finally, we outline further extensions and investigations.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Butcher-et-al-Poster-VIS2018/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Butcher-et-al-Poster-VIS2018.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Butcher-et-al-Poster-VIS2018/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Jackson-et-al-Poster-VIS2018&quot;&gt;J. Jackson, P. D. Ritsos, and J. C. Roberts, “Creating Small Unit Based Glyph Visualisations,” in &lt;i&gt;Posters presented at the IEEE Conference on Visualization (IEEE VIS 2018), Berlin, Germany&lt;/i&gt;, 2018.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Many modern day tasks involve the use of small screens, where users want to see a summary visualisation of an activity. For example, a runner using a smart watch needs to quickly view their progress, heart rate, comparison to previous races, etc. Subsequently, there is a need to portray data to users in small, yet well-defined, spaces. We define this space to be a single self-contained “unit”. In this paper we introduce a glyph visualisation algorithm that creates a diverse range of visualisation designs; each design contains many separate parts, whereupon different parameters can be mapped. Our algorithm uses a path based approach which allows designers to create deterministic, yet unique designs, in a unit space to display multivariate data.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Jackson-et-al-Poster-VIS2018/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Jackson-et-al-Poster-VIS2018.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Jackson-et-al-Poster-VIS2018/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Varghese-et-al-VISREG-2018&quot;&gt;D. Varghese, J. C. Roberts, and P. D. Ritsos, “Developing a formative visual feedback report for data brokering,” in &lt;i&gt;Workshop on Visual Summarization and Report Generation, IEEE Conference on Visualization (IEEE VIS 2018), Berlin, Germany&lt;/i&gt;, 2018.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We present the development of a visualisation framework, used to provide formative feedback to clients who engage with data brokering companies. Data brokers receive, clean, store and re-sell data from many clients. However the usage of the data and the brokering process can be improved at source by enhancing the client’s data creation and management processes. We propose to achieve this through providing formative feedback, as a visualisation report, to the client. Working closely with a travel agent data broker, we present a three-part framework, where we (1) evaluate data creation and provision processes of the client, (2) develop metrics for quantitative analytics on the data, (3) aggregate the analytics in a visual report.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Varghese-et-al-VISREG-2018/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Varghese-et-al-VISREG-2018.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Varghese-et-al-VISREG-2018/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Paper at the Fragile Earth Workshop (FEED) @ KDD'18</title>
   <link href="http://localhost:4000/news/2018/07/20/publications/"/>
   <updated>2018-07-20T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2018/07/20/publications</id>
   <content type="html">&lt;p&gt;We presented a workshop paper, below,  introducing the &lt;a href=&quot;/projects/Water4Cities/&quot;&gt;Water4Cities project&lt;/a&gt;, in the &lt;a href=&quot;https://ai4good.org/kdd-2018-workshop/&quot;&gt;&lt;em&gt;Fragile Earth Workshop (FEED) @ KDD’18&lt;/em&gt;&lt;/a&gt;, held in London, UK, on the 20th of August, 2018.&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Kenda-et-al-FEED-2018&quot;&gt;K. Kenda, S. Rizou, N. Mellios, D. Kofinas, P. D. Ritsos, M. Senozetnik, and C. Laspidou, “Smart Water Management for Cities,” in &lt;i&gt;Fragile Earth: Theory Guided Data Science to Enhance Scientific Discovery Workshop of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD2018)&lt;/i&gt;, 2018.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    The deployment of real-world water monitoring and analytics tools is still far behind the growing needs of cities, which are facing constant urbanisation and overgrowth of the population. This paper presents a full-stack data-mining infrastructure for smart water management for cities being developed within Water4Cities project. The stack is tested in two use cases - Greek island of Skiathos and Slovenian capital Ljubljana, each facing its own challenges related to groundwater. Bottom layer of the platform provides data gathering and provision infrastructure based on IoT standards. The layer is enriched with a dedicated missing data imputation infrastructure, which supports coherent analysis of long-term impacts of urbanisation and population growth on groundwater reserves. Data-driven approach to groundwater levels analysis, which is important for decision support in flood and groundwater management, has shown promising results and could replace or complement traditional process-driven models. Data visualization capabilities of the platform expose powerful synergies with data mining and contribute significantly to the design of future decision support systems in water management for cities.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Kenda-et-al-FEED-2018/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Kenda-et-al-FEED-2018.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Kenda-et-al-FEED-2018/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Paper at the 3rd EWaS International Conference</title>
   <link href="http://localhost:4000/news/2018/06/21/publications/"/>
   <updated>2018-06-21T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2018/06/21/publications</id>
   <content type="html">&lt;p&gt;We presented a conference publication, below, introducing the &lt;a href=&quot;/projects/Water4Cities/&quot;&gt;Water4Cities project&lt;/a&gt; in the &lt;a href=&quot;http://ewas3.civ.uth.gr/index.html&quot;&gt;&lt;em&gt;3rd Efficient Water Systems (EWaS) International Conference&lt;/em&gt;&lt;/a&gt; on “insights on the Water-Energy-Food Nexus”, held in Lefkada Island, Greece, from 27th to 30th of June 2018.&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Rizou-et-al-EWAS-2018&quot;&gt;S. Rizou, K. Kenda, D. Kofinas, N. M. Mellios, P. Pergar, P. D. Ritsos, J. Vardakas, K. Kalaboukas, C. Laspidou, M. Senožetnik, and A. Spyropoulou, “Water4Cities: An ICT platform enabling Holistic Surface Water and Groundwater Management for Sustainable Cities,” in &lt;i&gt;Proceedings of 3rd EWaS International Conference, Lefkada, Greece&lt;/i&gt;, 2018.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    To enable effective decision-making at the entire city level, both surface water and groundwater should be viewed as part of the extended urban water ecosystem with its spatiotemporal availability, quantity, quality and competing uses being taken into account. The Water4Cities project aims to build an ICT solution for the monitoring, visualization and analysis of urban water at a holistic urban setting to provide added-value decision support services to multiple water stakeholders. This paper presents the main stakeholders identified, the overall approach and the target use cases, where Water4Cities platform will be tested and validated.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Rizou-et-al-EWAS-2018/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Rizou-et-al-EWAS-2018.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Rizou-et-al-EWAS-2018/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;More information on Water4Cities can be found &lt;a href=&quot;http://www.water4cities.eu/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>IEEE TVCG paper invited to IEEE VR 2018</title>
   <link href="http://localhost:4000/news/2018/02/25/publications/"/>
   <updated>2018-02-25T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2018/02/25/publications</id>
   <content type="html">&lt;p&gt;One of our past &lt;a href=&quot;https://www.computer.org/web/tvcg&quot;&gt;&lt;em&gt;IEEE TVCG&lt;/em&gt;&lt;/a&gt;  articles was invited for presentation at &lt;a href=&quot;http://ieeevr.org/2018/&quot;&gt;IEEE VR 2018&lt;/a&gt;, held in Reutlingen, Germany.&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Papers at IEEE VIS 2017</title>
   <link href="http://localhost:4000/news/2017/08/01/publications/"/>
   <updated>2017-08-01T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2017/08/01/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/vis2017.jpg&quot; alt=&quot;Vis2017 Header&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We visited Phoenix, Arizona, in October 2017 for &lt;a href=&quot;http://ieeevis.org&quot;&gt;IEEE VIS2017&lt;/a&gt;, the premier forum for advances in scientific and information visualization. We presented the following:&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-TVCG-2018&quot;&gt;J. C. Roberts, P. D. Ritsos, J. Jackson, and C. Headleand, “The explanatory visualization framework: An active learning framework for teaching creative computing using explanatory visualizations,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/i&gt;, vol. 24, no. 1, pp. 791–801, Jan. 2018.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Visualizations are nowadays appearing in popular media and are used everyday in the workplace. This democratisation of visualization challenges educators to develop effective learning strategies, in order to train the next generation of creative visualization specialists. There is high demand for skilled individuals who can analyse a problem, consider alternative designs, develop new visualizations, and be creative and innovative. Our three-stage framework, leads the learner through a series of tasks, each designed to develop different skills necessary for coming up with creative, innovative, effective, and purposeful visualizations. For that, we get the learners to create an explanatory visualization of an algorithm of their choice. By making an algorithm choice, and by following an active-learning and project-based strategy, the learners take ownership of a particular visualization challenge. They become enthusiastic to develop good results and learn different creative skills on their learning journey.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-TVCG-2018/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-TVCG-2018.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/  10.1109/TVCG.2017.2745878&quot; class=&quot;doilink&quot;&gt;  10.1109/TVCG.2017.2745878&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2017&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-TVCG-2018/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-VIS2017-Tutorial&quot;&gt;J. C. Roberts, C. Headleand, and P. D. Ritsos, “Half-day Tutorial on Sketching Visualization designs, and using the Five Design-Sheet (FdS) Methodology in Teaching,” in &lt;i&gt;Tutorials of at the IEEE Conference on Visualization (IEEE VIS 2017), Phoenix, AZ, USA&lt;/i&gt;, 2017.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    This tutorial leads attendees through sketching designs following the Five Design-Sheet methodology (FdS) and discusses how it can be used in teaching. The first part (before the break) will introduce the FdS, place it in context with other methods, discuss creative thinking and different problem types, explain the benefit of sketching designs, and provide a worked example of the FdS. The second part (after the break) focuses on using the FdS in teaching in Higher Education We give examples of students’ work, and discuss issues and challenges of using sketching for designing and prototyping in teaching, followed by a question and answer session.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-VIS2017-Tutorial/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-VIS2017-Tutorial.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-VIS2017-Tutorial/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Ritsos-et-al-Poster-VIS2017&quot;&gt;P. D. Ritsos, J. Jackson, and J. C. Roberts, “Web-based Immersive Analytics in Handheld Augmented Reality,” in &lt;i&gt;Posters presented at the IEEE Conference on Visualization (IEEE VIS 2017), Phoenix, Arizona, USA&lt;/i&gt;, 2017.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    The recent popularity of virtual reality (VR), and the emergence of a number of affordable VR interfaces, have prompted researchers and developers to explore new, immersive ways to visualize data. This has resulted in a new research thrust, known as Immersive Analytics (IA). However, in IA little attention has been given to the paradigms of augmented/mixed reality (AR/MR), where computer-generated and physical objects co-exist. In this work, we explore the use of contemporary web-based technologies for the creation of immersive visualizations for handheld AR, combining D3.js with the open standards-based Argon AR framework and A-frame/WebVR. We argue in favor of using emerging standards-based web technologies as they work well with contemporary visualization tools, that are purposefully built for data binding and manipulation.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Ritsos-et-al-Poster-VIS2017/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Ritsos-et-al-Poster-VIS2017.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Ritsos-et-al-Poster-VIS2017/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Ritsos-et-al-IAW-2017&quot;&gt;P. D. Ritsos, J. Mearman, J. R. Jackson, and J. C. Roberts, “Synthetic Visualizations in Web-based Mixed Reality,” in &lt;i&gt;Immersive Analytics: Exploring Future Visualization and Interaction Technologies for Data Analytics Workshop, IEEE Conference on Visualization (VIS), Phoenix, Arizona, USA&lt;/i&gt;, 2017.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    The way we interact with computers is constantly evolving, with technologies like Mixed/Augmented Reality (MR/AR) and the Internet of Things (IoT) set to change our perception of informational and physical space. In parallel, interest for interacting with data in new ways is driving the investigation of the synergy of these domains with data visualization. We are seeking new ways to contextualize, visualize, interact-with and interpret our data. In this paper we present the notion of Synthetic Visualizations, which enable us to visualize in situ, data embedded in physical objects, using MR. We use a combination of established ‘markers’, such as Quick Response Codes (QR Codes) and Augmented Reality Markers (AR Markers), not only to register objects in physical space, but also to contain data to be visualized, and interchange the type of visualization to be used. We visualize said data in Mixed Reality (MR), using emerging web-technologies and open-standards.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Ritsos-et-al-IAW-2017/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Ritsos-et-al-IAW-2017.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Ritsos-et-al-IAW-2017/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-PDVW-2017&quot;&gt;J. C. Roberts, P. D. Ritsos, and C. Headleand, “Experience and Guidance for the use of Sketching and low-fidelity Visualisation-design in teaching,” in &lt;i&gt;Pedagogy of Data Visualization Workshop, IEEE Conference on Visualization (VIS), Phoenix, Arizona, USA&lt;/i&gt;, 2017.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We, like other educators, are keen to develop the next generation of visualisation designers. The use of sketching and low-fidelity designs are becoming popular methods to help developers and students consider many alternative ideas and plan what they should build. But especially within an education setting, there are often many issues that challenge students as they create low-fidelity prototypes. Students can be unwilling to contemplate alternatives, reluctant to use pens and paper, or sketch on paper, and inclined to code the first idea in their mind. In this paper we discuss these issues, and investigate strategies to help increase the breadth of low-fidelity designs, especially for developing data-visualisation tools. We draw together experiences and advice of how we have used the Five Design-Sheets method over eight years, for different assessment styles and across two institutions. This paper would be useful for anyone who wishes to use sketching in their teaching, or to improve their own experiences.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-PDVW-2017/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-PDVW-2017.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-PDVW-2017/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Pereda-et-al-VIS4DH-2017&quot;&gt;J. Pereda, P. Murietta-Flores, P. D. Ritsos, and J. C. Roberts, “Tangible User Interfaces as a Pathway for Information Visualisation for Low Digital Literacy in the Digital Humanities,” in &lt;i&gt;2nd Workshop on Visualization for the Digital Humanities, IEEE Conference on Visualization (VIS), Phoenix, Arizona, USA&lt;/i&gt;, 2017.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Information visualisation has become a key element for empowering users to answer and produce new questions, make sense and create narratives about specific sets of information. Current technologies, such as Linked Data, have changed how researchers and professionals in the Humanities and the Heritage sector engage with information. Digital literacy is of concern in many sectors, but is especially of concern for Digital Humanities. This is due to the fact that the Humanities and Heritage sector face an important division based on digital literacy that produce gaps in the way research can be carried out. One way to overcome the challenge of digital literacy and improve access to information can be Tangible User Interfaces (TUIs), which allow a more meaningful and natural pathway for a wide range of users. TUIs make use of physical objects to interact with the computer. In particular, they can facilitate the interaction process between the user and a data visualisation system. This position paper discusses the opportunity to engage with Digital Humanities information via TUIs and data visualisation tools, offering new ways to analyse, investigate and interpret the past.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Pereda-et-al-VIS4DH-2017/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Pereda-et-al-VIS4DH-2017.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Pereda-et-al-VIS4DH-2017/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Short paper at Cyberworlds 2017</title>
   <link href="http://localhost:4000/news/2017/07/30/publications/"/>
   <updated>2017-07-30T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2017/07/30/publications</id>
   <content type="html">&lt;p&gt;We presented a short paper in Cyberworlds 2017, held in Chester, UK, in September. More information about the conference can be found &lt;a href=&quot;http://cw2017.org/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Butcher-Ritsos-CW2017&quot;&gt;P. W. Butcher and P. D. Ritsos, “Building Immersive Data Visualizations for the Web,” in &lt;i&gt;Proceedings of International Conference on Cyberworlds (CW’17), Chester, UK&lt;/i&gt;, 2017.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We present our early work on building prototype applications for Immersive Analytics using emerging standardsbased web technologies for VR. For our preliminary investigations we visualize 3D bar charts that attempt to resemble recent physical visualizations built in the visualization community. We explore some of the challenges faced by developers in working with emerging VR tools for the web, and in building effective and informative immersive 3D visualizations.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Butcher-Ritsos-CW2017/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Butcher-Ritsos-CW2017.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/CW.2017.11&quot; class=&quot;doilink&quot;&gt;10.1109/CW.2017.11&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Butcher-Ritsos-CW2017/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Journal paper at IEEE TVCG / VIS 2017</title>
   <link href="http://localhost:4000/news/2017/07/25/publications/"/>
   <updated>2017-07-25T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2017/07/25/publications</id>
   <content type="html">&lt;p&gt;Our publication &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7192707&quot;&gt;The explanatory visualization framework: An active learning framework for teaching creative computing using explanatory visualizations&lt;/a&gt; was presented in &lt;em&gt;IEEE VIS2017&lt;/em&gt;, held in Phoenix, AZ, USA.&lt;/p&gt;

&lt;p&gt;While visualization techniques are starting to pervade our work and leisure, we feel that there are many opportunities to use explanatoryvisualizations in teaching and learning. The Explanatory Visualization Framework (EVF) guides a learner to think through an explanatory visualization task, consider alternative solutions and reflect on their design, implementation choices, and actions. The framework is designed to provide a good balance between fostering creative thinking and providing the structured guidance that students need. It also enables teachers to swap-in their favoured exercise or assessment and apply the model to their situation. By creating their own explanatory visualizations, students learn and develop their creative, reflection and communication skills.&lt;/p&gt;

&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/assets/figures/evf3.png&quot;&gt;&lt;img src=&quot;/assets/figures/evf3.png&quot; alt=&quot;EVF - The explanatory visualization framework&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: EVF - The explanatory visualization framework [&lt;a href=&quot;/assets/figures/evf3.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-TVCG-2018&quot;&gt;J. C. Roberts, P. D. Ritsos, J. Jackson, and C. Headleand, “The explanatory visualization framework: An active learning framework for teaching creative computing using explanatory visualizations,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/i&gt;, vol. 24, no. 1, pp. 791–801, Jan. 2018.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Visualizations are nowadays appearing in popular media and are used everyday in the workplace. This democratisation of visualization challenges educators to develop effective learning strategies, in order to train the next generation of creative visualization specialists. There is high demand for skilled individuals who can analyse a problem, consider alternative designs, develop new visualizations, and be creative and innovative. Our three-stage framework, leads the learner through a series of tasks, each designed to develop different skills necessary for coming up with creative, innovative, effective, and purposeful visualizations. For that, we get the learners to create an explanatory visualization of an algorithm of their choice. By making an algorithm choice, and by following an active-learning and project-based strategy, the learners take ownership of a particular visualization challenge. They become enthusiastic to develop good results and learn different creative skills on their learning journey.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-TVCG-2018/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-TVCG-2018.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/  10.1109/TVCG.2017.2745878&quot; class=&quot;doilink&quot;&gt;  10.1109/TVCG.2017.2745878&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2017&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-TVCG-2018/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Journal paper in IEEE TVCG</title>
   <link href="http://localhost:4000/news/2017/04/30/publications/"/>
   <updated>2017-04-30T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2017/04/30/publications</id>
   <content type="html">&lt;p&gt;Our journal article &lt;a href=&quot;/publications#John-et-al-TVCG-2017&quot;&gt;“The Implementation &amp;amp; Validation of a Virtual Environment for Training Powered Wheelchair Manoeuvres”&lt;/a&gt; has been accepted in &lt;a href=&quot;https://www.computer.org/web/tvcg&quot;&gt;&lt;em&gt;IEEE Transactions of Visualization and Computer Graphics (TVCG)&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We present the utility of a cost-effective virtual environment for powered wheelchair training, built from commercial off-the-shelf components. Our  indings support our hypothesis that a serious game (virtual environment) utilising the Oculus Rift HMD can provide a safe environment in which a new user of a powered wheelchair can quickly learn driving skills. Our results indicate that: a) the learning done in the HMD-based VR environment was transferred to the physical world; and b) there is a benefit in using a VR-HMD simulator for training wheelchair users, in terms of navigation performance improvement. Coupling that with the safety that users felt and overall confidence the HMD version provided, there are clear benefits in using HMD-based VR for training wheelchair users.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;div class=&quot;bibliography&quot;&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Papers at IEEE VIS 2016</title>
   <link href="http://localhost:4000/news/2016/10/30/publications/"/>
   <updated>2016-10-30T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2016/10/30/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/Vis16-header.jpg&quot; alt=&quot;Vis2016 Header&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We visited Baltimore, MD in October 2016 for &lt;em&gt;IEEE VIS2016&lt;/em&gt;, the premier forum for advances in scientific and information visualization. This week-long event convenes an international community of researchers and practitioners from academia, government, and industry to explore their shared interests in tools, techniques, and technology.&lt;/p&gt;

&lt;p&gt;We presented the following:&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-VIS2016-Tutorial&quot;&gt;J. C. Roberts, C. Headleand, and P. D. Ritsos, “Sketching Designs for Data-Visualization using the Five Design-Sheet Methodology,” in &lt;i&gt;Tutorials of at the IEEE Conference on Visualization (IEEE VIS 2016), Baltimore, MD, USA&lt;/i&gt;, 2016.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    The tutorial will be useful for anyone who has to create visualization interfaces, and needs to think through different potential ways to display their data. At the end of the tutorial participants will understand techniques to help them be more structured in their ideation. They will be able to sketch interface designs using the Five Design Sheet methodology (FdS). While we know that some developers have started to use the Five Design-Sheet methodology, but this tutorial will start from the beginning and be suitable for any attendee. More information and resources are found on http://fds.design.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-VIS2016-Tutorial/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-VIS2016-Tutorial.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-VIS2016-Tutorial/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-Viz4DH-VIS2016&quot;&gt;J. C. Roberts, J. W. Mearman, P. D. Ritsos, H. C. Miles, A. T. Wilson, D. Perkins, J. R. Jackson, B. Tiddeman, F. Labrosse, B. Edwards, and R. Karl, “Immersive Analytics and Deep Maps – the Next Big Thing for Cultural Heritage &amp;amp; Archaeology,” in &lt;i&gt;Visualization for Digital Humanities Workshop, IEEE Conference on Visualization (VIS), Baltimore, MD, USA&lt;/i&gt;, 2016.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Archaeologists and cultural heritage experts explore complex multifaceted data that is often highly interconnected. We argue for new ways to interact with this data. Such data analysis provides a ‘grand challenge’ for computer science and heritage researchers, it is big Data, multi-dimensional, multi-typed, contains uncertain information, and the questions posed by researchers are often ill-defined (where it is difficult to guarantee an answer). We present two visions (Immersive Analytics, and Deep Mapping) as solutions to allow both expert users and the general public to interact and explore heritage data. We use pre-historic data as a case study, and discuss key technologies that need to develop further, to help accomplish these two visions.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-Viz4DH-VIS2016/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-Viz4DH-VIS2016.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-Viz4DH-VIS2016/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Butcher-et-al-Poster-VIS2016&quot;&gt;P. W. S. Butcher, J. C. Roberts, and P. D. Ritsos, “Immersive Analytics with WebVR and Google Cardboard,” in &lt;i&gt;Posters presented at the IEEE Conference on Visualization (IEEE VIS 2016), Baltimore, MD, USA&lt;/i&gt;, 2016.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We present our initial investigation of a low-cost, web-based virtual reality platform for immersive analytics, using a Google Cardboard, with a view of extending to other similar platforms such as Samsung’s Gear VR. Our prototype uses standards-based emerging frameworks, such as WebVR and explores some the challenges faced by developers in building effective and informative immersive 3D visualizations, particularly those that attempt to resemble recent physical visualizations built in the community.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Butcher-et-al-Poster-VIS2016/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Butcher-et-al-Poster-VIS2016.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Butcher-et-al-Poster-VIS2016/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-Poster-VIS2016&quot;&gt;J. C. Roberts, J. Jackson, C. Headleand, and P. D. Ritsos, “Creating Explanatory Visualizations of Algorithms for Active Learning,” in &lt;i&gt;Posters presented at the IEEE Conference on Visualization (IEEE VIS 2016), Baltimore, MD, USA&lt;/i&gt;, 2016.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Visualizations have been used to explain algorithms to learners, in order to help them understand complex processes. These ‘explanatory visualizations’ can help learners understand computer algorithms and data-structures. But most are created by an educator and merely watched by the learner. In this paper, we explain how we get learners to plan and develop their own explanatory visualizations of algorithms. By actively developing their own visualizations learners gain a deeper insight of the algorithms that they are explaining. These depictions can also help other learners understand the algorithm.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-Poster-VIS2016/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-Poster-VIS2016.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-Poster-VIS2016/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;You can find more information on the conference at &lt;a href=&quot;http://ieeevis.org/year/2016/info/vis-welcome/welcome&quot;&gt;IEEE VIS2016&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper at MMVR 22</title>
   <link href="http://localhost:4000/news/2016/05/01/publications/"/>
   <updated>2016-05-01T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2016/05/01/publications</id>
   <content type="html">&lt;p&gt;We presented a conference paper at &lt;a href=&quot;http://www.nextmed.com/&quot;&gt;Medicine Meets Virtual Reality Conference (MMVR22)&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Headleand-et-al-MMVR2016&quot;&gt;C. J. Headleand, T. Day, S. R. Pop, P. D. Ritsos, and N. W. John, “A Cost-Effective Virtual Environment for Simulating and Training Powered Wheelchairs Manoeuvres,” &lt;i&gt;Proceedings of NextMed/MMVR22, Los Angeles, USA&lt;/i&gt;, 2016.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Control of a powered wheelchair is often not intuitive, making training of new users a challenging and sometimes hazardous task. Collisions, due to a lack of experience can result in injury for the user and other individuals. By conducting training activities in virtual reality (VR), we can potentially improve driving skills whilst avoiding the risks inherent to the real world. However, until recently VR technology has been expensive and limited the commercial feasibility of a general training solution. We describe Wheelchair-Rift, a cost effective prototype simulator that makes use of the Oculus Rift head mounted display and the Leap Motion hand tracking device. It has been assessed for face validity by a panel of experts from a local Posture and Mobility Service. Initial results augur well for our cost-effective training solution.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Headleand-et-al-MMVR2016/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Headleand-et-al-MMVR2016.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  [PMID:&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/27046566&quot; class=&quot;doilink&quot;&gt;27046566&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Headleand-et-al-MMVR2016/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Journal paper in ACM Journal on Computing and Cultural Heritage (JOCCH)</title>
   <link href="http://localhost:4000/news/2015/11/21/publications/"/>
   <updated>2015-11-21T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2015/11/21/publications</id>
   <content type="html">&lt;p&gt;A journal paper, in which we have contributed the design of a Handheld Augmented Reality application for cultural heritage was published in ACM Journal on Computing and Cultural Heritage (JOCCH). The paper provides an overview of the &lt;em&gt;Heritage Together&lt;/em&gt; project, lead by Bangor University.&lt;/p&gt;

&lt;p&gt;More information can be obtained from the projects website, &lt;a href=&quot;http://heritagetogether.org/?lang=en&quot;&gt;HeritageTogether.org&lt;/a&gt; or the project’s twitter page &lt;a href=&quot;https://twitter.com/heritage2gether&quot;&gt;@heritagetogether&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Miles-et-al-JOCCH-2016&quot;&gt;H. C. Miles, A. T. Wilson, F. Labrosse, B. Tiddeman, S. Griffiths, B. Edwards, P. D. Ritsos, J. W. Mearman, K. Möller, R. Karl, and J. C. Roberts, “Alternative Representations of 3D-Reconstructed Heritage Data,” &lt;i&gt;ACM Journal on Computing and Cultural Heritage (JOCCH)&lt;/i&gt;, vol. 9, no. 1, pp. 4:1–4:18, Nov. 2015.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    By collecting images of heritage assets from members of the public and processing them to create 3D-reconstructed models, the HeritageTogether project has accomplished the digital recording of nearly 80 sites across Wales, UK. A large amount of data has been collected and produced in the form of photographs, 3D models, maps, condition reports, and more. Here we discuss some of the different methods used to realize the potential of this data in different formats and for different purposes. The data are explored in both virtual and tangible settings, and—with the use of a touch table—a combination of both. We examine some alternative representations of this community-produced heritage data for educational, research, and public engagement applications.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Miles-et-al-JOCCH-2016/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Miles-et-al-JOCCH-2016.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1145/2795233&quot; class=&quot;doilink&quot;&gt;10.1145/2795233&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Miles-et-al-JOCCH-2016/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Papers at IEEE VIS 2015</title>
   <link href="http://localhost:4000/news/2015/10/20/publications/"/>
   <updated>2015-10-20T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2015/10/20/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/Vis15-header.png&quot; alt=&quot;Vis2015 Header&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We visited Chicago, IL in October 2015 for &lt;em&gt;IEEE VIS2015&lt;/em&gt;, the premier forum for advances in scientific and information visualization. This week-long event convenes an international community of researchers and practitioners from academia, government, and industry to explore their shared interests in tools, techniques, and technology.&lt;/p&gt;

&lt;p&gt;We presented the following:&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-TVCG-2016&quot;&gt;J. C. Roberts, C. Headleand, and P. D. Ritsos, “Sketching Designs Using the Five Design-Sheet Methodology,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/i&gt;, vol. 22, no. 1, pp. 419–428, Jan. 2016.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Sketching designs has been shown to be a useful way of planning and considering alternative solutions. The use of lo-fidelity prototyping, especially paper-based sketching, can save time, money and converge to better solutions more quickly. However, this design process is often viewed to be too informal. Consequently users do not know how to manage their thoughts and ideas (to first think divergently, to then finally converge on a suitable solution). We present the Five Design Sheet (FdS) methodology. The methodology enables users to create information visualization interfaces through lo-fidelity methods. Users sketch and plan their ideas, helping them express different possibilities, think through these ideas to consider their potential effectiveness as solutions to the task (sheet 1); they create three principle designs (sheets 2,3 and 4); before converging on a final realization design that can then be implemented (sheet 5). In this article, we present (i) a review of the use of sketching as a planning method for visualization and the benefits of sketching, (ii) a detailed description of the Five Design Sheet (FdS) methodology, and (iii) an evaluation of the FdS using the System Usability Scale, along with a case-study of its use in industry and experience of its use in teaching.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-TVCG-2016/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-TVCG-2016.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/TVCG.2015.2467271&quot; class=&quot;doilink&quot;&gt;10.1109/TVCG.2015.2467271&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2015&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-TVCG-2016/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-CGA2014&quot;&gt;J. C. Roberts, P. D. Ritsos, S. K. Badam, D. Brodbeck, J. Kennedy, and N. Elmqvist, “Visualization Beyond the Desktop - the next big thing,” &lt;i&gt;IEEE Computer Graphics and Applications&lt;/i&gt;, vol. 34, no. 6, pp. 26–34, Nov. 2014.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Visualization is coming of age. With visual depictions being seamlessly integrated into documents, and data visualization techniques being used to understand increasingly large and complex datasets, the term &quot;visualization&quot;’ is becoming used in everyday conversations. But we are on a cusp; visualization researchers need to develop and adapt to today’s new devices and tomorrow’s technology. Today, people interact with visual depictions through a mouse. Tomorrow, they’ll be touching, swiping, grasping, feeling, hearing, smelling, and even tasting data. The next big thing is multisensory visualization that goes beyond the desktop.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-CGA2014/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-CGA2014.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/MCG.2014.82&quot; class=&quot;doilink&quot;&gt;10.1109/MCG.2014.82&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2015&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-CGA2014/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Gray-et-al-VizSec2015&quot;&gt;C. C. Gray, P. D. Ritsos, and J. C. Roberts, “Contextual Network Navigation; Situational Awareness for Network Administrators,” in &lt;i&gt;IEEE Symposium on Visualization for Cyber Security (VizSec), Chicago, IL, USA&lt;/i&gt;, 2015.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    One of the goals of network administrators is to identify and block sources of attacks from a network steam. Various tools have been developed to help the administrator identify the IP or subnet to be blocked, however these tend to be non-visual. Having a good perception of the wider network can aid the administrator identify their origin, but while network maps of the Internet can be useful for such endeavors, they are difficult to construct, comprehend and even utilize in an attack, and are often referred to as being “hairballs”. We present a visualization technique that displays pathways back to the attacker; we include all potential routing paths with a best-efforts identification of the commercial relationships involved. These two techniques can potentially highlight common pathways and/or networks to allow faster, more complete resolution to the incident, as well as fragile or incomplete routing pathways to/from a network. They can help administrators re-profile their choice of IP transit suppliers to better serve a target audience.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Gray-et-al-VizSec2015/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Gray-et-al-VizSec2015.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/VIZSEC.2015.7312769&quot; class=&quot;doilink&quot;&gt;10.1109/VIZSEC.2015.7312769&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Gray-et-al-VizSec2015/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-Viz4me-VIS2015&quot;&gt;J. C. Roberts, C. Headleand, D. Perkins, and P. D. Ritsos, “Personal Visualisation for Learning,” in &lt;i&gt;Personal Visualization: Exploring Data in Everyday Life Workshop, IEEE Conference on Visualization (VIS), Chicago, IL, USA&lt;/i&gt;, 2015.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Learners have personal data, such as grades, feedback and statistics on how they fair or compare with the class. But, data focusing on their personal learning is lacking, as it does not get updated regularly (being updated at the end of a taught session) and the displayed information is generally a single grade. Consequently, it is difficult for students to use this information to adapt their behavior, and help them on their learning journey. Yet, there is a rich set of data that could be captured and help students learn better. What is required is dynamically, regularly updated personal data, that is displayed to students in a timely way. Such ‘personal data’ can be presented to the student through ‘personal visualizations’ that engender ‘personal learning’. In this paper we discuss our journey into developing learning systems and our resulting experience with learners. We present a vision, to integrate new technologies and visualization solutions, in order to encourage and develop personal learning that employs the visualization of personal learning data.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-Viz4me-VIS2015/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-Viz4me-VIS2015.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-Viz4me-VIS2015/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Gray-et-al-Poster-VIS2015&quot;&gt;C. C. Gray, J. C. Roberts, and P. D. Ritsos, “Where Can I Go From Here? Drawing Contextual Navigation Maps of the London Underground,” in &lt;i&gt;Posters presented at the IEEE Conference on Visualization (IEEE VIS 2015), Chicago, IL, USA&lt;/i&gt;, 2015.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Network administrators often wish to ascertain where network attackers are located; therefore it would be useful to display the network map from the context of either the attacker’s potential location or the attacked host. As part of a bigger project we are investigating how to best visualize contextual network data. We use a dataset of station adjacencies with journey times as edge weights, to explore which visualization design is most suitable, and also ascertain the best network shortest-path metric. This short paper presents our initial findings, and a visualization for Contextual Navigation using circular, centered-phylogram projections of the network. Our visualizations are interactive allowing users to explore different scenarios and observe relative distances in the data.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Gray-et-al-Poster-VIS2015/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Gray-et-al-Poster-VIS2015.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Gray-et-al-Poster-VIS2015/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;You can find more information on the conference at &lt;a href=&quot;http://ieeevis.org/&quot;&gt;IEEE VIS2015&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Posters at EG VCBM 2015</title>
   <link href="http://localhost:4000/news/2015/09/19/publications/"/>
   <updated>2015-09-19T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2015/09/19/publications</id>
   <content type="html">&lt;p&gt;We presented two posters in &lt;a href=&quot;http://www.vcbm.org/&quot;&gt;EG VCBM2015&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Headleand-et-al-Poster-VCBM2015&quot;&gt;C. J. Headleand, T. Day, S. R. Pop, P. D. Ritsos, and N. W. John, “Challenges and Technologies for Low Cost Wheelchair Simulation,” in &lt;i&gt;Eurographics Workshop on Visual Computing for Biology and Medicine&lt;/i&gt;, 2015.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    The use of electric wheelchairs is inherently risky, as collisions due to lack of control can result in injury for the user, but also potentially for other pedestrians. Introducing new users to powered chairs via virtual reality (VR) provides one possible solution, as it eliminates the risks inherent to the real world during training. However, traditionally simulator technology has been too expensive to make VR a financially viable solution. Also, current simulators lack the natural interaction possible in the real world, limiting their operational value. We present the early stages of a VR, electric wheelchair simulator built using low-cost, consumer level gaming hardware. The simulator makes use use of the the Leap Motion, to provide a level of interaction with the virtual world which has not previously been demonstrated in wheelchair training simulators. Furthermore, the Occulous Rift provides an immersive experience suitable for our training application
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Headleand-et-al-Poster-VCBM2015/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Headleand-et-al-Poster-VCBM2015.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.2312/vcbm.20151225&quot; class=&quot;doilink&quot;&gt;10.2312/vcbm.20151225&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Headleand-et-al-Poster-VCBM2015/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Ritsos-et-al-Poster-VCBM2015&quot;&gt;P. D. Ritsos, M. R. Edwards, I. S. Shergill, and N. W. John, “A Haptics-enabled Simulator for Transperineal Ultrasound-Guided Biopsy,” in &lt;i&gt;Eurographics Workshop on Visual Computing for Biology and Medicine&lt;/i&gt;, 2015.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    We present the development of a transperineal prostate biopsy, with high fidelity haptic feedback. We describe our current prototype, which is using physical props and a Geomagic Touch. In addition, we discuss a method for collecting in vitro axial needle forces, for programming haptic feedback, along with implemented an forthcoming features such as a display of 2D ultrasonic images for targeting, biopsy needle bending, prostate bleeding and calcification. Our ultimate goal is to provide an affordable high-fidelity simulation by integrating contemporary off-the-shelf technology components.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Ritsos-et-al-Poster-VCBM2015/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Ritsos-et-al-Poster-VCBM2015.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.2312/vcbm.20151229&quot; class=&quot;doilink&quot;&gt;10.2312/vcbm.20151229&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Ritsos-et-al-Poster-VCBM2015/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Paper at IEEE VIS 2015 Personal Visualization Workshop</title>
   <link href="http://localhost:4000/news/2015/09/06/publications/"/>
   <updated>2015-09-06T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2015/09/06/publications</id>
   <content type="html">&lt;p&gt;Our publication &lt;a href=&quot;http://pdritsos.com/files/Roberts-et-al-Viz4me-VIS2015.pdf&quot;&gt;Personal Learning Visualization&lt;/a&gt; was presented in the &lt;em&gt;Personal Visualization: Exploring Data in Everyday Life&lt;/em&gt; workshop of &lt;em&gt;IEEE VIS2015&lt;/em&gt;. You can have a look at the accepted papers, news and updates from the workshop at  &lt;a href=&quot;http://www.vis4me.com/personalvis15/&quot;&gt;Vis4me&lt;/a&gt;.&lt;/p&gt;

&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/assets/figures/banner-for-personal-learning.png&quot;&gt;&lt;img src=&quot;/assets/figures/banner-for-personal-learning.png&quot; alt=&quot;Personal Visualization, from data to learning&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: Personal Visualization, from data to learning [&lt;a href=&quot;/assets/figures/banner-for-personal-learning.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;span class=&quot;mystrong&quot;&gt;Abstract&lt;/span&gt; - Learners have personal data, such as grades, feedback and statistics on how they fair or compare with the class. But, data focusing on their personal learning is lacking, as it does not get updated regularly (being updated at the end of a taught session) and the displayed information is generally a single grade. Consequently, it is difficult for students to use this information to adapt their behavior, and help them on their learning journey. Yet, there is a rich set of data that could be captured and help students learn better. What is required is dynamically, regularly updated personal data, that is displayed to students in a timely way. Such ‘personal data’ can be presented to the student through ‘personal visualizations’ that engender ‘personal learning’. In this paper we discuss our journey into developing learning systems and our resulting experience with learners. We present a vision, to integrate new technologies and visualization solutions, in order to encourage and develop personal learning that employs the visualization of personal learning data.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ul class=&quot;bibliography&quot;&gt;
&lt;li&gt;&lt;span id=&quot;Roberts-et-al-Viz4me-VIS2015&quot;&gt;J. C. Roberts, C. Headleand, D. Perkins, and P. D. Ritsos, “Personal Visualisation for Learning,” in &lt;i&gt;Personal Visualization: Exploring Data in Everyday Life Workshop, IEEE Conference on Visualization (VIS), Chicago, IL, USA&lt;/i&gt;, 2015.&lt;/span&gt; &lt;a href=&quot;/publications#Roberts-et-al-Viz4me-VIS2015&quot; title=&quot;To Publications...&quot;&gt;&lt;i class=&quot;fa fa-forward pinky&quot;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper at IEEE VizSec 2015</title>
   <link href="http://localhost:4000/news/2015/09/05/publications/"/>
   <updated>2015-09-05T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2015/09/05/publications</id>
   <content type="html">&lt;p&gt;Our publication &lt;a href=&quot;http://pdritsos.com/files/Gray-et-al-VizSec2015.pdf&quot;&gt;Contextual Network Navigation; Situational Awareness for Network Administrators&lt;/a&gt; was presented in the &lt;em&gt;IEEE Symposium on Visualization for Cyber Security&lt;/em&gt; held in conjunction with &lt;em&gt;IEEE VIS2015&lt;/em&gt;. You can have a look at the accepted papers, news and updates from the symposium at  &lt;a href=&quot;http://vizsec.org/&quot;&gt;VisSec&lt;/a&gt;.&lt;/p&gt;

&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/assets/figures/cnvizsec.png&quot;&gt;&lt;img src=&quot;/assets/figures/cnvizsec.png&quot; alt=&quot;Contextual Navigation Projection for AS34623 with AS5577 highlighted as a ‘network of interest’. The yellow nodes shown where AS5577 appears, allowing the user to visually reason potential blocking points.&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: Contextual Navigation Projection for AS34623 with AS5577 highlighted as a ‘network of interest’. The yellow nodes shown where AS5577 appears, allowing the user to visually reason potential blocking points. [&lt;a href=&quot;/assets/figures/cnvizsec.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;span class=&quot;mystrong&quot;&gt;Abstract&lt;/span&gt; - One of the goals of network administrators is to identify and block sources of attacks from a network steam. Various tools have been developed to help the administrator identify the IP or subnet to be blocked, however these tend to be non-visual. Having a good perception of the wider network can aid the administrator identify their origin, but while network maps of the Internet can be useful for such endeavors, they are difficult to construct, comprehend and even utilize in an attack, and are often referred to as being “hairballs”. We present a visualization technique that displays pathways back to the attacker; we include all potential routing paths with a best-efforts identification of the commercial relationships involved. These two techniques can potentially highlight common pathways and/or networks to allow faster, more complete resolution to the incident, as well as fragile or incomplete routing pathways to/from a network. They can help administrators re-profile their choice of IP transit suppliers to better serve a target audience.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ul class=&quot;bibliography&quot;&gt;
&lt;li&gt;&lt;span id=&quot;Gray-et-al-VizSec2015&quot;&gt;C. C. Gray, P. D. Ritsos, and J. C. Roberts, “Contextual Network Navigation; Situational Awareness for Network Administrators,” in &lt;i&gt;IEEE Symposium on Visualization for Cyber Security (VizSec), Chicago, IL, USA&lt;/i&gt;, 2015.&lt;/span&gt; &lt;a href=&quot;/publications#Gray-et-al-VizSec2015&quot; title=&quot;To Publications...&quot;&gt;&lt;i class=&quot;fa fa-forward pinky&quot;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Poster at IEEE VIS 2015</title>
   <link href="http://localhost:4000/news/2015/07/24/publications/"/>
   <updated>2015-07-24T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2015/07/24/publications</id>
   <content type="html">&lt;p&gt;Our poster &lt;a href=&quot;http://pdritsos.com/files/Gray-et-al-ContextNav-VIS2015.pdf&quot;&gt;Where Can I Go From Here? Drawing Contextual Navigation Maps of the London Underground&lt;/a&gt; was presented in &lt;em&gt;IEEE VIS2015&lt;/em&gt; held in Chicago, IL. You can have a look at the accepted papers, news and updates from the symposium at  &lt;a href=&quot;http://www.ieeevis.org&quot;&gt;VIS2015&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;mystrong&quot;&gt;Abstract&lt;/span&gt; - Network administrators often wish to ascertain where network attackers are located; therefore it would be useful to display the network map from the context of either the attacker’s potential location or the attacked host. As part of a bigger project we are investigating how to best visualize contextual network data. We use a dataset of station adjacencies with journey times as edge weights, to explore which visualization design is most suitable, and also ascertain the best network shortest-path metric. This short paper presents our initial findings, and a visualization for Contextual Navigation using circular, centered-phylogram projections of the network. Our visualizations are interactive allowing users to explore different scenarios and observe relative distances in the data.&lt;/p&gt;

&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/assets/figures/cont_nav.png&quot;&gt;&lt;img src=&quot;/assets/figures/cont_nav.png&quot; alt=&quot;Contextual Navigation (CN) creates topological projections centered on a given point. Routes to the remainder of the network radiate from this point. Especially useful for creating maps to answer ‘Where can I go from here?’. &quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: Contextual Navigation (CN) creates topological projections centered on a given point. Routes to the remainder of the network radiate from this point. Especially useful for creating maps to answer ‘Where can I go from here?’.  [&lt;a href=&quot;/assets/figures/cont_nav.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&quot;video-preview&quot;&gt;Video Preview&lt;/h3&gt;

&lt;div class=&quot;embed-container&quot;&gt;
&lt;iframe src=&quot;https://player.vimeo.com/video/136252426&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ul class=&quot;bibliography&quot;&gt;
&lt;li&gt;&lt;span id=&quot;Gray-et-al-Poster-VIS2015&quot;&gt;C. C. Gray, J. C. Roberts, and P. D. Ritsos, “Where Can I Go From Here? Drawing Contextual Navigation Maps of the London Underground,” in &lt;i&gt;Posters presented at the IEEE Conference on Visualization (IEEE VIS 2015), Chicago, IL, USA&lt;/i&gt;, 2015.&lt;/span&gt;&lt;a href=&quot;/publications#Gray-et-al-Poster-VIS2015&quot; title=&quot;To Publications...&quot;&gt;&lt;i class=&quot;fa fa-forward pinky&quot;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Journal paper at IEEE TVCG / VIS 2015</title>
   <link href="http://localhost:4000/news/2015/06/08/publications/"/>
   <updated>2015-06-08T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2015/06/08/publications</id>
   <content type="html">&lt;p&gt;Our publication &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7192707&quot;&gt;Sketching designs using the Five Design-Sheet methodology&lt;/a&gt; was presented in &lt;em&gt;IEEE VIS2015&lt;/em&gt; held in Chicago, IL. More information on the FDS methodology can be found over at the project’s website, at &lt;a href=&quot;http://fds.design/&quot;&gt;fds.design&lt;/a&gt;.&lt;/p&gt;

&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/assets/figures/ideas-designs-realisation2.jpg&quot;&gt;&lt;img src=&quot;/assets/figures/ideas-designs-realisation2.jpg&quot; alt=&quot;Five-design sheets - creating information visualization interfaces through lo-fidelity methods.&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: Five-design sheets - creating information visualization interfaces through lo-fidelity methods. [&lt;a href=&quot;/assets/figures/ideas-designs-realisation2.jpg&quot;&gt;JPG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-TVCG-2016&quot;&gt;J. C. Roberts, C. Headleand, and P. D. Ritsos, “Sketching Designs Using the Five Design-Sheet Methodology,” &lt;i&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/i&gt;, vol. 22, no. 1, pp. 419–428, Jan. 2016.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Sketching designs has been shown to be a useful way of planning and considering alternative solutions. The use of lo-fidelity prototyping, especially paper-based sketching, can save time, money and converge to better solutions more quickly. However, this design process is often viewed to be too informal. Consequently users do not know how to manage their thoughts and ideas (to first think divergently, to then finally converge on a suitable solution). We present the Five Design Sheet (FdS) methodology. The methodology enables users to create information visualization interfaces through lo-fidelity methods. Users sketch and plan their ideas, helping them express different possibilities, think through these ideas to consider their potential effectiveness as solutions to the task (sheet 1); they create three principle designs (sheets 2,3 and 4); before converging on a final realization design that can then be implemented (sheet 5). In this article, we present (i) a review of the use of sketching as a planning method for visualization and the benefits of sketching, (ii) a detailed description of the Five Design Sheet (FdS) methodology, and (iii) an evaluation of the FdS using the System Usability Scale, along with a case-study of its use in industry and experience of its use in teaching.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-TVCG-2016/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-TVCG-2016.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/TVCG.2015.2467271&quot; class=&quot;doilink&quot;&gt;10.1109/TVCG.2015.2467271&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2015&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-TVCG-2016/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>IEEE CG&amp;A journal paper invited in IEEE VIS 2015</title>
   <link href="http://localhost:4000/news/2015/05/08/publications/"/>
   <updated>2015-05-08T00:00:00+02:00</updated>
   <id>http://localhost:4000/news/2015/05/08/publications</id>
   <content type="html">&lt;p&gt;Our publication &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6879056&quot;&gt;Visualization Beyond the Desktop - the next big thing&lt;/a&gt; was invited and presented in &lt;a href=&quot;http://www.ieeevis.org&quot;&gt;VIS2015&lt;/a&gt;, held in Chicago, IL. Our paper presents a road-map to the future of data visualization, facilitated from a technological metamorphosis in interface design, towards a mixed reality world, that encompasses various sensors, is transparent to the user and can be accessed via the Web.&lt;/p&gt;

&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/assets/figures/vis_perception.png&quot;&gt;&lt;img src=&quot;/assets/figures/vis_perception.png&quot; alt=&quot;caption:Visualization processes between the computer and human. Data is mapped onto perceptual variables and presented through various technologies. This all occurs in a particular place—a space with context (for example, a classroom, laboratory, or means of transportation). Through perception we acquire meaning of the presented data and awareness of our context.&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: caption:Visualization processes between the computer and human. Data is mapped onto perceptual variables and presented through various technologies. This all occurs in a particular place—a space with context (for example, a classroom, laboratory, or means of transportation). Through perception we acquire meaning of the presented data and awareness of our context. [&lt;a href=&quot;/assets/figures/vis_perception.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-CGA2014&quot;&gt;J. C. Roberts, P. D. Ritsos, S. K. Badam, D. Brodbeck, J. Kennedy, and N. Elmqvist, “Visualization Beyond the Desktop - the next big thing,” &lt;i&gt;IEEE Computer Graphics and Applications&lt;/i&gt;, vol. 34, no. 6, pp. 26–34, Nov. 2014.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Visualization is coming of age. With visual depictions being seamlessly integrated into documents, and data visualization techniques being used to understand increasingly large and complex datasets, the term &quot;visualization&quot;’ is becoming used in everyday conversations. But we are on a cusp; visualization researchers need to develop and adapt to today’s new devices and tomorrow’s technology. Today, people interact with visual depictions through a mouse. Tomorrow, they’ll be touching, swiping, grasping, feeling, hearing, smelling, and even tasting data. The next big thing is multisensory visualization that goes beyond the desktop.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-CGA2014/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-CGA2014.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.1109/MCG.2014.82&quot; class=&quot;doilink&quot;&gt;10.1109/MCG.2014.82&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   [&lt;span class=&quot;addedInfo&quot;&gt;Presented at IEEE VIS 2015&lt;/span&gt;] 
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-CGA2014/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Papers at IEEE VIS 2014 Death of the Desktop Wokshop</title>
   <link href="http://localhost:4000/news/2014/11/30/publications/"/>
   <updated>2014-11-30T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2014/11/30/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/Death_of_the_Desktop.png&quot; alt=&quot;DoTD&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We presented two position papers in the Death of the Desktop Workshop of &lt;em&gt;IEEE VIS2014&lt;/em&gt;:&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Ritsos-et-al-DOTD-VIS2014&quot;&gt;P. D. Ritsos, J. W. Mearman, A. Vande Moere, and J. C. Roberts, “Sewn with Ariadne’s Thread - Visualizations for Wearable &amp;amp; Ubiquitous Computing,” in &lt;i&gt;Death of the Desktop Workshop, IEEE Conference on Visualization (VIS), Paris, France&lt;/i&gt;, 2014.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Lance felt a buzz on his wrist, as Alicia, his wearable, informed him via the bone-conduction ear-piece - ‘You have received an email from Dr Jones about the workshop’. His wristwatch displayed an unread email glyph icon. Lance tapped it and listened to the voice of Dr Jones, talking about the latest experiment. At the same time he scanned through the email attachments, projected in front of his eyes, through his contact lenses. One of the files had a dataset of a carbon femtotube structure
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Ritsos-et-al-DOTD-VIS2014/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Ritsos-et-al-DOTD-VIS2014.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Ritsos-et-al-DOTD-VIS2014/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Roberts-et-al-DOTD-VIS2014&quot;&gt;J. C. Roberts, J. W. Mearman, and P. D. Ritsos, “The desktop is dead, long live the desktop! – Towards a multisensory desktop for visualization,” in &lt;i&gt;Death of the Desktop Workshop, IEEE Conference on Visualization (VIS), Paris, France&lt;/i&gt;, 2014.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    “Le roi est mort, vive le roi!”; or “The King is dead, long live the King” was a phrase originally used for the French throne of Charles VII in 1422, upon the death of his father Charles VI. To stave civil unrest the governing figures wanted perpetuation of the monarchs. Likewise, while the desktop as-we-know-it is dead (the use of the WIMP interface is becoming obsolete in visualization) it is being superseded by a new type of desktop environment: a multisensory visualization space. This space is still a personal workspace, it’s just a new kind of desk environment. Our vision is that data visualization will become more multisensory, integrating and demanding all our senses (sight, touch, audible, taste, smell etc.), to both manipulate and perceive the underlying data and information.
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Roberts-et-al-DOTD-VIS2014/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Roberts-et-al-DOTD-VIS2014.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Roberts-et-al-DOTD-VIS2014/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;You can find more information on the conference at &lt;a href=&quot;http://ieeevis.org/year/2014/info/vis-welcome/welcome&quot;&gt;IEEE VIS2014&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper at EG GCH Workshop</title>
   <link href="http://localhost:4000/news/2014/10/30/publications/"/>
   <updated>2014-10-30T00:00:00+01:00</updated>
   <id>http://localhost:4000/news/2014/10/30/publications</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/images/EGGCH.png&quot; alt=&quot;EGGCH&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We presented a short paper in the &lt;em&gt;12th Eurographics Workshop on Graphics and Cultural Heritage&lt;/em&gt;:&lt;/p&gt;

&lt;div class=&quot;bibliography&quot;&gt;&lt;p&gt;

&lt;span id=&quot;Ritsos-et-al-EGGCH2014&quot;&gt;P. D. Ritsos, A. T. Wilson, H. C. Miles, L. F. Williams, B. Tiddeman, F. Labrosse, S. Griffiths, B. Edwards, K. Möller, R. Karl, and J. C. Roberts, “Community-driven Generation of 3D and Augmented Web Content for Archaeology,” in &lt;i&gt;Eurographics Workshop on Graphics and Cultural Heritage (EGGCH) - Short Papers and Posters&lt;/i&gt;, Darmstadt, Germany, 2014, pp. 25–28.&lt;/span&gt;

  &lt;span class=&quot;toggle&quot;&gt;
    Heritage sites (such as prehistoric burial cairns and standing stones) are prolific in Europe; although there is a wish to scan each of these sites, it would be time-consuming to achieve. Citizen science approaches enable us to involve the public to perform a metric survey by capturing images. In this paper, discussing work-in progress, we present our automatic process that takes the user’s uploaded photographs, converts them into 3D models and displays them in two presentation platforms – in a web gallery application, using X3D/X3DOM, and in mobile augmented reality, using awe.js
  &lt;/span&gt;
  &lt;br /&gt;
  &lt;span class=&quot;trigger biblinks&quot;&gt;[&lt;span class=&quot;abstractlink&quot;&gt;Abstract&lt;/span&gt;]&amp;nbsp;&amp;nbsp; &lt;/span&gt;

&lt;span class=&quot;biblinks&quot;&gt;
  [&lt;a href=&quot;/bibliography/Ritsos-et-al-EGGCH2014/&quot;&gt;Details&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
  
        
          [&lt;a href=&quot;/files/Ritsos-et-al-EGGCH2014.pdf&quot; class=&quot;pdflink&quot;&gt;PDF&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
         
  
   
  
  
  [doi:&lt;a href=&quot;http://dx.doi.org/10.2312/gch.20141321&quot; class=&quot;doilink&quot;&gt;10.2312/gch.20141321&lt;/a&gt;]&amp;nbsp;&amp;nbsp;
  
   
&lt;/span&gt;
&lt;a class=&quot;details&quot; href=&quot;/bibliography/Ritsos-et-al-EGGCH2014/&quot;&gt;&lt;span hidden=&quot;&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;You can find more information on the workshop at &lt;a href=&quot;https://diglib.eg.org/handle/10.2312/7755&quot;&gt;EGGHC 2014&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 

</feed>
