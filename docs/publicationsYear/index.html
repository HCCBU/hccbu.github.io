<!DOCTYPE html>
<!--[if lt IE 7]>  <html class="ie ie6 lte9 lte8 lte7" lang="en-us"> <![endif]-->
<!--[if IE 7]>     <html class="ie ie7 lte9 lte8 lte7" lang="en-us"> <![endif]-->
<!--[if IE 8]>     <html class="ie ie8 lte9 lte8" lang="en-us"> <![endif]-->
<!--[if IE 9]>     <html class="ie ie9 lte9" lang="en-us"> <![endif]-->
<!--[if gt IE 9]>  <html> <![endif]-->
<!--[if !IE]><!--> <html lang="en-us"><!--<![endif]-->



  <head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="description" content="XRVA - XReality, Visualization and Analytics lab, School of Computer Science and Engineering, Bangor University, research on mixed and virtual reality, information visualization, visual analytics, immersive analytics, situated analytics">
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Publications &middot; XRVA - Bangor University
    
  </title>

  <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href='https://fonts.googleapis.com/css?family=Roboto' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Lato"rel="stylesheet" type="text/css" />

  <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">


  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
 <!-- <link rel="stylesheet" href="/public/css/academicons.css">-->
  <link rel="stylesheet" href="/public/css/xrva.css">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin="anonymous" referrerpolicy="no-referrer" />

<link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet" type="text/css" >


  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.png">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="XRVA - Bangor University" />
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Publications | XRVA - Bangor University</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Publications" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="XRealities, Visualization and Analytics (XRVA) lab, Visualization, Data, Modelling and Graphics (VDMG) research group, School of Computer Science and Engineering, College of Environmental Sciences and Engineering, Bangor University, &lt;/br&gt;, Dean Street, Bangor, Gwynedd, UK, LL57 1UT" />
<meta property="og:description" content="XRealities, Visualization and Analytics (XRVA) lab, Visualization, Data, Modelling and Graphics (VDMG) research group, School of Computer Science and Engineering, College of Environmental Sciences and Engineering, Bangor University, &lt;/br&gt;, Dean Street, Bangor, Gwynedd, UK, LL57 1UT" />
<link rel="canonical" href="http://localhost:4000/publicationsYear/" />
<meta property="og:url" content="http://localhost:4000/publicationsYear/" />
<meta property="og:site_name" content="XRVA - Bangor University" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Publications" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"XRealities, Visualization and Analytics (XRVA) lab, Visualization, Data, Modelling and Graphics (VDMG) research group, School of Computer Science and Engineering, College of Environmental Sciences and Engineering, Bangor University, &lt;/br&gt;, Dean Street, Bangor, Gwynedd, UK, LL57 1UT","headline":"Publications","url":"http://localhost:4000/publicationsYear/"}</script>
<!-- End Jekyll SEO tag -->

</head>


      <body id="publications">
        <div class="wrapper">
          <div id="mysidebar">

  <div class="author-info">
<img class="logo" src="/assets/images/hccbu_logo.png"></img>
<!--<h2 class="role"><span class="subtitleAccent">XR</span>eality, <span class="subtitleAccent">V</span>isualization &amp; <span class="subtitleAccent">A</span>nalytics</h2>-->

<!--<h2 class="role">Senior Lecturer in Visualization</h2>-->

<!--<nav class="sidenav">
    <ul>
     
      
      <li id="nav-home"><a href="/">Home</a></li>
      
            
              <li id="nav-labs"><a href="/labs/">Labs</a></li>
                
      
            
              <li id="nav-research"><a href="/projects/">Projects</a></li>
                
      
            
              <li id="nav-team"><a href="/team/">Team</a></li>
                
      
            
              <li id="nav-publications"><a href="/publications/">Publications</a></li>
                
      
            
              <li id="nav-culture"><a href="/culture/">Culture</a></li>
                
      
    </ul>
  </nav>
  -->

  <a href="https://www.bangor.ac.uk/computer-science-and-electronic-engineering/" target="_blank">School of Computer Science and<br/>Engineering, </a>
  <a href="https://www.bangor.ac.uk/" target="_blank">Bangor University,</a><br/>
  Dean Street, Bangor,<br/>
  Gwynedd, UK, LL57 1UT<br/>
</div>

  <div class="author-links">
   <!--- <span class="icoFrame"><i class="ico fas fa-university fa-fw"></i></span> &nbsp; <a href="https://www.bangor.ac.uk/computer-science-and-electronic-engineering/staff/panagiotis-ritsos/en" target="_blank">University Page</a><br/>                
    <span class="icoFrame"><i class="ico fab fa-github fa-fw"></i></span> &nbsp; <a href="https://github.com/ritsosp" target="_blank">GitHub</a><br/>-->
    

    <nav class="sidenav">
      <span>
       
        
        <spam id="nav-home"><a href="/">Home</a></span><br>
        
              
              <span id="nav-labs"><a href="/labs/">Labs</a></span></br>
                  
        
              
              <span id="nav-research"><a href="/projects/">Projects</a></span></br>
                  
        
              
              <span id="nav-team"><a href="/team/">Team</a></span></br>
                  
        
              
              <span id="nav-publications"><a href="/publications/">Publications</a></span></br>
                  
        
              
              <span id="nav-culture"><a href="/culture/">Culture</a></span></br>
                  
        
      </ul>
    </nav>

    <div class="social noBorder">
      <a href="https://github.com/xrvalab" title="XRVA @ Github" target="_blank"><i class="fa-brands fa-github"></i></a>
      <a href="https://twitter.com/xrvalab" title="XRVA @ Twitter :: https://twitter.com/xrvalab" target="_blank"><i class="fa-brands fa-x-twitter"></i></a>
      <a href="https://vis.social/@xrva" title="XRVA @ Mastodon Social :: https://vis.social/@xrva" target="_blank"><i class="fa-brands fa-mastodon"></i></a>          			
    </div>
              

    <div class="legal">
      <p>Built with Jekyll <br> Last updated on November 2024<br/>
              &copy; 2025. All rights reserved.</p>
    </div>
  </div> 
</div>

          <div class="main">
          <!--  <nav class="topnav">
    <ul>
     
      
      <li id="nav-home"><a href="/">Home</a></li>
      
            
              <li id="nav-labs"><a href="/labs/">Labs</a></li>
                
      
            
              <li id="nav-research"><a href="/projects/">Projects</a></li>
                
      
            
              <li id="nav-team"><a href="/team/">Team</a></li>
                
      
            
              <li id="nav-publications"><a href="/publications/">Publications</a></li>
                
      
            
              <li id="nav-culture"><a href="/culture/">Culture</a></li>
                
      
    </ul>
  </nav>
  -->

          <!-- -->


            <h1 id="head-publications">Publications</h1>
            <div class="page">
  <!--<h1 class="page-title">Publications</h1>-->
  <div class="dredd">
  <p class="datenote"><a href="/publications/">Display per type</a> | Updated November 2024</p>

<h2 id="2024">2024</h2>

<div class="bibliography"><p>

<span id="Batch-et-al-TVCG-2024">A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “Wizualization: A ’Hard Magic’ Visualization System for Immersive and Ubiquitous Analytics,” <i>IEEE Transactions on Visualization and Computer Graphics (to appear)</i>, 2024.</span>

  <span class="toggle">
    What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (SpellBook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Batch-et-al-TVCG-2024/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Batch-et-al-TVCG-2024.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2023.3326580" class="doilink">10.1109/TVCG.2023.3326580</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2023</span>] 
</span>
<a class="details" href="/bibliography/Batch-et-al-TVCG-2024/"><span hidden=""></span></a></p></div>

<h2 id="2023">2023</h2>

<div class="bibliography"><p>

<span id="Coughlan-et-al-JABS-2023">P. Coughlan, R. Bellini, A. Bello-Dambatta, R. Dallison, K. Dreyer-Gibney, J. Gallagher, I. Harris, A. McNabola, D. Mitrovic, M. Murali, D. Novara, S. Patil, A. Rigby, P. Ritsos, I. Schestak, A. Singh, N. Walker, and P. Williams, “Researching Green Process Innovation Across Borders and Boundaries Through Collaborative Inquiry,” <i>The Journal of Applied Behavioral Science</i>, vol. 59, no. 4, pp. 556–584, Aug. 2023.</span>

  <span class="toggle">
    Research involving multistakeholder collaborative partnerships is growing, as both academia and funding agencies align their objectives with societal challenges and undertake research in the context of application. In particular, the UN sustainable development goals mandate green process innovation research that transcends disciplinary boundaries. Responding to this opportunity, this article explores the question: how can researchers, as societal stakeholders, collaborate in the design and implementation of a green process innovation research initiative and produce actionable research-based contributions to knowledge? Drawing upon our shared experience of realizing green process innovation, we describe and conceptualize the collaborative inquiry process, reflecting on the interplay of modes of knowledge production and the complementarity of researchers’ roles. We conclude by noting how researchers collaborating in a green process innovation initiative can shape the environment in which Transdisciplinary research (TDR) develops and play different roles enabling breadth and diversity of interaction, depth of disciplinary integration, and production of different types of knowledge.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Coughlan-et-al-JABS-2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Coughlan-et-al-JABS-2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1177/00218863231194655" class="doilink">10.1177/00218863231194655</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Coughlan-et-al-JABS-2023/"><span hidden=""></span></a></p>
<p>

<span id="Batch-et-al-CGF-2023">A. Batch, S. Shin, J. Liu, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “Evaluating View Management for Situated Visualization in Web-based Handheld AR,” <i>Computer Graphics Forum</i>, vol. 42, no. 3, pp. 349–360, Jun. 2023.</span>

  <span class="toggle">
    As visualization makes the leap to mobile and situated settings, where data is increasingly integrated with the physical world using mixed reality, there is a corresponding need for effectively managing the immersed user’s view of situated visualizations. In this paper we present an analysis of view management techniques for situated 3D visualizations in handheld augmented reality: a shadowbox, a world-in-miniature metaphor, and an interactive tour. We validate these view management solutions through a concrete implementation of all techniques within a situated visualization framework built using a web-based augmented reality visualization toolkit, and present results from a user study in augmented reality accessed using handheld mobile devices.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Batch-et-al-CGF-2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Batch-et-al-CGF-2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1111/cgf.14835" class="doilink">10.1111/cgf.14835</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at EG EuroVis 2023</span>] 
</span>
<a class="details" href="/bibliography/Batch-et-al-CGF-2023/"><span hidden=""></span></a></p>
<p>

<span id="Shin-et-al-TVCG-2023">S. Shin, A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “The Reality of the Situation: A Survey of Situated Analytics,” <i>IEEE Transactions on Visualization and Computer Graphics (to appear)</i>, 2023.</span>

  <span class="toggle">
     The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user’s physical location. In this work, we identify prior literature in this emerging field with a focus on situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. We also assess the level which these systems support the sensemaking process. Finally, we discuss insights and design guidelines that we learned from our analysis.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Shin-et-al-TVCG-2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Shin-et-al-TVCG-2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2023.3285546" class="doilink">10.1109/TVCG.2023.3285546</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2023</span>] 
</span>
<a class="details" href="/bibliography/Shin-et-al-TVCG-2023/"><span hidden=""></span></a></p></div>
<div class="bibliography"></div>
<div class="bibliography"><p>

<span id="Rigby-et-al-Poster-VIS2023">A. Rigby, P. W. S. Butcher, R. Bellini, P. Coughlan, A. Mc Nabola, and P. D. Ritsos, “DUVis: A visual analytics tool for supporting a trans-disciplinary project,” in <i>Posters presented at the IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia</i>, 2023.</span>

  <span class="toggle">
    We present DUVis, a visual analytics application developed to support the analysis and appraisal, of the transdiciplinary project Dŵr Uisce, from internal project managers and external stakeholders. DUVis provides a number of visualizations and additional features to facilitate data exploration of a project’s progress. It presents a map of stakeholders’ activities, and their engagement with each other, as well as outputs, workpackages, their completion status and potential impact. We present our preliminary design and provide a blueprint for further development.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Rigby-et-al-Poster-VIS2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Rigby-et-al-Poster-VIS2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Rigby-et-al-Poster-VIS2023/"><span hidden=""></span></a></p>
<p>

<span id="Roberts-et-al-Poster-VIS2023">J. C. Roberts, H. Alnjar, A. E. Owen, and P. D. Ritsos, “A method for Critical and Creative Visualisation Design-Thinking,” in <i>Posters presented at the IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia</i>, 2023.</span>

  <span class="toggle">
    Visualisation design requires critical thought: to understand important facets, investigate design suitability and explore alternatives. But, especially for learners, it can be difficult to structure a critical reflection of creative solutions. We introduce the Critical Design Survey (CDS): structured method that facilitates visualisation design analysis through reflective and critical thought. Applying the CDS helps someone to structure critical thought, provides a unified method that can be readily taught, learners can actively engage with the process and directly use it to write a critical-thinking report of their design ideas. The CDS contains three steps: Step 1, summarise and write down the essence of the idea. Step 2, perform an in-depth critique (we define 30 questions structured in six perspectives). Step 3, synthesise the ideas, implications, and decide on the next steps. We present the CDS, describe our design process (critical thinking workshops, talk aloud, and student use), and describe our use in teaching visualisation to undergraduate and postgraduate students.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-Poster-VIS2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-Poster-VIS2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-Poster-VIS2023/"><span hidden=""></span></a></p>
<p>

<span id="Bellini-et-al-Poster-EGU2023">R. Bellini, P. Coughlan, A. Bello-Dambatta, A. Rigby, P. D. Ritsos, and A. Mc Nabola, “An interactive visualisation tool to manage metadata in engaged research projects, track progress, map stakeholders, and evaluate output, outcomes and impacts,” in <i>EGU General Assembly, Vienna, Austria</i>, 2023.</span>

  <span class="toggle">
    This paper presents the research management experience of a multi-disciplinary team and their reflections on how they responded to these challenges and implemented working solutions. As a team from five disciplines, we reflect on this shared experience gained over a 6.5 year-long EU-funded project. Stimulated by the project complexity, we came to recognise that how we managed the data provided us with an opportunity to collaborate meaningfully and to link in novel ways the contributions of research activities to the outcomes and impacts of the project. In brief, we devised a new research data management approach through which we collated and visualised the data so as to facilitate deeper exploration of the interactions among the researchers, tasks and deliverables.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Bellini-et-al-Poster-EGU2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Bellini-et-al-Poster-EGU2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.5194/egusphere-egu23-2630" class="doilink">10.5194/egusphere-egu23-2630</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Bellini-et-al-Poster-EGU2023/"><span hidden=""></span></a></p></div>
<div class="bibliography"></div>

<h2 id="2022">2022</h2>

<div class="bibliography"><p>

<span id="Rigby-et-al-EnvModSoft-2022">A. M. F. Rigby, P. W. S. Butcher, P. D. Ritsos, and S. D. Patil, “LUCST: A novel toolkit for Land Use Land Cover change assessment in SWAT+ to support flood management decisions,” <i>Environmental Modelling &amp; Software</i>, vol. 156, no. 105469, Aug. 2022.</span>

  <span class="toggle">
    Land Use Land Cover (LULC) change is widely recognised as one of the most important factors impacting the hydrological response of river basins. SWAT +, the latest version of the Soil and Water Assessment Tool, has been used extensively to assess the hydrological impacts of LULC change. However, the process of making and assessing such changes in SWAT+ is often cumbersome and non-intuitive, thereby reducing its usability amongst a wider pool of applied users. We address this issue by developing a user-friendly toolkit, Land Use Change SWAT+ Toolkit (LUCST), that will: (1) allow the end-user to define various LULC change scenarios in their study catchment, (2) run the SWAT+ model with the specified LULC changes, and (3) enable interactive visualisation of the different SWAT+ output variables. A good System Usability Score (79.8) and positive feedback from end-users promises the potential for adopting LUCST in future LULC change studies.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Rigby-et-al-EnvModSoft-2022/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Rigby-et-al-EnvModSoft-2022.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1016/j.envsoft.2022.105469" class="doilink">10.1016/j.envsoft.2022.105469</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Rigby-et-al-EnvModSoft-2022/"><span hidden=""></span></a></p>
<p>

<span id="Dykes-et-al-PhilTransA-2022">J. Dykes, A. Abdul-Rahman, D. Archambault, B. Bach, R. Borgo, M. Chen, J. Enright, H. Fang, E. E. Firat, E. Freeman, T. Gönen, C. Harris, R. Jianu, N. W. John, S. Khan, A. Lahiff, R. S. Laramee, L. Matthews, S. Mohr, P. H. Nguyen, A. A. M. Rahat, R. Reeve, P. D. Ritsos, J. C. Roberts, A. Slingsby, B. Swallow, T. Torsney-Weir, C. Turkay, R. Turner, F. P. Vidal, Q. Wang, J. Wood, and K. Xu, “Visualization for Epidemiological Modelling: Challenges, Solutions, Reflections &amp; Recommendations,” <i>Philosophical Transactions of the Royal Society A (Special issue on ’Technical challenges of modelling real-life epidemics and examples of overcoming these’) </i>, vol. 380, no. 2233, p. 20210299, Aug. 2022.</span>

  <span class="toggle">
    We report on an ongoing collaboration between epidemiological modellers and visualization researchers by documenting and reflecting upon knowledge constructs - a series of ideas, approaches and methods taken from existing visualization research and practice – deployed and developed to support modelling of the COVID-19 pandemic. Structured independent commentary on these efforts is synthesized through iterative reflection to develop: evidence of the effectiveness and value of visualization in this context; open problems upon which the research communities may focus; guidance for future activity of this type; and recommendations to safeguard the achievements and promote, advance, secure and prepare for future collaborations of this kind. In describing and comparing a series of related projects that were undertaken in unprecedented conditions, our hope is that this unique report, and its rich interactive supplementary materials, will guide the scientific community in embracing visualization in its observation, analysis and modelling of data as well as in disseminating findings. Equally we hope to encourage the visualization community to engage with impactful science in addressing its emerging data challenges. If we are successful, this showcase of activity may stimulate mutually beneficial engagement between communities with complementary expertise to address problems of significance in epidemiology and beyond.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Dykes-et-al-PhilTransA-2022/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Dykes-et-al-PhilTransA-2022.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
   [<a href="https://arxiv.org/abs/2204.06946" class="doilink">Preprint</a>]&nbsp;&nbsp;
  
  [doi:<a href="http://dx.doi.org/10.1098/rsta.2021.0299" class="doilink">10.1098/rsta.2021.0299</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Dykes-et-al-PhilTransA-2022/"><span hidden=""></span></a></p>
<p>

<span id="Chen-et-al-Epidemics-2022">M. Chen, A. Abdul-Rahman, D. Archambault, J. Dykes, A. Slingsby, P. D. Ritsos, T. Torsney-Weir, C. Turkay, B. Bach, R. Borgo, A. Brett, H. Fang, R. Jianu, S. Khan, R. S. Laramee, P. H. Nguyen, R. Reeve, J. C. Robert, F. Vidal, Q. Wang, J. Wood, and K. Xu, “RAMPVIS: Answering the Challenges of Building Visualisation Capabilities for Large-scale Emergency Responses,” <i>Epidemics</i>, vol. 39, no. 100569, Jun. 2022.</span>

  <span class="toggle">
    The effort for combating the COVID-19 pandemic around the world has resulted in a huge amount of data, e.g., from testing, contact tracing, modelling, treatment, vaccine trials, and more. In addition to numerous challenges in epidemiology, healthcare, biosciences, and social sciences, there has been an urgent need to develop and provide visualisation and visual analytics (VIS) capacities to support emergency responses under difficult operational conditions. In this paper, we report the experience of a group of VIS volunteers who have been working in a large research and development consortium and providing VIS support to various observational, analytical, model-developmental, and disseminative tasks. In particular, we describe our approaches to the challenges that we have encountered in requirements analysis, data acquisition, visual design, software design, system development, team organisation, and resource planning. By reflecting on our experience, we propose a set of recommendations as the first step towards a methodology for developing and providing rapid VIS capacities to support emergency responses.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Chen-et-al-Epidemics-2022/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Chen-et-al-Epidemics-2022.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
   [<a href="https://arxiv.org/abs/2012.04757" class="doilink">Preprint</a>]&nbsp;&nbsp;
  
  [doi:<a href="http://dx.doi.org/10.1016/j.epidem.2022.100569" class="doilink">10.1016/j.epidem.2022.100569</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Chen-et-al-Epidemics-2022/"><span hidden=""></span></a></p>
<p>

<span id="Roberts-et-al-MDPI-2022">J. C. Roberts, P. W. S. Butcher, and P. D. Ritsos, “One View Is Not Enough: Review of and Encouragement for Multiple and Alternative Representations in 3D and Immersive Visualisation,” <i>Computers</i>, vol. 11, no. 2, Feb. 2022.</span>

  <span class="toggle">
    The opportunities for 3D visualisations are huge. People can be immersed inside their data, interface with it in natural ways, and see it in ways that are not possible on a traditional desktop screen. Indeed, 3D visualisations, especially those that are immersed inside head-mounted displays are becoming popular. Much of this growth is driven by the availability, popularity and falling cost of head-mounted displays and other immersive technologies. However, there are also challenges. For example, data visualisation objects can be obscured, important facets missed (perhaps behind the viewer), and the interfaces may be unfamiliar. Some of these challenges are not unique to 3D immersive technologies. Indeed, developers of traditional 2D exploratory visualisation tools would use alternative views, across a multiple coordinated view (MCV) system. Coordinated view interfaces help users explore the richness of the data. For instance, an alphabetical list of people in one view shows everyone in the database, while a map view depicts where they live. Each view provides a different task or purpose. While it is possible to translate some desktop interface techniques into the 3D immersive world, it is not always clear what equivalences would be. In this paper, using several case studies, we discuss the challenges and opportunities for using multiple views in immersive visualisation. Our aim is to provide a set of concepts that will enable developers to perform critical thinking, creative thinking and push the boundaries of what is possible with 3D and immersive visualisation. In summary developers should consider how to integrate many views, techniques and presentation styles, and one view is not enough when using 3D and immersive visualisations.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-MDPI-2022/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-MDPI-2022.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.3390/computers11020020" class="doilink">10.3390/computers11020020</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-MDPI-2022/"><span hidden=""></span></a></p></div>
<div class="bibliography"></div>
<div class="bibliography"><p>

<span id="Rigby-et-al-Poster-WDNA-2022">A. M. F. Rigby, P. W. S. Butcher, S. D. Patil, and P. D. Ritsos, “Using AI and big data to optimise land management decisions for reducing river flood risk,” in <i>Data Transformation: Wales Data Nations Accelerator, Cardiff, UK</i>, 2022.</span>

  <span class="toggle">
    Local authorities across Wales are increasingly seeking natural approaches to river flood management, especially the role of land management decisions in reducing peak flows. Physics-based hydrological models, which simulate river flood response to storm events, can provide multi-scenario assessment of land-use changes on floods. However, they require prior calibration of parameters using measured streamflow data, which is not available for many rivers. We investigate how AI and big data can be used to implement hydrological models in river basins with no streamflow data.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Rigby-et-al-Poster-WDNA-2022/">Details</a>]&nbsp;&nbsp;
  
  
           
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Rigby-et-al-Poster-WDNA-2022/"><span hidden=""></span></a></p></div>
<div class="bibliography"></div>

<h2 id="2021">2021</h2>

<div class="bibliography"><p>

<span id="Butcher-et-al-TVCG-2021">P. W. S. Butcher, N. W. John, and P. D. Ritsos, “VRIA: A Web-based Framework for Creating Immersive Analytics Experiences,” <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 27, no. 07, pp. 3213–3225, Jul. 2021.</span>

  <span class="toggle">
    We present &#60;VRIA&#62;, a Web-based framework for creating Immersive Analytics (IA) experiences in Virtual Reality. &#60;VRIA&#62; is built upon WebVR, A-Frame, React and D3.js, and offers a visualization creation workflow which enables users, of different levels of
  expertise, to rapidly develop Immersive Analytics experiences for the Web. The use of these open-standards Web-based technologies
  allows us to implement VR experiences in a browser and offers strong synergies with popular visualization libraries, through the HTML
  Document Object Model (DOM). This makes &#60;VRIA&#62; ubiquitous and platform-independent. Moreover, by using WebVR’s progressive
  enhancement, the experiences &#60;VRIA&#62; creates are accessible on a plethora of devices. We elaborate on our motivation for focusing on
  open-standards Web technologies, present the &#60;VRIA&#62; creation workflow and detail the underlying mechanics of our framework. We also
  report on techniques and optimizations necessary for implementing Immersive Analytics experiences on the Web, discuss scalability
  implications of our framework, and present a series of use case applications to demonstrate the various features of &#60;VRIA&#62;. Finally, we
  discuss current limitations of our framework, the lessons learned from its development, and outline further extensions.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Butcher-et-al-TVCG-2021/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Butcher-et-al-TVCG-2021.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2020.2965109" class="doilink">10.1109/TVCG.2020.2965109</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2020</span>] 
</span>
<a class="details" href="/bibliography/Butcher-et-al-TVCG-2021/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Roberts-et-al-CGVC-2021">J. C. Roberts, J. W. Mearman, P. W. S. Butcher, H. M. Al-Maneea, and P. D. Ritsos, “3D Visualisations Should Not be Displayed Alone - Encouraging a Need for Multivocality in Visualisation,” in <i>Proceedings of the Eurographics Conference in Computer Graphics and Visual Computing (CGVC) 2021, Lincoln, UK</i>, 2021.</span>

  <span class="toggle">
    We believe that 3D visualisations should not be used alone; by coincidentally displaying alternative views the user can gain the best understanding of all situations. The different presentations signify manifold meanings and afford different tasks. Natural 3D worlds implicitly tell many stories. For instance, walking into a living room, seeing the TV, types of magazines, pictures on the wall, tells us much about the occupiers: their occupation, standards of living, taste in design, whether they have kids, and so on. How can we similarly create rich and diverse 3D visualisation presentations? How can we create visualisations that allow people to understand different stories from the data? In a multivariate 2D visualisation a developer may coordinate and link many views together to provide exploratory visualisation functionality. But how can this be achieved in 3D and in immersive visualisations? Different visualisation types, each have specific uses, and each has the potential to tell or evoke a different story. Through several use-cases, we discuss challenges of 3D visualisation, and present our argument for concurrent and coordinated visualisations of alternative styles, and encourage developers to consider using alternative representations with any 3D view, even if that view is displayed in a virtual, augmented or mixed reality setup.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-CGVC-2021/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-CGVC-2021.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
   [<a href="https://arxiv.org/abs/2108.04680" class="doilink">Preprint</a>]&nbsp;&nbsp;
  
  [doi:<a href="http://dx.doi.org/10.2312/cgvc.20211309" class="doilink">10.2312/cgvc.20211309</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-CGVC-2021/"><span hidden=""></span></a></p></div>
<!--<div class="bibliography"></div>-->
<div class="bibliography"><p>

<span id="Rigby-et-al-Poster-EGU-2021">A. Rigby, S. Patil, and P. D. Ritsos, “A novel toolkit to streamline Land Use Land Cover change assessment in the SWAT+ model to enhance flood management and infrastructure decisions,” in <i>EGU General Assembly 2021, online event</i>, 2021.</span>

  <span class="toggle">
    Land Use Land Cover (LULC) change is widely recognised as one of the most important factors impacting river basin hydrology.  It is therefore imperative that the hydrological impacts of various LULC changes are considered for effective flood management strategies and future infrastructure decisions within a catchment.  The Soil and Water assessment Tool (SWAT) has been used extensively to assess the hydrological impacts of LULC change.  Areas with assumed homogeneous hydrologic properties, based on their LULC, soil type and slope, make up the basic computational units of SWAT known as the Hydrologic Response Units (HRUs).  LULC changes in a catchment are typically modelled by SWAT through alterations to the input files that define the properties of these HRUs.  However, to our knowledge at least, the process of making such changes to the SWAT input files is often cumbersome and non-intuitive.  This affects the useability of SWAT as a decision support tool amongst a wider pool of applied users (e.g., engineering teams in environmental regulatory agencies and local authorities).  In this study, we seek to address this issue by developing a user-friendly toolkit that will: (1) allow the end user to specify, through a Graphical User Interface (GUI), various types of LULC changes at multiple locations within their study catchment, (2) run the SWAT+ model (the latest version of SWAT) with the specified LULC changes, and (3) enable interactive visualisation of the different SWAT+ output variables to quantify the hydrological impacts of these scenarios.  Importantly, our toolkit does not require the end user to have any operational knowledge of the SWAT+ model to use it as a decision support tool.  Our toolkit will be trialled at 15 catchments in Gwynedd county, Wales, which has experienced multiple occurrences of high flood events, and consequent economic damage, in the recent past.  We anticipate this toolkit to be a valuable addition to the decision-making processes of Gwynedd County Council for the planning and development of future flood alleviation schemes as well as other infrastructure projects.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Rigby-et-al-Poster-EGU-2021/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Rigby-et-al-Poster-EGU-2021.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.5194/egusphere-egu21-4139" class="doilink">10.5194/egusphere-egu21-4139</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Rigby-et-al-Poster-EGU-2021/"><span hidden=""></span></a></p></div>
<!--<div class="bibliography"></div> -->
<div class="bibliography"><p>

<span id="Roberts-et-al-VDMG-EG2021">J. C. Roberts, P. D. Ritsos, L. Kunchev, F. Vidal, I. S. Lim, L. ap Cennyd, C. Teahan William J. an Gray, and D. Perkins, “Visualisation Data Modelling Graphics (VDMG) at Bangor,” in <i>Eurographics 2021 - Projects and Labs</i>, 2021.</span>

  <span class="toggle">
    The Visualisation Data Modelling &amp; Graphics (VDMG) research group at Bangor University brings together researchers in visualisation, modelling, data-mining and Artificial Intelligence. Our vision is to help people understand data, depict it visually and deliver enjoyable experiences. We design, develop and evaluate computing solutions that often incorporate AI, machine learning, interaction, underpinned with advanced computing, and are always user-focused. Located in Bangor University – a civic University on the North Wales shoreline that is close to the Snowdonia mountain range and National Park – much of our research is inspired by nature, motivated to be sustainable, and people focused.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-VDMG-EG2021/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-VDMG-EG2021.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-VDMG-EG2021/"><span hidden=""></span></a></p></div>
<!--  <div class="bibliography"></div> -->

<h2 id="2020">2020</h2>

<div class="bibliography"><p>

<span id="Gray-et-al-CAEH-2020">C. C. Gray, D. Perkins, and P. D. Ritsos, “Degree Pictures: Visualizing the university student journey,” <i>Assessment &amp; Evaluation in Higher Education</i>, vol. 20, no. 4, pp. 568–578, Aug. 2020.</span>

  <span class="toggle">
    The field of learning analytics is progressing at a rapid rate. New tools, with ever-increasing number of features and a plethora of datasets that are increasingly utilized demonstrate the evolution and multifaceted nature of the field. In particular, the depth and scope of insight that can be gleaned from analysing related datasets can have a significant, and positive, effect in educational practices. We introduce the concept of degree pictures, a symbolic overview of students’ achievement. Degree pictures are small visualizations that depict graphically 16 categories of overall student achievement, over the duration of a higher education course. They offer a quick summary of students’ achievement and are intended to initiate appropriate responses, such as teaching and pastoral interventions. This can address the subjective nature of assessment, by providing a method for educators to calibrate their own marking practices by showing an overview of any cohort. We present a prototype implementation of degree pictures, which was evaluated within our School of Computer Science, with favourable results.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Gray-et-al-CAEH-2020/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Gray-et-al-CAEH-2020.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1080/02602938.2019.1676397" class="doilink">10.1080/02602938.2019.1676397</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Gray-et-al-CAEH-2020/"><span hidden=""></span></a></p>
<p>

<span id="Williams-et-al-MDPI-2020">B. Williams, P. D. Ritsos, and C. Headleand, “Virtual Forestry Generation: Evaluating Models for Tree Placement in Games,” <i>Computers</i>, vol. 9, no. 1, Mar. 2020.</span>

  <span class="toggle">
    A handful of approaches have been previously proposed to generate procedurally virtual forestry for virtual worlds and computer games, including plant growthmodels and point distribution methods. However, there has been no evaluation to date which assesses how effective these algorithms are at modelling real-world phenomena. In this paper we tackle this issue by evaluating three algorithms used in the generation of virtual forests – a randomly uniform point distribution method (control), a plant competition model, and an iterative random point distribution technique.Our results show that a plant competition model generated more believable content when viewed from an aerial perspective. Interestingly however, we also found that a randomly uniform point distribution method produced forestry which was rated higher in playability and photorealism, when viewed from a first-person perspective. We conclude that the objective of the game designer is important to consider when selecting an algorithm to generate forestry, as the algorithms produce forestry which is perceived differently.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Williams-et-al-MDPI-2020/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Williams-et-al-MDPI-2020.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.3390/computers9010020" class="doilink">10.3390/computers9010020</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Williams-et-al-MDPI-2020/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Roberts-Ritsos-EGEDU-2020">J. C. Roberts and P. D. Ritsos, “Critical Thinking Sheet (CTS) for Design Thinking in Programming Courses,” in <i>Eurographics 2020 - Education Papers</i>, 2020.</span>

  <span class="toggle">
    We present a quick design process, which encourages learners to sketch their design, reflect on the main algorithm and consider how to implement it. In-depth design processes have their advantages, but often are not practical within the time given to the student, and may not fit the learning outcomes of the module. Without any planning students often jump into coding without contemplating what they will do, leading to failure or poor design. Our single-sheet method, allows the learners to critically think of the challenge and decompose the problem into several subproblems (the appearance, functionality and algorithmic steps of the solution). We have successfully used this technique for three years in a second year computer graphics module, for undergraduate degree students studying Computer Science. We present our method, explain how we use it with second year computer graphics students, and discuss student’s experiences with the method
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-Ritsos-EGEDU-2020/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-Ritsos-EGEDU-2020.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.2312/eged.20201029" class="doilink">10.2312/eged.20201029</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Roberts-Ritsos-EGEDU-2020/"><span hidden=""></span></a></p></div>
<div class="bibliography"></div>
<div class="bibliography"><p>

<span id="Williams-et-al-VIS-2020">R. L. Williams, D. Farmer, J. C. Roberts, and P. D. Ritsos, “Immersive visualisation of COVID-19 UK travel and US happiness data,” in <i>Posters presented at the IEEE Conference on Visualization (IEEE VIS 2020), Virtual Event</i>, 2020.</span>

  <span class="toggle">
    The global COVID-19 pandemic has had great affect on the lives of everyone, from changing how children are educated to how or whether at all, we travel, go to work or do our shopping. Consequently, not only has people’s happiness changed throughout the pandemic, but there has been less vehicles on the roads. We present work to visualise both US happiness and UK travel data, as examples, in immersive environments. These impromptu visualisations encourage discussion and engagement with these topics, and can help people see the data in an alternative way.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Williams-et-al-VIS-2020/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Williams-et-al-VIS-2020.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Williams-et-al-VIS-2020/"><span hidden=""></span></a></p></div>
<div class="bibliography"></div>
<div class="bibliography"><p>

<span id="Perkins-et-al-JISC-2020">D. Dave Perkins, C. C. Gray, P. D. R. Ritsos, and L. I. Kuncheva, “JISC/Bangor University Learning Analytics Project Summary &amp; Case Study,” JISC, UK &amp; Bangor University, Commissioned Report, 2020.</span>

  <span class="toggle">
    Insights into activities we undertake as educators and students have the potential to enhance learning and reduce unintentional consequences for all. Educators have for a long time used data to monitor students and grade them. More recently additional yet still traditional metrics have been added to the available tools in every day education. The latest generation of information are derived metrics with additional intelligence. This project has developed a Work Pressure metric than can be used by both educator and learner. The focus is on the assessments for a given programme and Work Pressure that this generates. Additionally, included is behavioural characteristics, these have the potential to have significant impact upon the individual student journey.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Perkins-et-al-JISC-2020/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Perkins-et-al-JISC-2020.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Perkins-et-al-JISC-2020/"><span hidden=""></span></a></p></div>
<div class="bibliography"></div>

<h2 id="2019">2019</h2>

<div class="bibliography"></div>
<div class="bibliography"><p>

<span id="Williams-et-al-CGVC-2019">B. R. Williams, P. D. Ritsos, and C. Headleand, “Evaluating Models for Virtual Forestry Generation and Tree Placement in Games,” in <i>Proceedings of the Eurographics Conference in Computer Graphics and Visual Computing (CGVC) 2019, Bangor, UK</i>, 2019.</span>

  <span class="toggle">
    A handful of approaches have been previously proposed to generate procedurally virtual forestry for virtual worlds and computer games, including plant growth models and point distribution methods. However, there has been no evaluation to date which assesses how effective these algorithms are at modelling real-world phenomena. In this paper we tackle this issue by evaluating three algorithms used in the generation of virtual forests – a randomly uniform point distribution method (control), a plant competition model, and an iterative random point distribution technique. Our results show that a plant competition model generated more believable content when viewed from an aerial perspective. We also found that a randomly uniform point distribution method produced forest visualisations which were rated highest in playability and photorealism, when viewed from a first-person perspective. Our results indicate that when it comes to believability, the relationship between viewing perspective and procedural generation algorithm is more important than previously thought.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Williams-et-al-CGVC-2019/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Williams-et-al-CGVC-2019.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.2312/cgvc.20191259" class="doilink">10.2312/cgvc.20191259</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo"><i class="fa-solid fa-award"></i> &nbsp; Best Student Paper</span>] 
</span>
<a class="details" href="/bibliography/Williams-et-al-CGVC-2019/"><span hidden=""></span></a></p>
<p>

<span id="Jackson-et-al-CGVC-2019">J. R. Jackson, P. D. Ritsos, and J. C. Roberts, “Towards a tool for the creation of micro-visualisations,” in <i>Proceedings of the Eurographics Conference in Computer Graphics and Visual Computing (CGVC) 2019, Bangor, UK</i>, 2019.</span>

  <span class="toggle">
    As the every day use of mobile and small screen devices becomes more common, it is necessary to explore how we can visualise data effectively in small design spaces. These screens are often used in situations where it is necessary to convey information in a concise, readable, reliable and visually appealing way. Our work focuses on the design and development of a tool to facilitate the creation and manipulation of new micro-visualisations. The results show that the tool is suitable for creating large number of outputs quickly and efficiently.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Jackson-et-al-CGVC-2019/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Jackson-et-al-CGVC-2019.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.2312/cgvc.20191270" class="doilink">10.2312/cgvc.20191270</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Jackson-et-al-CGVC-2019/"><span hidden=""></span></a></p>
<p>

<span id="Butcher-et-al-LBW-CHI2019">P. W. S. Butcher, N. W. John, and P. D. Ritsos, “VRIA - A Framework for Immersive Analytics on the Web,” in <i>Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (ACM CHI 2019), Glasgow, UK</i>, 2019.</span>

  <span class="toggle">
    We report on the design, implementation and evaluation of VRIA, a framework for building immersive analytics (IA) solutions in Web-based Virtual Reality (VR), built upon WebVR, A-Frame, React and D3. The recent emergence of affordable VR interfaces have reignited the interest of researchers and developers in exploring new, immersive ways to visualize data. In particular, the use of open-standards web-based technologies for implementing VR in a browser facilitates the ubiquitous and platform-independent adoption of IA systems. Moreover, such technologies work in synergy with established visualization libraries, through the HTML document object model (DOM). We discuss high-level features of VRIA and present a preliminary user experience evaluation of one of our use cases.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Butcher-et-al-LBW-CHI2019/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Butcher-et-al-LBW-CHI2019.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1145/3290607.3312798" class="doilink">10.1145/3290607.3312798</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Butcher-et-al-LBW-CHI2019/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Edwards-Ritsos-CMHCI-2019">S. C. Edwards and P. D. Ritsos, “A Framework for Modelling Human Emotion,” in <i>Workshop on Computational Modeling in Human-Computer Interaction, CHI Conference on Human Factors in Computing Systems (ACM CHI 2019), Glasgow, UK</i>, 2019.</span>

  <span class="toggle">
    This paper describes the design of a modular framework, for constructing models of interacting systems. In particular, systems that can adapt and have different objectives; we also consider that these objectives could be of an emotional/hedonistic form. To that end, we introduce Pask’s conversation theory, and Boyd’s thoughts on decision making under uncertainty. In conclusion we describe modes of studying interacting systems.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Edwards-Ritsos-CMHCI-2019/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Edwards-Ritsos-CMHCI-2019.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Edwards-Ritsos-CMHCI-2019/"><span hidden=""></span></a></p>
<p>

<span id="Mearman-et-al-CHI-2019">J. W. Mearman, P. W. S. Butcher, P. D. Ritsos, and J. C. Roberts, “Tangible papercraft visualisations for education,” in <i>Workshop on Troubling Innovation: Craft and Computing Across Boundaries Workshop, CHI Conference on Human Factors in Computing Systems (ACM CHI 2019), Glasgow, UK</i>, 2019.</span>

  <span class="toggle">
    We have been exploring how papercraft can be used to create ‘data physicalisations’ of student data, which act as physical artefacts and data sculptures that can be used in discussions. Papercrafting is cheap and quick to produce, and easily disposed of. Papercrafting student data is powerful as it acts as a focal point for discussions about the progression of their students and the effects of any extenuating circumstances. During such meetings teachers often reference spreadsheets and dashboard visualisations to explore the data. They focus and shift their attention to individual students, often commenting on individual performance and circumstances in turn. Tangible depictions, such as the ones we present, can be passed around, facilitating discussions, and can act as a focal-point for conversation. We present several prototypes and discuss our design process.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Mearman-et-al-CHI-2019/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Mearman-et-al-CHI-2019.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Mearman-et-al-CHI-2019/"><span hidden=""></span></a></p>
<p>

<span id="Roberts-Ritsos-EduCHI-2019">J. C. Roberts and P. D. Ritsos, “Critical Thinking Sheets: Encouraging critical thought and sketched implementation design,” in <i>EduCHI 2019 Symposium: Global Perspectives on HCI Education, CHI Conference on Human Factors in Computing Systems (ACM CHI 2019), Glasgow, UK</i>, 2019.</span>

  <span class="toggle">
    Learners are often asked to create an interface as part of their course. For example, they could be asked to “create a calculator”, “develop a stopwatch” or “develop an image processing app”. But students often struggle to know how to start. At the same time, teachers want their students to think critically about their assignments and plan how they will build an interface. We have developed, and used for two academic years, a structured “critical thinking sheet (CTS)”. It is a method to help students consider a problem from different views, and help them critically consider different aspects of the task. The sheet gets the learners to (1) sketch the solution, (2) explain the challenge, (3) detail system components, (4) list algorithmic steps, and (5) explain next steps and issues of implementation. In this paper we introduce the sheet, explain how we have used it, and discuss learner experience.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-Ritsos-EduCHI-2019/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-Ritsos-EduCHI-2019.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-Ritsos-EduCHI-2019/"><span hidden=""></span></a></p></div>
<div class="bibliography"></div>
<div class="bibliography"></div>

<h2 id="2018">2018</h2>

<div class="bibliography"><p>

<span id="John-et-al-TVCG-2018">N. W. John, S. R. Pop, T. W. D. Day, P. D. Ritsos, and C. J. Headleand, “The Implementation and Validation of a Virtual Environment for Training Powered Wheelchair Manoeuvres,” <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 24, no. 5, pp. 1867–1878, May 2018.</span>

  <span class="toggle">
    Navigating a powered wheelchair and avoiding collisions is often a daunting task for new wheelchair users. It takes time and practice to gain the coordination needed to become a competent driver and this can be even more of a challenge for someone with a disability. We present a cost-effective virtual reality (VR) application that takes advantage of consumer level VR hardware. The system can be easily deployed in an assessment centre or for home use, and does not depend on a specialized high-end virtual environment such as a Powerwall or CAVE. This paper reviews previous work that has used virtual environments technology for training tasks, particularly wheelchair simulation. We then describe the implementation of our own system and the first validation study carried out using thirty three able bodied volunteers. The study results indicate that at a significance level of 5% then there is an improvement in driving skills from the use of our VR system. We thus have the potential to develop the competency of a wheelchair user whilst avoiding the risks inherent to training in the real world. However, the occurrence of cybersickness is a particular problem in this application that will need to be addressed.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/John-et-al-TVCG-2018/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/John-et-al-TVCG-2018.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2017.2700273" class="doilink">10.1109/TVCG.2017.2700273</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VR 2018</span>] 
</span>
<a class="details" href="/bibliography/John-et-al-TVCG-2018/"><span hidden=""></span></a></p>
<p>

<span id="Roberts-et-al-TVCG-2018">J. C. Roberts, P. D. Ritsos, J. Jackson, and C. Headleand, “The explanatory visualization framework: An active learning framework for teaching creative computing using explanatory visualizations,” <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 24, no. 1, pp. 791–801, Jan. 2018.</span>

  <span class="toggle">
    Visualizations are nowadays appearing in popular media and are used everyday in the workplace. This democratisation of visualization challenges educators to develop effective learning strategies, in order to train the next generation of creative visualization specialists. There is high demand for skilled individuals who can analyse a problem, consider alternative designs, develop new visualizations, and be creative and innovative. Our three-stage framework, leads the learner through a series of tasks, each designed to develop different skills necessary for coming up with creative, innovative, effective, and purposeful visualizations. For that, we get the learners to create an explanatory visualization of an algorithm of their choice. By making an algorithm choice, and by following an active-learning and project-based strategy, the learners take ownership of a particular visualization challenge. They become enthusiastic to develop good results and learn different creative skills on their learning journey.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-TVCG-2018/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-TVCG-2018.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/  10.1109/TVCG.2017.2745878" class="doilink">  10.1109/TVCG.2017.2745878</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2017</span>] 
</span>
<a class="details" href="/bibliography/Roberts-et-al-TVCG-2018/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Rizou-et-al-EWAS-2018">S. Rizou, K. Kenda, D. Kofinas, N. M. Mellios, P. Pergar, P. D. Ritsos, J. Vardakas, K. Kalaboukas, C. Laspidou, M. Senožetnik, and A. Spyropoulou, “Water4Cities: An ICT platform enabling Holistic Surface Water and Groundwater Management for Sustainable Cities,” in <i>Proceedings of 3rd EWaS International Conference, Lefkada, Greece</i>, 2018.</span>

  <span class="toggle">
    To enable effective decision-making at the entire city level, both surface water and groundwater should be viewed as part of the extended urban water ecosystem with its spatiotemporal availability, quantity, quality and competing uses being taken into account. The Water4Cities project aims to build an ICT solution for the monitoring, visualization and analysis of urban water at a holistic urban setting to provide added-value decision support services to multiple water stakeholders. This paper presents the main stakeholders identified, the overall approach and the target use cases, where Water4Cities platform will be tested and validated.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Rizou-et-al-EWAS-2018/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Rizou-et-al-EWAS-2018.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Rizou-et-al-EWAS-2018/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Varghese-et-al-VISREG-2018">D. Varghese, J. C. Roberts, and P. D. Ritsos, “Developing a formative visual feedback report for data brokering,” in <i>Workshop on Visual Summarization and Report Generation, IEEE Conference on Visualization (IEEE VIS 2018), Berlin, Germany</i>, 2018.</span>

  <span class="toggle">
    We present the development of a visualisation framework, used to provide formative feedback to clients who engage with data brokering companies. Data brokers receive, clean, store and re-sell data from many clients. However the usage of the data and the brokering process can be improved at source by enhancing the client’s data creation and management processes. We propose to achieve this through providing formative feedback, as a visualisation report, to the client. Working closely with a travel agent data broker, we present a three-part framework, where we (1) evaluate data creation and provision processes of the client, (2) develop metrics for quantitative analytics on the data, (3) aggregate the analytics in a visual report.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Varghese-et-al-VISREG-2018/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Varghese-et-al-VISREG-2018.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Varghese-et-al-VISREG-2018/"><span hidden=""></span></a></p>
<p>

<span id="Kenda-et-al-FEED-2018">K. Kenda, S. Rizou, N. Mellios, D. Kofinas, P. D. Ritsos, M. Senozetnik, and C. Laspidou, “Smart Water Management for Cities,” in <i>Fragile Earth: Theory Guided Data Science to Enhance Scientific Discovery Workshop of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD2018)</i>, 2018.</span>

  <span class="toggle">
    The deployment of real-world water monitoring and analytics tools is still far behind the growing needs of cities, which are facing constant urbanisation and overgrowth of the population. This paper presents a full-stack data-mining infrastructure for smart water management for cities being developed within Water4Cities project. The stack is tested in two use cases - Greek island of Skiathos and Slovenian capital Ljubljana, each facing its own challenges related to groundwater. Bottom layer of the platform provides data gathering and provision infrastructure based on IoT standards. The layer is enriched with a dedicated missing data imputation infrastructure, which supports coherent analysis of long-term impacts of urbanisation and population growth on groundwater reserves. Data-driven approach to groundwater levels analysis, which is important for decision support in flood and groundwater management, has shown promising results and could replace or complement traditional process-driven models. Data visualization capabilities of the platform expose powerful synergies with data mining and contribute significantly to the design of future decision support systems in water management for cities.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Kenda-et-al-FEED-2018/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Kenda-et-al-FEED-2018.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Kenda-et-al-FEED-2018/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Jackson-et-al-Poster-VIS2018">J. Jackson, P. D. Ritsos, and J. C. Roberts, “Creating Small Unit Based Glyph Visualisations,” in <i>Posters presented at the IEEE Conference on Visualization (IEEE VIS 2018), Berlin, Germany</i>, 2018.</span>

  <span class="toggle">
    Many modern day tasks involve the use of small screens, where users want to see a summary visualisation of an activity. For example, a runner using a smart watch needs to quickly view their progress, heart rate, comparison to previous races, etc. Subsequently, there is a need to portray data to users in small, yet well-defined, spaces. We define this space to be a single self-contained “unit”. In this paper we introduce a glyph visualisation algorithm that creates a diverse range of visualisation designs; each design contains many separate parts, whereupon different parameters can be mapped. Our algorithm uses a path based approach which allows designers to create deterministic, yet unique designs, in a unit space to display multivariate data.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Jackson-et-al-Poster-VIS2018/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Jackson-et-al-Poster-VIS2018.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Jackson-et-al-Poster-VIS2018/"><span hidden=""></span></a></p>
<p>

<span id="Butcher-et-al-Poster-VIS2018">P. W. S. Butcher, N. W. John, and P. D. Ritsos, “Towards a Framework for Immersive Analytics on the Web,” in <i>Posters presented at the IEEE Conference on Visualization (IEEE VIS 2018), Berlin, Germany</i>, 2018.</span>

  <span class="toggle">
    We present work-in-progress on the design and implementation of a Web framework for building Immersive Analytics (IA) solutions in Virtual Reality (VR). We outline the design of our prototype framework, VRIA, which facilitates the development of VR spaces for IA solutions, which can be accessed via a Web browser. VRIA is built on emerging open-standards Web technologies such as WebVR, A-Frame and React, and supports a variety of interaction devices (e.g., smartphones, head-mounted displays etc.). We elaborate on our motivation for focusing on open-standards Web technologies and provide an overview of our framework. We also present two early visualization components. Finally, we outline further extensions and investigations.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Butcher-et-al-Poster-VIS2018/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Butcher-et-al-Poster-VIS2018.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Butcher-et-al-Poster-VIS2018/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="ALIA16-book">P. R. Lewis, C. J. Headleand, S. Battle, and P. D. Ritsos, Eds., <i>Artificial Life and Intelligent Agents</i>, vol. 732. Springer International Publishing, 2018.</span>

  <span class="toggle">
    This book constitutes the refereed proceedings of the First International Symposium on Artificial Life and Intelligent Agents, ALIA 2014, held in Bangor, UK, in November 2014. The 10 revised full papers were carefully reviewed and selected from 20 submissions. The papers are organized in topical sections on learning and evolution; human interaction; robotic simulation.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">About</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/ALIA16-book/">Details</a>]&nbsp;&nbsp;
  
  
   
  
  
  [<span class="isbnlink">ISBN:978-3-319-90418-4</span>]&nbsp;&nbsp;
   
</span>
<a class="details" href="/bibliography/ALIA16-book/"><span hidden=""></span></a></p></div>

<h2 id="2017">2017</h2>

<div class="bibliography"><p>

<span id="Butcher-Ritsos-CW2017">P. W. Butcher and P. D. Ritsos, “Building Immersive Data Visualizations for the Web,” in <i>Proceedings of International Conference on Cyberworlds (CW’17), Chester, UK</i>, 2017.</span>

  <span class="toggle">
    We present our early work on building prototype applications for Immersive Analytics using emerging standardsbased web technologies for VR. For our preliminary investigations we visualize 3D bar charts that attempt to resemble recent physical visualizations built in the visualization community. We explore some of the challenges faced by developers in working with emerging VR tools for the web, and in building effective and informative immersive 3D visualizations.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Butcher-Ritsos-CW2017/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Butcher-Ritsos-CW2017.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/CW.2017.11" class="doilink">10.1109/CW.2017.11</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Butcher-Ritsos-CW2017/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Roberts-et-al-PDVW-2017">J. C. Roberts, P. D. Ritsos, and C. Headleand, “Experience and Guidance for the use of Sketching and low-fidelity Visualisation-design in teaching,” in <i>Pedagogy of Data Visualization Workshop, IEEE Conference on Visualization (VIS), Phoenix, Arizona, USA</i>, 2017.</span>

  <span class="toggle">
    We, like other educators, are keen to develop the next generation of visualisation designers. The use of sketching and low-fidelity designs are becoming popular methods to help developers and students consider many alternative ideas and plan what they should build. But especially within an education setting, there are often many issues that challenge students as they create low-fidelity prototypes. Students can be unwilling to contemplate alternatives, reluctant to use pens and paper, or sketch on paper, and inclined to code the first idea in their mind. In this paper we discuss these issues, and investigate strategies to help increase the breadth of low-fidelity designs, especially for developing data-visualisation tools. We draw together experiences and advice of how we have used the Five Design-Sheets method over eight years, for different assessment styles and across two institutions. This paper would be useful for anyone who wishes to use sketching in their teaching, or to improve their own experiences.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-PDVW-2017/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-PDVW-2017.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-PDVW-2017/"><span hidden=""></span></a></p>
<p>

<span id="Pereda-et-al-VIS4DH-2017">J. Pereda, P. Murietta-Flores, P. D. Ritsos, and J. C. Roberts, “Tangible User Interfaces as a Pathway for Information Visualisation for Low Digital Literacy in the Digital Humanities,” in <i>2nd Workshop on Visualization for the Digital Humanities, IEEE Conference on Visualization (VIS), Phoenix, Arizona, USA</i>, 2017.</span>

  <span class="toggle">
    Information visualisation has become a key element for empowering users to answer and produce new questions, make sense and create narratives about specific sets of information. Current technologies, such as Linked Data, have changed how researchers and professionals in the Humanities and the Heritage sector engage with information. Digital literacy is of concern in many sectors, but is especially of concern for Digital Humanities. This is due to the fact that the Humanities and Heritage sector face an important division based on digital literacy that produce gaps in the way research can be carried out. One way to overcome the challenge of digital literacy and improve access to information can be Tangible User Interfaces (TUIs), which allow a more meaningful and natural pathway for a wide range of users. TUIs make use of physical objects to interact with the computer. In particular, they can facilitate the interaction process between the user and a data visualisation system. This position paper discusses the opportunity to engage with Digital Humanities information via TUIs and data visualisation tools, offering new ways to analyse, investigate and interpret the past.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Pereda-et-al-VIS4DH-2017/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Pereda-et-al-VIS4DH-2017.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Pereda-et-al-VIS4DH-2017/"><span hidden=""></span></a></p>
<p>

<span id="Ritsos-et-al-IAW-2017">P. D. Ritsos, J. Mearman, J. R. Jackson, and J. C. Roberts, “Synthetic Visualizations in Web-based Mixed Reality,” in <i>Immersive Analytics: Exploring Future Visualization and Interaction Technologies for Data Analytics Workshop, IEEE Conference on Visualization (VIS), Phoenix, Arizona, USA</i>, 2017.</span>

  <span class="toggle">
    The way we interact with computers is constantly evolving, with technologies like Mixed/Augmented Reality (MR/AR) and the Internet of Things (IoT) set to change our perception of informational and physical space. In parallel, interest for interacting with data in new ways is driving the investigation of the synergy of these domains with data visualization. We are seeking new ways to contextualize, visualize, interact-with and interpret our data. In this paper we present the notion of Synthetic Visualizations, which enable us to visualize in situ, data embedded in physical objects, using MR. We use a combination of established ‘markers’, such as Quick Response Codes (QR Codes) and Augmented Reality Markers (AR Markers), not only to register objects in physical space, but also to contain data to be visualized, and interchange the type of visualization to be used. We visualize said data in Mixed Reality (MR), using emerging web-technologies and open-standards.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-IAW-2017/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-IAW-2017.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-IAW-2017/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Ritsos-et-al-Poster-VIS2017">P. D. Ritsos, J. Jackson, and J. C. Roberts, “Web-based Immersive Analytics in Handheld Augmented Reality,” in <i>Posters presented at the IEEE Conference on Visualization (IEEE VIS 2017), Phoenix, Arizona, USA</i>, 2017.</span>

  <span class="toggle">
    The recent popularity of virtual reality (VR), and the emergence of a number of affordable VR interfaces, have prompted researchers and developers to explore new, immersive ways to visualize data. This has resulted in a new research thrust, known as Immersive Analytics (IA). However, in IA little attention has been given to the paradigms of augmented/mixed reality (AR/MR), where computer-generated and physical objects co-exist. In this work, we explore the use of contemporary web-based technologies for the creation of immersive visualizations for handheld AR, combining D3.js with the open standards-based Argon AR framework and A-frame/WebVR. We argue in favor of using emerging standards-based web technologies as they work well with contemporary visualization tools, that are purposefully built for data binding and manipulation.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-Poster-VIS2017/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-Poster-VIS2017.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-Poster-VIS2017/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="FDS-book">J. C. Roberts, C. J. Headleand, and P. D. Ritsos, <i>Five Design-Sheets: Creative Design and Sketching for Computing and Visualisation</i>. Springer, 2017.</span>

  <span class="toggle">
    This book describes a structured sketching methodology to help you create alternative design ideas and sketch them on paper. The Five Design-Sheet method acts as a check-list of tasks, to help you think through the problem, create new ideas and to reflect upon the suitability of each idea. To complement the FdS method, we present practical sketching techniques, discuss problem solving, consider professional and ethical issues of designing interfaces, and work through many examples. Five Design-Sheets: Creative Design and Sketching for Computing and Visualization is useful for designers of computer interfaces, or researchers needing to explore alternative solutions in any field. It is written for anyone who is studying on a computing course and needs to design a computing-interface or create a well-structured design chapter for their dissertation, for example. We do acknowledge that throughout this book we focus on the creation of interactive software tools, and use the case study of building data-visualization tools. We have however, tried to keep the techniques general enough such that it is beneficial for a wide range of people, with different challenges and different situations, and for different applications. 
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">About</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/FDS-book/">Details</a>]&nbsp;&nbsp;
  
  
   
  
  
  [<span class="isbnlink">ISBN:978-3319556260</span>]&nbsp;&nbsp;
   
</span>
<a class="details" href="/bibliography/FDS-book/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Roberts-et-al-VIS2017-Tutorial">J. C. Roberts, C. Headleand, and P. D. Ritsos, “Half-day Tutorial on Sketching Visualization designs, and using the Five Design-Sheet (FdS) Methodology in Teaching,” in <i>Tutorials of at the IEEE Conference on Visualization (IEEE VIS 2017), Phoenix, AZ, USA</i>, 2017.</span>

  <span class="toggle">
    This tutorial leads attendees through sketching designs following the Five Design-Sheet methodology (FdS) and discusses how it can be used in teaching. The first part (before the break) will introduce the FdS, place it in context with other methods, discuss creative thinking and different problem types, explain the benefit of sketching designs, and provide a worked example of the FdS. The second part (after the break) focuses on using the FdS in teaching in Higher Education We give examples of students’ work, and discuss issues and challenges of using sketching for designing and prototyping in teaching, followed by a question and answer session.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-VIS2017-Tutorial/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-VIS2017-Tutorial.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-VIS2017-Tutorial/"><span hidden=""></span></a></p></div>

<h2 id="2016">2016</h2>

<div class="bibliography"><p>

<span id="Roberts-et-al-TVCG-2016">J. C. Roberts, C. Headleand, and P. D. Ritsos, “Sketching Designs Using the Five Design-Sheet Methodology,” <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 22, no. 1, pp. 419–428, Jan. 2016.</span>

  <span class="toggle">
    Sketching designs has been shown to be a useful way of planning and considering alternative solutions. The use of lo-fidelity prototyping, especially paper-based sketching, can save time, money and converge to better solutions more quickly. However, this design process is often viewed to be too informal. Consequently users do not know how to manage their thoughts and ideas (to first think divergently, to then finally converge on a suitable solution). We present the Five Design Sheet (FdS) methodology. The methodology enables users to create information visualization interfaces through lo-fidelity methods. Users sketch and plan their ideas, helping them express different possibilities, think through these ideas to consider their potential effectiveness as solutions to the task (sheet 1); they create three principle designs (sheets 2,3 and 4); before converging on a final realization design that can then be implemented (sheet 5). In this article, we present (i) a review of the use of sketching as a planning method for visualization and the benefits of sketching, (ii) a detailed description of the Five Design Sheet (FdS) methodology, and (iii) an evaluation of the FdS using the System Usability Scale, along with a case-study of its use in industry and experience of its use in teaching.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-TVCG-2016/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-TVCG-2016.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2015.2467271" class="doilink">10.1109/TVCG.2015.2467271</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2015</span>] 
</span>
<a class="details" href="/bibliography/Roberts-et-al-TVCG-2016/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Headleand-et-al-MMVR2016">C. J. Headleand, T. Day, S. R. Pop, P. D. Ritsos, and N. W. John, “A Cost-Effective Virtual Environment for Simulating and Training Powered Wheelchairs Manoeuvres,” <i>Proceedings of NextMed/MMVR22, Los Angeles, USA</i>, 2016.</span>

  <span class="toggle">
    Control of a powered wheelchair is often not intuitive, making training of new users a challenging and sometimes hazardous task. Collisions, due to a lack of experience can result in injury for the user and other individuals. By conducting training activities in virtual reality (VR), we can potentially improve driving skills whilst avoiding the risks inherent to the real world. However, until recently VR technology has been expensive and limited the commercial feasibility of a general training solution. We describe Wheelchair-Rift, a cost effective prototype simulator that makes use of the Oculus Rift head mounted display and the Leap Motion hand tracking device. It has been assessed for face validity by a panel of experts from a local Posture and Mobility Service. Initial results augur well for our cost-effective training solution.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Headleand-et-al-MMVR2016/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Headleand-et-al-MMVR2016.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  [PMID:<a href="https://pubmed.ncbi.nlm.nih.gov/27046566" class="doilink">27046566</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Headleand-et-al-MMVR2016/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Roberts-et-al-Viz4DH-VIS2016">J. C. Roberts, J. W. Mearman, P. D. Ritsos, H. C. Miles, A. T. Wilson, D. Perkins, J. R. Jackson, B. Tiddeman, F. Labrosse, B. Edwards, and R. Karl, “Immersive Analytics and Deep Maps – the Next Big Thing for Cultural Heritage &amp; Archaeology,” in <i>Visualization for Digital Humanities Workshop, IEEE Conference on Visualization (VIS), Baltimore, MD, USA</i>, 2016.</span>

  <span class="toggle">
    Archaeologists and cultural heritage experts explore complex multifaceted data that is often highly interconnected. We argue for new ways to interact with this data. Such data analysis provides a ‘grand challenge’ for computer science and heritage researchers, it is big Data, multi-dimensional, multi-typed, contains uncertain information, and the questions posed by researchers are often ill-defined (where it is difficult to guarantee an answer). We present two visions (Immersive Analytics, and Deep Mapping) as solutions to allow both expert users and the general public to interact and explore heritage data. We use pre-historic data as a case study, and discuss key technologies that need to develop further, to help accomplish these two visions.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-Viz4DH-VIS2016/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-Viz4DH-VIS2016.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-Viz4DH-VIS2016/"><span hidden=""></span></a></p>
<p>

<span id="Edwards-et-al-VCBM2016">M. R. Edwards, S. R. Pop, N. W. John, P. D. Ritsos, and N. Avis, “Real-Time Guidance and Anatomical Information by Image Projection onto Patients,” in <i>Eurographics Workshop on Visual Computing for Biology and Medicine (VCBM)</i>, 2016.</span>

  <span class="toggle">
    The Image Projection onto Patients (IPoP) system is work in progress intended to assist medical practitioners perform procedures such as biopsies, or provide a novel anatomical education tool, by projecting anatomy and other relevant information from the operating room directly onto a patient’s skin. This approach is not currently used widely in hospitals but has the benefit of providing effective procedure guidance without the practitioner having to look away from the patient. Developmental work towards the alpha-phase of IPoP is presented including tracking methods for tools such as biopsy needles, patient tracking, image registration and problems encountered with the multi-mirror effect.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Edwards-et-al-VCBM2016/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Edwards-et-al-VCBM2016.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.2312/vcbm.20161270" class="doilink">10.2312/vcbm.20161270</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Edwards-et-al-VCBM2016/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Butcher-et-al-Poster-VIS2016">P. W. S. Butcher, J. C. Roberts, and P. D. Ritsos, “Immersive Analytics with WebVR and Google Cardboard,” in <i>Posters presented at the IEEE Conference on Visualization (IEEE VIS 2016), Baltimore, MD, USA</i>, 2016.</span>

  <span class="toggle">
    We present our initial investigation of a low-cost, web-based virtual reality platform for immersive analytics, using a Google Cardboard, with a view of extending to other similar platforms such as Samsung’s Gear VR. Our prototype uses standards-based emerging frameworks, such as WebVR and explores some the challenges faced by developers in building effective and informative immersive 3D visualizations, particularly those that attempt to resemble recent physical visualizations built in the community.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Butcher-et-al-Poster-VIS2016/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Butcher-et-al-Poster-VIS2016.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Butcher-et-al-Poster-VIS2016/"><span hidden=""></span></a></p>
<p>

<span id="Roberts-et-al-Poster-VIS2016">J. C. Roberts, J. Jackson, C. Headleand, and P. D. Ritsos, “Creating Explanatory Visualizations of Algorithms for Active Learning,” in <i>Posters presented at the IEEE Conference on Visualization (IEEE VIS 2016), Baltimore, MD, USA</i>, 2016.</span>

  <span class="toggle">
    Visualizations have been used to explain algorithms to learners, in order to help them understand complex processes. These ‘explanatory visualizations’ can help learners understand computer algorithms and data-structures. But most are created by an educator and merely watched by the learner. In this paper, we explain how we get learners to plan and develop their own explanatory visualizations of algorithms. By actively developing their own visualizations learners gain a deeper insight of the algorithms that they are explaining. These depictions can also help other learners understand the algorithm.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-Poster-VIS2016/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-Poster-VIS2016.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-Poster-VIS2016/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Roberts-et-al-VIS2016-Tutorial">J. C. Roberts, C. Headleand, and P. D. Ritsos, “Sketching Designs for Data-Visualization using the Five Design-Sheet Methodology,” in <i>Tutorials of at the IEEE Conference on Visualization (IEEE VIS 2016), Baltimore, MD, USA</i>, 2016.</span>

  <span class="toggle">
    The tutorial will be useful for anyone who has to create visualization interfaces, and needs to think through different potential ways to display their data. At the end of the tutorial participants will understand techniques to help them be more structured in their ideation. They will be able to sketch interface designs using the Five Design Sheet methodology (FdS). While we know that some developers have started to use the Five Design-Sheet methodology, but this tutorial will start from the beginning and be suitable for any attendee. More information and resources are found on http://fds.design.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-VIS2016-Tutorial/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-VIS2016-Tutorial.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-VIS2016-Tutorial/"><span hidden=""></span></a></p></div>

<h2 id="2015">2015</h2>

<div class="bibliography"><p>

<span id="Miles-et-al-JOCCH-2016">H. C. Miles, A. T. Wilson, F. Labrosse, B. Tiddeman, S. Griffiths, B. Edwards, P. D. Ritsos, J. W. Mearman, K. Möller, R. Karl, and J. C. Roberts, “Alternative Representations of 3D-Reconstructed Heritage Data,” <i>ACM Journal on Computing and Cultural Heritage (JOCCH)</i>, vol. 9, no. 1, pp. 4:1–4:18, Nov. 2015.</span>

  <span class="toggle">
    By collecting images of heritage assets from members of the public and processing them to create 3D-reconstructed models, the HeritageTogether project has accomplished the digital recording of nearly 80 sites across Wales, UK. A large amount of data has been collected and produced in the form of photographs, 3D models, maps, condition reports, and more. Here we discuss some of the different methods used to realize the potential of this data in different formats and for different purposes. The data are explored in both virtual and tangible settings, and—with the use of a touch table—a combination of both. We examine some alternative representations of this community-produced heritage data for educational, research, and public engagement applications.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Miles-et-al-JOCCH-2016/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Miles-et-al-JOCCH-2016.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1145/2795233" class="doilink">10.1145/2795233</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Miles-et-al-JOCCH-2016/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Gray-et-al-VizSec2015">C. C. Gray, P. D. Ritsos, and J. C. Roberts, “Contextual Network Navigation; Situational Awareness for Network Administrators,” in <i>IEEE Symposium on Visualization for Cyber Security (VizSec), Chicago, IL, USA</i>, 2015.</span>

  <span class="toggle">
    One of the goals of network administrators is to identify and block sources of attacks from a network steam. Various tools have been developed to help the administrator identify the IP or subnet to be blocked, however these tend to be non-visual. Having a good perception of the wider network can aid the administrator identify their origin, but while network maps of the Internet can be useful for such endeavors, they are difficult to construct, comprehend and even utilize in an attack, and are often referred to as being “hairballs”. We present a visualization technique that displays pathways back to the attacker; we include all potential routing paths with a best-efforts identification of the commercial relationships involved. These two techniques can potentially highlight common pathways and/or networks to allow faster, more complete resolution to the incident, as well as fragile or incomplete routing pathways to/from a network. They can help administrators re-profile their choice of IP transit suppliers to better serve a target audience.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Gray-et-al-VizSec2015/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Gray-et-al-VizSec2015.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/VIZSEC.2015.7312769" class="doilink">10.1109/VIZSEC.2015.7312769</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Gray-et-al-VizSec2015/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Roberts-et-al-Viz4me-VIS2015">J. C. Roberts, C. Headleand, D. Perkins, and P. D. Ritsos, “Personal Visualisation for Learning,” in <i>Personal Visualization: Exploring Data in Everyday Life Workshop, IEEE Conference on Visualization (VIS), Chicago, IL, USA</i>, 2015.</span>

  <span class="toggle">
    Learners have personal data, such as grades, feedback and statistics on how they fair or compare with the class. But, data focusing on their personal learning is lacking, as it does not get updated regularly (being updated at the end of a taught session) and the displayed information is generally a single grade. Consequently, it is difficult for students to use this information to adapt their behavior, and help them on their learning journey. Yet, there is a rich set of data that could be captured and help students learn better. What is required is dynamically, regularly updated personal data, that is displayed to students in a timely way. Such ‘personal data’ can be presented to the student through ‘personal visualizations’ that engender ‘personal learning’. In this paper we discuss our journey into developing learning systems and our resulting experience with learners. We present a vision, to integrate new technologies and visualization solutions, in order to encourage and develop personal learning that employs the visualization of personal learning data.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-Viz4me-VIS2015/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-Viz4me-VIS2015.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-Viz4me-VIS2015/"><span hidden=""></span></a></p>
<p>

<span id="Headleand-et-al-BHCI2015">C. H. Headleand, L. ap Cenydd, L. Priday, P. D. Ritsos, J. C. Roberts, and W. Teahan, “Anthropomorphisation of Software Agents as a Persuasive Tool,” in <i>Understanding Persuasion: HCI as a Medium for Persuasion Workshop, British HCI</i>, 2015.</span>

  <span class="toggle">
    In this position paper, we make an argument for the anthropomorphism of software agents as a persuasive tool. We begin by discussing some of the relevant applications, before providing a brief introduction to the CASA theory of social interaction with computers. We conclude by describing a selection of the evidence for anthropomorphism, and an argument for further research into this area.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Headleand-et-al-BHCI2015/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Headleand-et-al-BHCI2015.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Headleand-et-al-BHCI2015/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Gray-et-al-Poster-VIS2015">C. C. Gray, J. C. Roberts, and P. D. Ritsos, “Where Can I Go From Here? Drawing Contextual Navigation Maps of the London Underground,” in <i>Posters presented at the IEEE Conference on Visualization (IEEE VIS 2015), Chicago, IL, USA</i>, 2015.</span>

  <span class="toggle">
    Network administrators often wish to ascertain where network attackers are located; therefore it would be useful to display the network map from the context of either the attacker’s potential location or the attacked host. As part of a bigger project we are investigating how to best visualize contextual network data. We use a dataset of station adjacencies with journey times as edge weights, to explore which visualization design is most suitable, and also ascertain the best network shortest-path metric. This short paper presents our initial findings, and a visualization for Contextual Navigation using circular, centered-phylogram projections of the network. Our visualizations are interactive allowing users to explore different scenarios and observe relative distances in the data.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Gray-et-al-Poster-VIS2015/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Gray-et-al-Poster-VIS2015.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Gray-et-al-Poster-VIS2015/"><span hidden=""></span></a></p>
<p>

<span id="Headleand-et-al-Poster-VCBM2015">C. J. Headleand, T. Day, S. R. Pop, P. D. Ritsos, and N. W. John, “Challenges and Technologies for Low Cost Wheelchair Simulation,” in <i>Eurographics Workshop on Visual Computing for Biology and Medicine</i>, 2015.</span>

  <span class="toggle">
    The use of electric wheelchairs is inherently risky, as collisions due to lack of control can result in injury for the user, but also potentially for other pedestrians. Introducing new users to powered chairs via virtual reality (VR) provides one possible solution, as it eliminates the risks inherent to the real world during training. However, traditionally simulator technology has been too expensive to make VR a financially viable solution. Also, current simulators lack the natural interaction possible in the real world, limiting their operational value. We present the early stages of a VR, electric wheelchair simulator built using low-cost, consumer level gaming hardware. The simulator makes use use of the the Leap Motion, to provide a level of interaction with the virtual world which has not previously been demonstrated in wheelchair training simulators. Furthermore, the Occulous Rift provides an immersive experience suitable for our training application
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Headleand-et-al-Poster-VCBM2015/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Headleand-et-al-Poster-VCBM2015.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.2312/vcbm.20151225" class="doilink">10.2312/vcbm.20151225</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Headleand-et-al-Poster-VCBM2015/"><span hidden=""></span></a></p>
<p>

<span id="Ritsos-et-al-Poster-VCBM2015">P. D. Ritsos, M. R. Edwards, I. S. Shergill, and N. W. John, “A Haptics-enabled Simulator for Transperineal Ultrasound-Guided Biopsy,” in <i>Eurographics Workshop on Visual Computing for Biology and Medicine</i>, 2015.</span>

  <span class="toggle">
    We present the development of a transperineal prostate biopsy, with high fidelity haptic feedback. We describe our current prototype, which is using physical props and a Geomagic Touch. In addition, we discuss a method for collecting in vitro axial needle forces, for programming haptic feedback, along with implemented an forthcoming features such as a display of 2D ultrasonic images for targeting, biopsy needle bending, prostate bleeding and calcification. Our ultimate goal is to provide an affordable high-fidelity simulation by integrating contemporary off-the-shelf technology components.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-Poster-VCBM2015/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-Poster-VCBM2015.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.2312/vcbm.20151229" class="doilink">10.2312/vcbm.20151229</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-Poster-VCBM2015/"><span hidden=""></span></a></p></div>

<h2 id="2014">2014</h2>

<div class="bibliography"><p>

<span id="Roberts-et-al-CGA2014">J. C. Roberts, P. D. Ritsos, S. K. Badam, D. Brodbeck, J. Kennedy, and N. Elmqvist, “Visualization Beyond the Desktop - the next big thing,” <i>IEEE Computer Graphics and Applications</i>, vol. 34, no. 6, pp. 26–34, Nov. 2014.</span>

  <span class="toggle">
    Visualization is coming of age. With visual depictions being seamlessly integrated into documents, and data visualization techniques being used to understand increasingly large and complex datasets, the term "visualization"’ is becoming used in everyday conversations. But we are on a cusp; visualization researchers need to develop and adapt to today’s new devices and tomorrow’s technology. Today, people interact with visual depictions through a mouse. Tomorrow, they’ll be touching, swiping, grasping, feeling, hearing, smelling, and even tasting data. The next big thing is multisensory visualization that goes beyond the desktop.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-CGA2014/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-CGA2014.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/MCG.2014.82" class="doilink">10.1109/MCG.2014.82</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2015</span>] 
</span>
<a class="details" href="/bibliography/Roberts-et-al-CGA2014/"><span hidden=""></span></a></p>
<p>

<span id="George-et-al-Evn2014">R. L. S. F. George, P. E. Robins, A. G. Davies, P. D. Ritsos, and J. C. Roberts, “Interactive visual analytics of hydrodynamic flux for the coastal zone,” <i>Environmental Earth Sciences</i>, vol. 72, no. 10, pp. 3753–3766, Nov. 2014.</span>

  <span class="toggle">
    Researchers wish to study the potential impact of sea level rise from climate change, and visual analytic tools can allow scientists to visually examine and explore different possible scenarios from simulation runs. In particular, hydrodynamic flux is calculated to understand the net movement of water; but typically this calculation is tedious and is not easily achieved with traditional visualization and analytic tools. We present a visual analytic method that incorporates a transect profiler and flux calculator. The analytic software is incorporated into our visual analytics tool Vinca, and generates multiple transects, which can be visualized and analysed in several alternative visualizations; users can choose specific transects to compare against real-world data; users can explore how flux changes within a domain. In addition, we report how ocean scientists have used our tool to display multiple-view views of their data and analyse hydrodynamic flux for the coastal zone.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/George-et-al-Evn2014/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/George-et-al-Evn2014.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1007/s12665-014-3283-9" class="doilink">10.1007/s12665-014-3283-9</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/George-et-al-Evn2014/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Ritsos-et-al-DOTD-VIS2014">P. D. Ritsos, J. W. Mearman, A. Vande Moere, and J. C. Roberts, “Sewn with Ariadne’s Thread - Visualizations for Wearable &amp; Ubiquitous Computing,” in <i>Death of the Desktop Workshop, IEEE Conference on Visualization (VIS), Paris, France</i>, 2014.</span>

  <span class="toggle">
    Lance felt a buzz on his wrist, as Alicia, his wearable, informed him via the bone-conduction ear-piece - ‘You have received an email from Dr Jones about the workshop’. His wristwatch displayed an unread email glyph icon. Lance tapped it and listened to the voice of Dr Jones, talking about the latest experiment. At the same time he scanned through the email attachments, projected in front of his eyes, through his contact lenses. One of the files had a dataset of a carbon femtotube structure
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-DOTD-VIS2014/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-DOTD-VIS2014.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-DOTD-VIS2014/"><span hidden=""></span></a></p>
<p>

<span id="Roberts-et-al-DOTD-VIS2014">J. C. Roberts, J. W. Mearman, and P. D. Ritsos, “The desktop is dead, long live the desktop! – Towards a multisensory desktop for visualization,” in <i>Death of the Desktop Workshop, IEEE Conference on Visualization (VIS), Paris, France</i>, 2014.</span>

  <span class="toggle">
    “Le roi est mort, vive le roi!”; or “The King is dead, long live the King” was a phrase originally used for the French throne of Charles VII in 1422, upon the death of his father Charles VI. To stave civil unrest the governing figures wanted perpetuation of the monarchs. Likewise, while the desktop as-we-know-it is dead (the use of the WIMP interface is becoming obsolete in visualization) it is being superseded by a new type of desktop environment: a multisensory visualization space. This space is still a personal workspace, it’s just a new kind of desk environment. Our vision is that data visualization will become more multisensory, integrating and demanding all our senses (sight, touch, audible, taste, smell etc.), to both manipulate and perceive the underlying data and information.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-DOTD-VIS2014/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-DOTD-VIS2014.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-DOTD-VIS2014/"><span hidden=""></span></a></p>
<p>

<span id="Ritsos-et-al-EGGCH2014">P. D. Ritsos, A. T. Wilson, H. C. Miles, L. F. Williams, B. Tiddeman, F. Labrosse, S. Griffiths, B. Edwards, K. Möller, R. Karl, and J. C. Roberts, “Community-driven Generation of 3D and Augmented Web Content for Archaeology,” in <i>Eurographics Workshop on Graphics and Cultural Heritage (EGGCH) - Short Papers and Posters</i>, Darmstadt, Germany, 2014, pp. 25–28.</span>

  <span class="toggle">
    Heritage sites (such as prehistoric burial cairns and standing stones) are prolific in Europe; although there is a wish to scan each of these sites, it would be time-consuming to achieve. Citizen science approaches enable us to involve the public to perform a metric survey by capturing images. In this paper, discussing work-in progress, we present our automatic process that takes the user’s uploaded photographs, converts them into 3D models and displays them in two presentation platforms – in a web gallery application, using X3D/X3DOM, and in mobile augmented reality, using awe.js
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-EGGCH2014/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-EGGCH2014.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.2312/gch.20141321" class="doilink">10.2312/gch.20141321</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-EGGCH2014/"><span hidden=""></span></a></p>
<p>

<span id="Ritsos-Roberts-EuroVA-LA-2014">P. D. Ritsos and J. C. Roberts, “Towards more Visual Analytics in Learning Analytics,” in <i>EuroVis Workshop on Visual Analytics (EuroVA)</i>, Swansea, UK, 2014, pp. 61–65.</span>

  <span class="toggle">
    Learning Analytics is the collection, management and analysis of students’ learning. It is used to enable teachers
to understand how their students are progressing and for learners to ascertain how well they are performing.
Often the data is displayed through dashboards. However, there is a huge opportunity to include more comprehensive
and interactive visualizations that provide visual depictions and analysis throughout the lifetime of the
learner, monitoring their progress from novices to experts. We therefore encourage researchers to take a comprehensive
approach and re-think how visual analytics can be applied to the learning environment, and develop more
interactive and exploratory interfaces for the learner and teacher.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-Roberts-EuroVA-LA-2014/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-Roberts-EuroVA-LA-2014.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Ritsos-Roberts-EuroVA-LA-2014/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Roberts-et-al-Poster-VIS2014">J. C. Roberts, R. T. Walker, L. Roberts, R. S. Laramee, and P. D. Ritsos, “Exploratory Visualization through Copy, Cut and Paste,” in <i>Posters presented at the IEEE Conference on Visualization (VIS), November 9-14, Paris, France</i>, 2014.</span>

  <span class="toggle">
    Our goal is to help oceanographers to visualize and navigate their data over several runs. We have been using parallel coordinate plots to display every data value. Through our copy, cut, paste interactions we aim to enable users to drill-down into specific data points and to explore the datasets in a more expressive way. The method allows users to manipulate the PCP on a ZUI canvas, take copies of the current PCP and paste different subset views.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-Poster-VIS2014/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-Poster-VIS2014.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-Poster-VIS2014/"><span hidden=""></span></a></p>
<p>

<span id="George-et-al-Poster-EuroVis2014">R. L. S. F. George, P. D. Ritsos, and J. C. Roberts, “Interactive Oceanographic Visualization using spatially-aggregated Parallel Coordinate Plots,” in <i>Posters presented at EuroVis 2014, June 9-13 , Swansea, Wales, UK</i>, 2014.</span>

  <span class="toggle">
    Visual Analytics interfaces allow ocean scientists to interactively investigate and compare different runs and parameterizations. However, oceanographic models are complex, temporal and the datasets that are generated are huge. Parallel Coordinate Plots can help explore multivariate data such as ocean-science data. Common issues with traditional PCPs of clutter and performance inhibit interactive spatial exploration. We describe techniques that aggregates the PCP based on the spatial nature of the data and we render the polylines as ranges.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/George-et-al-Poster-EuroVis2014/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/George-et-al-Poster-EuroVis2014.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/George-et-al-Poster-EuroVis2014/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Ritsos-Real-Virtuality-book">P. D. Ritsos, “Mixed Reality - A paradigm for perceiving synthetic spaces,” in <i>Real Virtuality</i>, M. Reiche and U. Gehmann, Eds. Transcript-Verlag Bielefeld, 2014, pp. 283–310.</span>

  <span class="toggle">
    As our life becomes more intertwined with technology our capabilities in inter-acting and communicating with each other take a new form. In the distant past we relied on posted letters and postcards to contact each other, often requiring a lot of days for the correspondence to reach the intended recipient. Our perception of distance from each other – and therefore our world as space – changed with the introduction of telephony. Communicating with distant relatives was easier, albeit associated with physically being present in front of a telephone and, there-fore, still locus dependent. Mobile telephony brought even further immediacy of communication. Space matters even less now. We are either within network cov-erage – but maybe in the cinema and unavailable – or somewhere with poor re-ception. From being miles and days apart, we now feel like we are mere seconds apart. Our perception of space changes, as our friends and family seem closer, despite the fact they may be, physically, in a location that a century ago would take us weeks to reach with posted mail. 
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">About</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-Real-Virtuality-book/">Details</a>]&nbsp;&nbsp;
  
  
   
  
  
  [<span class="isbnlink">ISBN:978-3-8376-2608-7</span>]&nbsp;&nbsp;
   
</span>
<a class="details" href="/bibliography/Ritsos-Real-Virtuality-book/"><span hidden=""></span></a></p></div>

<h2 id="2013">2013</h2>

<div class="bibliography"><p>

<span id="Ritsos-et-al-Lncs2013">P. D. Ritsos, R. Gittins, S. Braun, C. Slater, and J. C. Roberts, “Training Interpreters using Virtual Worlds,” in <i>Transactions on Computational Science XVIII</i>, vol. 7848, Springer Berlin Heidelberg, 2013, pp. 21–40.</span>

  <span class="toggle">
    With the rise in population migration there has been an increased need for professional interpreters who can bridge language barriers and operate in a variety of fields such as business, legal, social and medical. Interpreters require specialized training to cope with the idiosyncrasies of each field and their potential clients need to be aware of professional parlance. We present ‘Project IVY’. In IVY, users can make a selection from over 30 interpreter training scenarios situated in the 3D virtual world. Users then interpret the oral interaction of two avatar actors. In addition to creating different 3D scenarios, we have developed an asset management system for the oral files and permit users (mentors of the training interpreters) to easily upload and customize the 3D environment and observe which scenario is being used by a student. In this article we present the design and development of the IVY Virtual Environment and the asset management system. Finally we make discussion over our plans for further development.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-Lncs2013/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-Lncs2013.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1007/978-3-642-38803-3_2" class="doilink">10.1007/978-3-642-38803-3_2</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-Lncs2013/"><span hidden=""></span></a></p>
<p>

<span id="Paneels-et-al-CG2013">S. A. Panëels, P. D. Ritsos, P. J. Rodgers, and J. C. Roberts, “Prototyping 3D haptic data visualizations,” <i>Computers and Graphics</i>, vol. 37, no. 3, pp. 179–192, May 2013.</span>

  <span class="toggle">
    Haptic devices are becoming more widely used as hardware becomes available and the cost of both low and high fidelity haptic devices decreases. One of the application areas of haptics is haptic data visualization (HDV). HDV provides functionality by which users can feel and touch data. Blind and partially sighted users can benefit from HDV, as it helps them manipulate and understand information. However, developing any 3D haptic world is difficult, time-consuming and requires skilled programmers. Therefore, systems that enable haptic worlds to be rapidly developed in a simple environment could enable non-computer skilled users to create haptic 3D interactions. In this article we present HITPROTO: a system that enables users, such as mentors or support workers, to quickly create haptic interactions (with an emphasis on HDVs) through a visual programming interface. We describe HITPROTO and include details of the design and implementation. We present the results of a detailed study using postgraduate students as potential mentors, which provides evidence of the usability of HITPROTO. We also present a pilot study of HITPROTO with a blind user. It can be difficult to create prototyping tools and support 3D interactions, therefore we present a detailed list of ‘lessons learnt’ that provides a set of guidelines for developers of other 3D haptic prototyping tools.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Paneels-et-al-CG2013/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Paneels-et-al-CG2013.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1016/j.cag.2013.01.009" class="doilink">10.1016/j.cag.2013.01.009</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Paneels-et-al-CG2013/"><span hidden=""></span></a></p></div>
<div class="bibliography"></div>
<div class="bibliography"><p>

<span id="Ritsos-et-al-Poster-VIS2013">P. D. Ritsos, S. A. Panëels, P. J. Rodgers, and J. C. Roberts, “Towards a Formalized Process for Creating Haptic Data Visualizations,” in <i>Posters presented at the IEEE Conference on Visualization (VIS), October 15-18, Atlanta, Georgia, USA</i>, 2013.</span>

  <span class="toggle">
    Haptic Data Visualization (HDV) is a novel application of haptics. It provides functionality by which users touch and feel data, making it a useful tool for users with vision impairments. However, creating such visualizations usually requires programming knowledge, that support workers and tutors of blind users may not possess. To address this issue we propose a formalized process for creating HDVs using the HITPROTO [5] toolkit, which requires no programming experience. We further illustrate this process using an example HDV.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-Poster-VIS2013/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-Poster-VIS2013.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-Poster-VIS2013/"><span hidden=""></span></a></p>
<p>

<span id="Roberts-et-al-VANATO">J. C. Roberts, L. ap Cenydd, P. D. Ritsos, R. George, W. Teahan, and R. Walker, “Visual Analytics with Storyboarding to engender multivocality and comprehension of Microblog data for Crisis Management,” in <i>The Information Systems Technology Panel Symposium on Visual Analytics (IST-116/RSY-028), Shrivenham, UK</i>, 2013.</span>

<br />

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-VANATO/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-VANATO.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-VANATO/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Braun-IVY2013-book">S. Braun, C. Slater, R. Gittins, P. D. Ritsos, and J. C. Roberts, “Interpreting in Virtual Reality: designing and developing a 3D virtual world to prepare interpreters and their clients for professional practice,” in <i>New Prospects and Perspectives for Educating Language Mediators</i>, D. Kiraly, S. Hansen-Schirra, and K. Maksymski, Eds. Tuebingen : Gunter Narr, 2013, pp. 93–120.</span>

  <span class="toggle">
    This paper reports on the conceptual design and development of an avatar-based 3D virtual environment in which trainee interpreters and their potential clients (e.g. students and professionals from the fields of law, business, tourism, medicine) can explore and simulate professional interpreting practice. The focus is on business and community interpreting and hence the short consecutive and liaison interpreting modes. The environment is a product of the European collaborate project IVY (Interpreting in Virtual Reality). The paper begins with a state-of-the-art overview of the current uses of ICT in interpreter training (section 2), with a view to showing how the IVY environment has evolved out of existing knowledge of these uses, before exploring how virtual worlds are already being used for pedagogical purposes in fields related to interpreting (section 3). Section 4 then shows how existing knowledge about learning in virtual worlds has fed into the conceptual design of the IVY environment and introduces that environment, its working modes and customised digital content. This is followed by an analysis of the initial evaluation feedback on the first environment prototype (section 5), a discussion of the main pedagogical implications (section 6) and concluding remarks (section 7). The more technical aspects of the IVY environment are described in Ritsos et al. (2012).
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Braun-IVY2013-book/">Details</a>]&nbsp;&nbsp;
  
  
   
  
  
  [<span class="isbnlink">ISBN:978-3-8233-6819-9</span>]&nbsp;&nbsp;
   
</span>
<a class="details" href="/bibliography/Braun-IVY2013-book/"><span hidden=""></span></a></p></div>
<div class="bibliography"><p>

<span id="Ritsos-et-al-8th-AR-Standards">P. D. Ritsos, N. W. John, and J. C. Roberts, “Standards in Augmented Reality: Towards Prototyping Haptic Medical AR,” in <i>8th International AR Standards Meeting</i>, 2013.</span>

  <span class="toggle">
    Augmented Reality technology has been used in medical visualization applications in various different ways. Haptics, on the other hand, are a popular method of interacting in Augmented and Virtual Reality environments. We present how reliance on standards benefits the fusion of these technologies, through a series of research themes, carried out in Bangor University, UK (and international partners), as well as within the activities domain of the Research Institute of Visual Computing (RIVIC), UK.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-8th-AR-Standards/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-8th-AR-Standards.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-8th-AR-Standards/"><span hidden=""></span></a></p></div>

<h2 id="2012">2012</h2>

<div class="bibliography"><p>

<span id="Ritsos-et-al-CW2012">P. D. Ritsos, R. Gittins, J. C. Roberts, S. Braun, and C. Slater, “Using Virtual Reality for Interpreter-mediated Communication and Training,” in <i>Proceedings of International Conference on Cyberworlds (CW’12), Darmstadt, Germany</i>, 2012, pp. 191–198.</span>

  <span class="toggle">
    As international businesses adopt social media and virtual worlds as mediums for conducting international business, so there is an increasing need for interpreters who can bridge the language barriers, and work within these new spheres. The recent rise in migration (within the EU) has also increased the need for professional interpreters in business, legal, medical and other settings. Project IVY attempts to provide bespoke 3D virtual environments that are tailor made to train interpreters to work in the new digital environments, responding to this increased demand. In this paper we present the design and development of the IVY Virtual Environment. We present past and current design strategies, our implementation progress and our future plans for further development.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-CW2012/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-CW2012.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/CW.2012.34" class="doilink">10.1109/CW.2012.34</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-CW2012/"><span hidden=""></span></a></p></div>

<h2 id="2011">2011</h2>

<div class="bibliography"><p>

<span id="Ritsos-et-al-4th-AR-Standards">P. D. Ritsos, D. P. Ritsos, and A. S. Gougoulis, “Standards for Augmented Reality: a User Experience perspective,” in <i>2nd International AR Standards Meeting</i>, 2011.</span>

  <span class="toggle">
    An important aspect of designing and implementing Augmented Reality (AR) applications and services, often disregarded for the sake of simplicity and speed, is the evaluation of such systems, particularly from non-expert users, in real operating conditions. We are strong advocates of the fact that in order to develop successful and highly immersive AR systems, that can be adopted in day-today scenarios, user assessment and feedback is of paramount importance. Consequently, we also feel that an important fragment of future AR Standardisation should focus on User eXperience (UX) aspects, such as the sense of presence, ergonomics, health and safety, overall usability and product identification. Our paper attempts an examination of these aspects and proposes an adaptive theoretical evaluation framework than can be standardised across the span of AR applications.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-4th-AR-Standards/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-4th-AR-Standards.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-4th-AR-Standards/"><span hidden=""></span></a></p></div>

<h2 id="2006">2006</h2>

<p>P. D. Ritsos, Architectures for Untethered Augmented Reality Using Wearable Computers, Ph.D. dissertation, Dept. Elect systems Engineering, University of Essex, 2006</p>

<h2 id="2003">2003</h2>

<div class="bibliography"><p>

<span id="Ritsos-et-al-Eurowearable2003">P. D. Ritsos, D. J. Johnston, C. Clark, and A. F. Clark, “Engineering an augmented reality tour guide,” in <i>Eurowearable, 2003. IEE, Birmingham, UK</i>, 2003, pp. 119–124.</span>

  <span class="toggle">
    This paper describes a mobile augmented reality system intended for in situ reconstructions of archaeological sites, The evolution of the system from proof of concept to something approaching a satisfactory ergonomic design is described, as are the various approaches to achieving real-time rendering performance from the accompanying software. Finally, some comments are made concerning the accuracy of such systems. 
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-Eurowearable2003/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-Eurowearable2003.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1049/ic:20030157" class="doilink">10.1049/ic:20030157</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-Eurowearable2003/"><span hidden=""></span></a></p></div>

<h2 id="2002">2002</h2>

<div class="bibliography"><p>

<span id="Ritsos-et-al-Eurowearable2003">P. D. Ritsos, D. J. Johnston, C. Clark, and A. F. Clark, “Engineering an augmented reality tour guide,” in <i>Eurowearable, 2003. IEE, Birmingham, UK</i>, 2003, pp. 119–124.</span>

  <span class="toggle">
    This paper describes a mobile augmented reality system intended for in situ reconstructions of archaeological sites, The evolution of the system from proof of concept to something approaching a satisfactory ergonomic design is described, as are the various approaches to achieving real-time rendering performance from the accompanying software. Finally, some comments are made concerning the accuracy of such systems. 
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-Eurowearable2003/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-Eurowearable2003.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1049/ic:20030157" class="doilink">10.1049/ic:20030157</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-Eurowearable2003/"><span hidden=""></span></a></p></div>

<p> </p>

<h2 id="press">Press</h2>

<div class="pubtype">
         <ul class="pres">
            <li><a href="https://theconversation.com/apple-vision-pro-headset-what-does-it-do-and-will-it-deliver-207125?utm_source=facebook&amp;utm_medium=bylinefacebookbutton&amp;fbclid=IwAR0aLfGwna0dSDgVZbJnUfjmbo-6nVVfJv3dG1_Ke1ToguHG7mXFbotArpo">Apple Vision Pro headset: what does it do and will it deliver?</a>, <em>The Conversation, UK</em>, Jun. 2023</li>
            <li><a href="https://www.northwaleschronicle.co.uk/news/19802617.bangor-researchers-gwynedd-council-manage-flood-risk-using-visual-analytics/">Bangor researchers and Gwynedd Council to manage flood risk using visual analytics"</a>, <em>North Wales Chronicle</em>, Dec. 2021</li>
        </ul>
</div>

<p class="goto">[<a href="#publications">Go to Top</a>]</p>

<h2 id="talks--presentations">Talks &amp; Presentations</h2>

<div class="pubtype">
         <ul class="pres">
        <li>"Visualization Beyond the Desktop: Immersed in Data, Anywhere, Anytime", <em>Virtual Worlds Symposium</em>, Staffordshire University, Staffordshire, UK, Jun. 2023</li>
        <li>"STEM and Student Engagement in HE", <em>CELT Conference - Celebrating Excellence in Learning And Teaching</em>, Bangor, Gwynedd, UK, Feb. 2018</li>
         <li>"Virtual Reality Demonstrator of an Advanced Boiling Water Reactor (VRABWR)", <em>BWR Hub Conference</em>, Bangor, Gwynedd, UK, Feb. 2018</li>
         <li>"Visualization Beyond the Desktop - the Next Big Thing", <em>Love Data Week, Bangor University</em>, Bangor, Gwynedd, UK, Feb. 2018</li>              
         <li>"Synthetic Visualizations in Web-based Mixed Reality", <em>Immersive Analytics: Exploring Future Visualization and Interaction Technologies for Data Analytics Workshop, IEEE Conference on Visualization (VIS)</em>, Phoenix, Arizona, USA, Oct. 2017</li>              
         <li>"Visualization Beyond the Desktop - the next big thing", <em>IEEE Conference on Visualization (VIS 2015), Invited CG&amp;A papers</em>, Chicago, Illinois, USA, Oct. 2015</li>
         <li>"Visualization Beyond the Desktop - the next big thing", <em>Research Seminar, University of Chester</em>, Chester, UK, Oct. 2015</li>
         <li>"Sewn with Ariadne’s Thread – Visualizations for Wearable &amp; Ubiquitous Computing", <em>Death of the Desktop Workshop, IEEE Conference on Visualization (VIS 2014)</em>, Paris, Nov. 2014</li>
         <li>"Towards more Visual Analytics in Learning Analytics", <em>Fifth EuroVis Workshop on Visual Analytics (EuroVA), Eurographics Association</em>, Swansea, UK, Jun. 2014</li>
         <li>"Evaluating Interpreting in Virtual Reality", <em>New Computer Technologies - Animation and Games Workshop</em>, Bangor University, UK, May 2014</li>
         <li>"Excitement of VisWeek 2013", <em>Visualization and Medical Graphics Group Seminars</em>, Bangor University, UK, Nov. 2013</li>
         <li>"Haptic Data Visualization”,<em>Visualization and Medical Graphics Group Seminars</em>, Bangor University, UK, Oct. 2013</li>
         <li>"WeARable Computing - From the Qing Dynasty to Project Glass: Prototypes, Myths, Confusion and Lots of Wires...", <em>Visualization and Medical Graphics Group Seminars</em>, Bangor University, UK, Mar. 2013</li>
         <li>"Project IVY – Interpreting in Virtual Reality", <em>IVY Dissemination Symposium: Exploiting Emerging Technologies to Prepare Interpreters and their Clients for Professional Practice</em>, Kia Oval, London, UK, Nov. 2012</li>
         <li>"Project IVY – Interpreting in Virtual Reality", <em>Virtual Learning Technologies 2012</em>, Bangor University, UK, Oct. 2012</li>
         <li>"Interpreting in Virtual Reality", <em>Virtual Worlds Education Forum</em>, Staffordshire University, UK, Mar. 2012</li>
         <li>"Project IVY – Interpreting in Virtual Reality – Virtual Environment Development", <em>Creating Second Lives 2011: Blurring Boundaries</em>, Bangor University, UK, Sep. 2011</li>
         <li>"Project IVY – Interpreting in Virtual Reality – Virtual Environment Development", <em>Visualization and Medical Graphics Group Seminars</em>, Bangor University, UK, Sep. 2011</li>
         </ul>
         </div>

<h2 id="vocational--training-seminars">Vocational &amp; Training Seminars</h2>

<div class="pubtype">
         <ul class="pubs">
         <li><span class="bold">P. D. Ritsos</span>, P. W. S. Butcher, "XReality for Aerospace"<em>, Airbus Training Seminars, Skills Factory Programme</em>, Online, UK, Feb. 2022</li>
         <li><span class="bold">P. D. Ritsos</span>, M. Drakos, C. Vasilatos and N. Fountas, "ActionStreamer System Administrator Training"<em>, Training Seminars, Intracom-Telecom S.A.</em>, Athens, Greece, Jul. 2010</li>
         <li><span class="bold">P. D. Ritsos</span>, "Hellas OnLine (HOL) Mediation System Administrator Training"<em>,Training Seminars, Hellas OnLine S.A.</em>, Athens, Greece, Jul. 2010</li>
         </ul>
</div>


  </div>
</div>

          </div>
      </div>




    <!-- Google Analytics: change UA-XXXXX-X to be your site's ID. -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-8751583-8', 'auto');
      ga('send', 'pageview');

    </script>

    
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
      <script async src="/public/js/abstract-min.js"></script>
      <script async src="/public/js/detailsTarget-min.js"></script>
      <script async src="/public/js/award.js"></script>
    


    

  </body>
</html>
