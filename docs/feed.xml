<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-11-04T15:01:52+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Human-Centred Computing Research Group, Bangor University</title><subtitle>Human-Centred Computing research group,&lt;br/&gt; School of Computer Science and Engineering, &lt;br/&gt; College of Environmental Sciences and Engineering, Bangor University, &lt;/br&gt;, Dean Street, Bangor, Gwynedd, UK, LL57 1UT&lt;span&gt;</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><entry><title type="html">Papers for presentation at IEEE VIS2025</title><link href="http://localhost:4000/news/2025/08/11/publications/" rel="alternate" type="text/html" title="Papers for presentation at IEEE VIS2025" /><published>2025-08-11T00:00:00+02:00</published><updated>2025-08-11T00:00:00+02:00</updated><id>http://localhost:4000/news/2025/08/11/publications</id><content type="html" xml:base="http://localhost:4000/news/2025/08/11/publications/"><![CDATA[<p><img src="/assets/images/vis2025_logo.png" class="prjImgFlt" alt="IEEE VIS2025 Banner and Logo" /></p>

<p>We are participating, with several works, at <a href="https://ieeevis.org/year/2025/welcome">IEEE VIS2025</a>, to be held in Vienna, Austria.</p>

<p>We will be presenting our VIS paper <a href="/publicationsYear#Roberts-et-al-TVCG-2026">“Critical Thinking in Data Visualisation: A Framework for Contextual Scenarios”</a>, which introduces the Critical Design Strategy (CDS)—a structured heuristic evaluation method designed to facilitate the examination of visualisation designs through reflection and critical thought.</p>

<p>We will also be presenting our IEEE TVCG Paper <a href="/publicationsYear#Borowski-et-al-TVCG-2025">“DashSpace: A Cross-Reality Toolkit for Immersive Analytics”</a>, which introduces DashSpace, a cross-reality toolkit designed to support immersive analytics. This toolkit allows users to create and manipulate visualizations in both 2D and 3D environments, facilitating a more integrated approach to data analysis across different platforms. DashSpace is part of our ongoing collaboration with Aarhus Unniversity on anywhere, anytime analytics toolkits, along with our recent UIST’25 paper <a href="/publicationsYear#Borowski-et-al-UIST-2025">“Spatialstrates: Cross-Reality Collaboration through Spatial Hypermedia”</a>.</p>

<p>Related to our DashSpace project, we will be presenting a tutorial on <a href="/publicationsYear#Borowski-et-al-IEEEVIS205-Tutorial">“Live Collaborative Immersive Analytics Development with DashSpace”</a>. This tutorial will cover the DashSpace/SpatialStrate platform’s architecture, its use of web technologies, and how it supports cross-reality collaboration.</p>

<p>We are also presenting a workshop paper titled <a href="/publicationsYear#Roberts-et-al-EduVis-2025">“From Data to Insight: Using Contextual Scenarios to Teach Critical Thinking in Data Visualisation”</a>, which discusses the use of contextual scenarios to enhance data visualization education.</p>

<p>You can find more details on these papers following the reference links below.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="Roberts-et-al-TVCG-2026">J. C. Roberts, A. Hanan, O. A. E., and P. D. Ritsos, “Critical Design Strategy: a Method for Heuristically Evaluating Visualisation Designs,” <i>IEEE Transactions on Visualization and Computer Graphics</i>, 2026.</span>

  <span class="toggle">
    We present the Critical Design Strategy (CDS)—a structured method designed to facilitate the examination of visualisation designs through reflection and critical thought. The CDS helps designers think critically and make informed improvements using heuristic evaluation. When developing a visual tool or pioneering a novel visualisation approach, identifying areas for enhancement can be challenging. Critical thinking is particularly crucial for visualisation designers and tool developers, especially those new to the field, such as studying visualisation in higher education. The CDS consists of three stages across six perspectives: Stage 1 captures the essence of the idea by assigning an indicative title and selecting five adjectives (from twenty options) to form initial impressions of the design. Stage 2 involves an in-depth critique using 30 heuristic questions spanning six key perspectives—user, environment, interface, components, design, and visual marks. Stage 3 focuses on synthesising insights, reflecting on design decisions, and determining the next steps forward. We introduce the CDS and explore its use across three visualisation modules in both undergraduate and postgraduate courses. Our longstanding experience with the CDS has allowed us to refine and develop it over time: from its initial creation through workshops in 2017/18 to improvements in wording and the development of two applications by 2020, followed by the expansion of support notes and refinement of heuristics through 2023; while using it in our teaching each year. This sustained use allows us to reflect on its practical application and offer guidance on how others can incorporate it into their own work.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-TVCG-2026/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-TVCG-2026.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
   [<a href="https://arxiv.org/abs/2508.05325" class="doilink">Preprint</a>]&nbsp;&nbsp;
  
  
   [<span class="addedInfo">To be presented at IEEE VIS 2025</span>] 
</span>
<a class="details" href="/bibliography/Roberts-et-al-TVCG-2026/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Borowski-et-al-TVCG-2025">M. Borowski, P. W. S. Butcher, J. B. Kristensen, J. O. Petersen, P. D. Ritsos, C. N. Klokmose, and N. Elmqvist, “DashSpace: A Live Collaborative Platform for Immersive and Ubiquitous Analytics,” <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 31, no. 10, pp. 7034–7047, 2025.</span>

  <span class="toggle">
    We introduce DashSpace, a live collaborative immersive and ubiquitous analytics (IA/UA) platform designed for handheld and head-mounted Augmented/Extended Reality (AR/XR) implemented using WebXR and open standards. To bridge the gap between existing web-based visualizations and the immersive analytics setting, DashSpace supports visualizing both legacy D3 and Vega-Lite visualizations on 2D planes, and extruding Vega-Lite specifications into 2.5D. It also supports fully 3D visual representations using the Optomancy grammar. To facilitate authoring new visualizations in immersive XR, the platform provides a visual authoring mechanism where the user groups specification snippets to construct visualizations dynamically. The approach is fully persistent and collaborative, allowing multiple participants—whose presence is shown using 3D avatars and webcam feeds—to interact with the shared space synchronously, both co-located and remotely. We present three examples of DashSpace in action: immersive data analysis in 3D space, synchronous collaboration, and immersive data presentations.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Borowski-et-al-TVCG-2025/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Borowski-et-al-TVCG-2025.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2025.3537679" class="doilink">10.1109/TVCG.2025.3537679</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">To be presented at IEEE VIS 2025</span>] 
</span>
<a class="details" href="/bibliography/Borowski-et-al-TVCG-2025/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Borowski-et-al-IEEEVIS205-Tutorial">M. Borowski, P. W. S. Butcher, S. J. Jones, P. D. Ritsos, C. N. Klokmose, and N. Elmqvist, “Tutorial: Live Collaborative Immersive Analytics Development with DashSpace,” in <i>Tutorials of at the IEEE VIS: Visualization &amp; Visual Analytics Conference, Vienna, Austria</i>, 2025.</span>

  <span class="toggle">
    Immersive, situated, and ubiquitous analytics (IA/SA/UA) envision scenarios where heterogeneous devices can be used in combination and multiple users can work in collaboration around data. While there are many research prototypes in this domain, there is a lack of a common platform to build upon. Furthermore, many prototypes build on proprietary game engines such as Unreal or Unity, which make distribution and reuse more difficult, require niche knowledge which is not often found in enterprise environments, as well as being prone to changing license agreements. We therefore advocate for the use of open standards, such as WebXR in order to overcome such restrictions. During the last year we created two iterations of an open, malleable, and collaborative platform for IA/SA/UA: DashSpace. We published both DashSpace as well as a its core WebXR framework Spatialstrates on GitHub. DashSpace and Spatialstrates build on the know-how and technical synergies between Web-based IA/SA frameworks, such as VRIA and Wizualization, UA frameworks such as Vistrates, and dynamic-media environments such as Webstrates, all converging together since 2019, through the binding affordances that the reliance on Web technologies enables.

	We deem the tutorial framing particularly well suited to introduce DashSpace in depth to a broader audience of IA/SA/UA researchers, with the goal of creating a growing and interoperable platform for these interaction flavors. We envision to grow the platform and create a “component store” or package repository with self-contained components and packages that can be mixed and matched within the same platform without the need to install new applications on multiple devices or re-implementing common features.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Borowski-et-al-IEEEVIS205-Tutorial/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Borowski-et-al-IEEEVIS205-Tutorial.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Borowski-et-al-IEEEVIS205-Tutorial/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Roberts-et-al-EduVis-2025">J. C. Roberts, P. W. S. Butcher, and P. D. Ritsos, “From Data to Insight: Using Contextual Scenarios to Teach Critical Thinking in Data Visualisation,” in <i>IEEE VIS Workshop on Visualization Education, Literacy, and Activities</i>, 2025.</span>

  <span class="toggle">
    This paper explores the use of scenario-based visualisation examples as a pedagogical strategy for teaching students the complexities of data insight, representation, and interpretation. Teaching data visualisation often involves explaining intricate issues related to data management and the challenges of presenting data meaningfully. In this work, we present a series of data-driven scenarios. These concise stories depict specific situations, and are created to help the educators highlight key concerns in data communication, such as chart selection, temporal versus categorical comparison, visual bias, and narrative framing. By grounding these examples in real-world contexts, students are encouraged to critically assess not only what the data shows, but how and why it is shown that way. This approach aims to deepen students’ data literacy and awareness of the ethical, analytical, and rhetorical choices that shape effective visual storytelling. The paper presents a collection of example scenarios, that educators can use for their own lessons; the work fits with a larger project on looking at critical thinking in the classroom, and developing appropriate tools. We also start to abstract principles, from our approach, so that others can develop their own scenarios for their teaching. Our approach aligns with principles of authentic and scenario-based learning, using real-world contexts to foster critical engagement with data.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-EduVis-2025/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-EduVis-2025.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
   [<a href="https://arxiv.org/abs/2508.08737" class="doilink">Preprint</a>]&nbsp;&nbsp;
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-EduVis-2025/"><span hidden=""></span></a></p></div>]]></content><author><name>Panagiotis Ritsos</name></author><category term="news" /><category term="XR" /><category term="WebXR" /><category term="situated" /><category term="analytics" /><category term="visualization" /><category term="authoring" /><category term="literacy" /><summary type="html"><![CDATA[This year, at IEEE VIS2025 we will present one VIS paper, one TVCG paper, one tutorial, and one workshop paper.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/vis2025_logo.png" /><media:content medium="image" url="http://localhost:4000/assets/images/vis2025_logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Paper accepted by UIST’25</title><link href="http://localhost:4000/news/2025/07/24/publications/" rel="alternate" type="text/html" title="Paper accepted by UIST’25" /><published>2025-07-24T00:00:00+02:00</published><updated>2025-07-24T00:00:00+02:00</updated><id>http://localhost:4000/news/2025/07/24/publications</id><content type="html" xml:base="http://localhost:4000/news/2025/07/24/publications/"><![CDATA[<p><img src="/assets/images/Spatialstrates.png" class="prjImgFlt" alt="Spatialstrates" /></p>

<p>Our paper <a href="/publicationsYear#Borowski-et-al-UIST-2025">“Spatialstrates: Cross-Reality Collaboration through Spatial Hypermedia</a> has been accepted to the <a href="https://uist.acm.org/2025/">38th Annual ACM Symposium on User Interface Software and Technology (UIST 2025)</a>, to be held in Busan, South Korea, in September 2025.</p>

<p>This article continues the work on immersive toolkits, explored initially with <a href="/bibliography/Borowski-et-al-TVCG-2025/">DashSpace</a>, and extends it towards the theme of cross-reality collaboration through spatial hypermedia.</p>

<p>Contemporary consumer-level XR hardware enables immersive spatial computing, yet most knowledge work remains confined to traditional 2D desktop environments. These worlds exist in isolation: writing emails or editing presentations favors desktop interfaces, while viewing 3D simulations or architectural models benefits from immersive environments.</p>

<p>We address this fragmentation by combining spatial hypermedia, shareable dynamic media, and cross-reality computing to provide (1) composability of heterogeneous content and of nested information spaces through spatial transclusion,(2) pervasive cooperation across heterogeneous devices and platforms, and (3) congruent spatial representations despite underlying environmental differences.</p>

<p>Our implementation, the Spatialstrates platform, embodies these principles using standard web technologies to bridge 2D desktop and 3D immersive environments.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="Borowski-et-al-UIST-2025">M. Borowski, J. E. Grønbæk, P. W. S. Butcher, P. D. Ritsos, C. N. Klokmose, and N. Elmqvist, “Spatialstrates: Cross-Reality Collaboration through Spatial Hypermedia,” in <i>Proceedings of the The 38th Annual ACM Symposium on User Interface Software and Technology (UIST 2025)</i>, New York, NY, USA, 2025.</span>

  <span class="toggle">
    Consumer-level XR hardware now enables immersive spatial computing, yet most knowledge work remains confined to traditional 2D desktop environments. These worlds exist in isolation: writing emails or editing presentations favors desktop interfaces, while viewing 3D simulations or architectural models benefits from immersive environments. We address this fragmentation by combining spatial hypermedia, shareable dynamic media, and cross-reality computing to provide (1) composability of heterogeneous content and of nested information spaces through spatial transclusion,(2) pervasive cooperation across heterogeneous devices and platforms, and (3) congruent spatial representations despite underlying environmental differences. Our implementation, the Spatialstrates platform, embodies these principles using standard web technologies to bridge 2D desktop and 3D immersive environments. Through four scenarios—collaborative brainstorming, architectural design, molecular science visualization, and immersive analytics—we demonstrate how Spatialstrates enables collaboration between desktop 2D and immersive 3D contexts, allowing users to select the most appropriate interface for each task while maintaining collaborative capabilities.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Borowski-et-al-UIST-2025/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Borowski-et-al-UIST-2025.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1145/3746059.3747708" class="doilink">10.1145/3746059.3747708</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Borowski-et-al-UIST-2025/"><span hidden=""></span></a></p></div>]]></content><author><name>Panagiotis Ritsos</name></author><category term="news" /><category term="XR" /><category term="WebXR" /><category term="ACM" /><category term="hypermedia" /><category term="cross-reality" /><category term="spatial" /><category term="computing" /><category term="collaboration" /><summary type="html"><![CDATA[This year, at UIST'25 we will present SpatialStrates, a web-based platform for collaborative cross-reality computing, that bridges the gap between everyday laptop-based computing and immersive experiences with augmented or virtual reality headsets.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/Spatialstrates.png" /><media:content medium="image" url="http://localhost:4000/assets/images/Spatialstrates.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Paper published in IEEE CG&amp;amp;A</title><link href="http://localhost:4000/news/2025/04/11/publications/" rel="alternate" type="text/html" title="Paper published in IEEE CG&amp;amp;A" /><published>2025-04-11T00:00:00+02:00</published><updated>2025-04-11T00:00:00+02:00</updated><id>http://localhost:4000/news/2025/04/11/publications</id><content type="html" xml:base="http://localhost:4000/news/2025/04/11/publications/"><![CDATA[<p><img src="/assets/images/prepareVisPandemic.png" class="prjImgFlt" alt="Dashspace" /></p>

<p>Our paper <a href="/publicationsYear#McGee-et-al-CGA-2025">Preparedness for visualization in the next pandemic</a> has been published in <a href="https://www.computer.org/csdl/magazine/cg">IEEE Computer Graphics and Applications</a>. This article discusses considerations on how visualization can be best positioned to help response to future pandemics. The work is a product of the Dagstuhl Seminar 23482 on “Visualization for Pandemic Preparedness” held in January 2024. You can find more info on the seminar <a href="https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=23482">here</a>.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="McGee-et-al-CGA-2025">F. McGee, M. Abu Sin, M. Chen, D. Ebert, K. Misue, P. D. Ritsos, and A. Wulff, “Preparedness for visualization in the next pandemic,” <i>IEEE Computer Graphics and Applications</i>, vol. 45, no. 3, pp. 95–103, 2025.</span>

  <span class="toggle">
    This article discusses considerations on how visualization can be best positioned to help response to future pandemics. We examine visualization, along with the corresponding and necessary enabling technologies and platforms, as a tool to facilitate a rapid and effective response to a forthcoming pandemic. We consider challenges in terms of an infrastructure supporting world-wide response, corresponding training and stakeholder engagement, integration of future technologies, and appraisal of such systems, Finally, we discuss how addressing these challenges also helps emergency response beyond infectious diseases.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/McGee-et-al-CGA-2025/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/McGee-et-al-CGA-2025.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/MCG.2025.3561619" class="doilink">10.1109/MCG.2025.3561619</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/McGee-et-al-CGA-2025/"><span hidden=""></span></a></p></div>]]></content><author><name>Panagiotis Ritsos</name></author><category term="news" /><category term="pandemics" /><category term="visualization" /><category term="IEEE" /><category term="preparedness" /><summary type="html"><![CDATA[Our paper "Preparedness for visualization in the next pandemic" has been published in IEEE CGA.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/prepareVisPandemic.png" /><media:content medium="image" url="http://localhost:4000/assets/images/prepareVisPandemic.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Paper published in IEEE TVCG, to be presented at IEEE VIS 2025</title><link href="http://localhost:4000/news/2025/01/22/publications/" rel="alternate" type="text/html" title="Paper published in IEEE TVCG, to be presented at IEEE VIS 2025" /><published>2025-01-22T00:00:00+01:00</published><updated>2025-01-22T00:00:00+01:00</updated><id>http://localhost:4000/news/2025/01/22/publications</id><content type="html" xml:base="http://localhost:4000/news/2025/01/22/publications/"><![CDATA[<p><img src="/assets/images/dashspace.png" class="prjImgFlt" alt="Dashspace" /></p>

<p>Our paper <a href="/publicationsYear#Borowsky-et-al-TVCG-2025">DashSpace: A Live Collaborative Platform for Immersive and Ubiquitous Analytics</a> has been published in <a href="https://www.computer.org/csdl/journal/tg">IEEE Transactions on Visualization and Computer Graphics</a> and will be presented in <a href="https://ieeevis.org/year/2025/welcome">IEEE VIS 2025</a> in Vienna, Austria.</p>

<p>DashSpace is a live collaborative immersive and ubiquitous analytics (IA/UA) platform designed for handheld and head-mounted Augmented/Extended Reality (AR/XR) implemented using WebXR and open standards. To bridge the gap between existing web-based visualizations and the immersive analytics setting, DashSpace supports visualizing both legacy D3 and Vega-Lite visualizations on 2D planes, and extruding Vega-Lite specifications into 2.5D. It also supports fully 3D visual representations using the Optomancy grammar.</p>

<p>To facilitate authoring new visualizations in immersive XR, the platform provides a visual authoring mechanism where the user groups specification snippets to construct visualizations dynamically. The approach is fully persistent and collaborative, allowing multiple participants—whose presence is shown using 3D avatars and webcam feeds—to interact with the shared space synchronously, both co-located and remotely. In our paper, we present three examples of DashSpace in action: immersive data analysis in 3D space, synchronous collaboration, and immersive data presentations.</p>

<p>You can find DashSpace on <a href="https://github.com/Webstrates/DashSpace">Girhub</a>.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"></div>]]></content><author><name>Panagiotis Ritsos</name></author><category term="news" /><category term="XR" /><category term="WebXR" /><category term="IEEE" /><category term="collaborative visualization" /><category term="situated analytics" /><category term="toolkits" /><summary type="html"><![CDATA[Our paper "DashSpace A Live Collaborative Platform for Immersive and Ubiquitous Analytics" has been published in IEEE TVCG, and will be presented at IEEE VIS 2025 in Vienna, Austria.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/dashspace.png" /><media:content medium="image" url="http://localhost:4000/assets/images/dashspace.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Presented at DAFNI Wales Roadshow 2024</title><link href="http://localhost:4000/news/2024/09/20/workshop/" rel="alternate" type="text/html" title="Presented at DAFNI Wales Roadshow 2024" /><published>2024-09-20T00:00:00+02:00</published><updated>2024-09-20T00:00:00+02:00</updated><id>http://localhost:4000/news/2024/09/20/workshop</id><content type="html" xml:base="http://localhost:4000/news/2024/09/20/workshop/"><![CDATA[<p><img src="/assets/images/DAFNI-logo.jpg" class="prjImgFlt" alt="DAFNI_banner" /></p>

<p>I presented a lightning talk on our <a href="/projects/ia/">Immersive and Situated Analytics</a> systems at the DAFNI Wales Roadshow 2024 event, held at the <a href="https://www.swansea.ac.uk/computational-foundry/">Computational Foundry</a>, <a href="https://www.swansea.ac.uk/">Swansea University</a>.  We discussed research into infrastructure systems, synergies with visualization, and the challenges and opportunities of data sharing.</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="XR" /><category term="WebXR" /><category term="glyphs" /><category term="situated analytics" /><category term="toolkits" /><category term="visualization authoring" /><category term="digital twins" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Papers accepted by IEEE TVCG/ VIS2024</title><link href="http://localhost:4000/news/2024/08/09/publications/" rel="alternate" type="text/html" title="Papers accepted by IEEE TVCG/ VIS2024" /><published>2024-08-09T00:00:00+02:00</published><updated>2024-08-09T00:00:00+02:00</updated><id>http://localhost:4000/news/2024/08/09/publications</id><content type="html" xml:base="http://localhost:4000/news/2024/08/09/publications/"><![CDATA[<p><img src="/assets/images/vi2024_logo.png" class="prjImgFlt" alt="IEEE VIS2024 Banner and Logo" /></p>

<p>We presented two papers in <a href="https://ieeevis.org/year/2024/welcome">IEEE VIS2024</a>, published in <a href="https://www.computer.org/csdl/journal/tg">IEEE Transactions on Visualization and Computer Graphics</a>, in 2025.</p>

<p>In our paper <a href="/publicationsYear#Srinivasan-et-al-TVCG-2025">Attention-Aware Visualization: Tracking and Responding to User Perception Over Time</a>, we propose the notion of Attention-Aware Visualizations (AAVs) that track the user’s perception of a visual representation over time and feed this information back to the visualization. We present two separate implementations of AAV: a 2D data-agnostic method for web-based visualizations that can use an embodied eyetracker to capture the user’s gaze, and a 3D data-aware one that uses the stencil buffer to track the visibility of each individual mark in a visualization.</p>

<p>In our paper <a href="/publicationsYear#Jackson-et-al-TVCG-2025">Path-based Design Model for Constructing and Exploring Alternative Visualisations</a>, we present a path-based design model and system for designing and creating visualisations. Our model represents a systematic approach to constructing visual representations of data or concepts following a predefined sequence of steps. Through our implementation we showcase the model in action. We (1) introduce, define and explain the path model and discuss possibilities for its use, (2) present our implementation, results, and evaluation, and (3) demonstrate and evaluate an application of its use on a mobile watch.</p>

<p>You can find more details on these papers following the reference links below.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="Srinivasan-et-al-TVCG-2025">A. Srinivasan, J. Ellemose, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “Attention-Aware Visualization: Tracking and Responding to User Perception Over Time,” <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 31, no. 1, pp. 1017–1027, 2025.</span>

  <span class="toggle">
    We propose the notion of Attention-Aware Visualizations (AAVs) that track the user’s perception of a visual representation over time and feed this information back to the visualization. Such context awareness is particularly useful for ubiquitous and immersive analytics where knowing which embedded visualizations the user is looking at can be used to make visualizations react appropriately to the user’s attention: for example, by highlighting data the user has not yet seen. We can separate the approach into three components: (1) measuring the user’s gaze on a visualization and its parts; (2) tracking the user’s attention over time; and (3) reactively modifying the visual representation based on the current attention metric. In this paper, we present two separate implementations of AAV: a 2D data-agnostic method for web-based visualizations that can use an embodied eyetracker to capture the user’s gaze, and a 3D data-aware one that uses the stencil buffer to track the visibility of each individual mark in a visualization. Both methods provide similar mechanisms for accumulating attention over time and changing the appearance of marks in response. We also present results from a qualitative evaluation studying visual feedback and triggering mechanisms for capturing and revisualizing attention.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Srinivasan-et-al-TVCG-2025/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Srinivasan-et-al-TVCG-2025.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
   [<a href="https://arxiv.org/abs/2404.10732" class="doilink">Preprint</a>]&nbsp;&nbsp;
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2024.3456300" class="doilink">10.1109/TVCG.2024.3456300</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2024</span>] 
</span>
<a class="details" href="/bibliography/Srinivasan-et-al-TVCG-2025/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Jackson-et-al-TVCG-2025">J. Jackson, P. D. Ritsos, P. W. S. Butcher, and J. C. Roberts, “Path-based Design Model for Constructing and Exploring Alternative Visualisations,” <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 31, no. 1, pp. 1158–1168, 2025.</span>

  <span class="toggle">
    We present a path-based design model and system for designing and creating visualisations. Our model represents a systematic approach to constructing visual representations of data or concepts following a predefined sequence of steps. The initial step involves outlining the overall appearance of the visualisation by creating a skeleton structure, referred to as a flowpath. Subsequently, we specify objects, visual marks, properties, and appearance, storing them in a gene. Lastly, we map data onto the flowpath, ensuring suitable morphisms. Alternative designs are created by exchanging values in the gene. For example, designs that share similar traits, are created by making small incremental changes to the gene. Our design methodology fosters the generatiion of diverse creative concepts, space-filling visualisations, and traditional formats like bar charts, circular plots and pie charts. Through our implementation we showcase the model in action. As an example application, we integrate the output visualisations onto a smartwatch and visualisation dashboards. In this article we (1) introduce, define and explain the path model and discuss possibilities for its use, (2) present our implementation, results, and evaluation, and (3) demonstrate and evaluate an application of its use on a mobile watch.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Jackson-et-al-TVCG-2025/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Jackson-et-al-TVCG-2025.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
   [<a href="https://arxiv.org/abs/2408.03681" class="doilink">Preprint</a>]&nbsp;&nbsp;
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2024.3456323" class="doilink">10.1109/TVCG.2024.3456323</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2024</span>] 
</span>
<a class="details" href="/bibliography/Jackson-et-al-TVCG-2025/"><span hidden=""></span></a></p></div>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="XR" /><category term="WebXR" /><category term="IEEE" /><category term="glyphs" /><category term="situated analytics" /><category term="toolkits" /><category term="visualization authoring" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Participated in Dagstuhl Seminar - Reflections on Pandemic Visualization</title><link href="http://localhost:4000/news/2024/03/01/travel/" rel="alternate" type="text/html" title="Participated in Dagstuhl Seminar - Reflections on Pandemic Visualization" /><published>2024-03-01T00:00:00+01:00</published><updated>2024-03-01T00:00:00+01:00</updated><id>http://localhost:4000/news/2024/03/01/travel</id><content type="html" xml:base="http://localhost:4000/news/2024/03/01/travel/"><![CDATA[<p><img src="/assets/images/dagstuhl.jpg" class="prjImgFlt" alt="Image of Schloss Dagstuhl" /></p>

<p>I attended the Reflections on <a href="https://www.dagstuhl.de/en/seminars/seminar-calendar/seminar-details/24091">Reflections on Pandemic Visualization Dagstuhl Seminar</a>, held in the <a href="https://www.dagstuhl.de/en/">Computer Science Centre, Schloss Dagstuhl, Germany</a>, between 24/2 and 1/3.</p>

<p>The purpose of this Dagstuhl Seminar was to bring the visualization community, modelling experts, epidemiologists, infection control experts, and end users together with other application domain experts to reflect on research work done in response to the COVID-19 pandemic.</p>

<p>Dagstuhl Seminars are the flagship of the event program at Schloss Dagstuhl. They are often praised by participants as the most productive academic events they have ever experienced. Consequently participation is highly coveted. Their topic can be anything that is computer science related. These seminars focus on the exchange and development of ideas and not so much on the presentation of already achieved results. They profit from Dagstuhl’s ambience and setup that promote personal interaction and open discussion (<a href="https://www.dagstuhl.de/en/seminars/dagstuhl-seminars">source</a>).</p>

<p>You can find the outputs produced below.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="DagRep_14_2_191">D. Archambault, F. McGee, N. Reinoso-Schiller, T. von Landesberger, and S. Scheithauer, “Reflections on Pandemic Visualization (Dagstuhl Seminar 24091),” <i>Dagstuhl Reports</i>, vol. 14, no. 2, pp. 191–205, 2024.</span>

<br />

<span class="biblinks">
  [<a href="/bibliography/DagRep_14_2_191/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.4230/DagRep.14.2.191" class="doilink">10.4230/DagRep.14.2.191</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/DagRep_14_2_191/"><span hidden=""></span></a></p></div>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="InfoVis" /><category term="Visual Analytics" /><category term="Dagstuhl" /><category term="Epidemiology" /><category term="Public Engagement" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Paper published in IEEE CG&amp;amp;A Vis Viewpoints</title><link href="http://localhost:4000/news/2024/02/15/publications/" rel="alternate" type="text/html" title="Paper published in IEEE CG&amp;amp;A Vis Viewpoints" /><published>2024-02-15T00:00:00+01:00</published><updated>2024-02-15T00:00:00+01:00</updated><id>http://localhost:4000/news/2024/02/15/publications</id><content type="html" xml:base="http://localhost:4000/news/2024/02/15/publications/"><![CDATA[<p><img src="/assets/images/unity-webxr-benchmarks.png" class="prjImgFlt" alt="WebXR vs Game Engine Bnechmarks" /></p>

<p>Our paper <a href="/publicationsYear#Butcher-et-al-CGA2024">“Is Native Naïve? Comparing Native Game Engines and WebXR as Immersive Analytics Development Platforms,”</a> has been published in <a href="https://tc.computer.org/vgtc/conferences/currently-sponsored-events/cga-viewpoints/">IEEE Computer Graphics and Applications Visualization Viewpoints</a>.</p>

<p>In this paper, we investigate common assumptions about native game engines vs. WebXR and find that while native engines still have an advantage in many areas, WebXR is rapidly catching up and is superior for many immersive analytics applications.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="Butcher-et-al-CGA2024">P. W. S. Butcher, A. Batch, D. Saffo, B. MacIntyre, N. Elmqvist, and P. D. Ritsos, “Is Native Naïve? Comparing Native Game Engines and WebXR as Immersive Analytics Development Platforms,” <i>IEEE Computer Graphics and Applications</i>, vol. 44, no. 3, pp. 91–98, May 2024.</span>

  <span class="toggle">
    Native game engines have long been the 3D development platform of choice for research in mixed and augmented reality. For this reason they have also been adopted in many immersive visualization and immersive analytics systems and toolkits. However, with the rapid improvements of WebXR and related open technologies, this choice may not always be optimal for future visualization research. In this paper, we investigate common assumptions about native game engines vs. WebXR and find that while native engines still have an advantage in many areas, WebXR is rapidly catching up and is superior for many immersive analytics applications.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Butcher-et-al-CGA2024/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Butcher-et-al-CGA2024.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/MCG.2024.3367422" class="doilink">10.1109/MCG.2024.3367422</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Butcher-et-al-CGA2024/"><span hidden=""></span></a></p></div>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="XR" /><category term="WebXR" /><category term="IEEE" /><category term="game engine" /><category term="immersive analytics" /><category term="situated analytics" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">[“XRVA at IEEE VIS 2023”]</title><link href="http://localhost:4000/news/2023/10/21/publications/" rel="alternate" type="text/html" title="[“XRVA at IEEE VIS 2023”]" /><published>2023-10-21T00:00:00+02:00</published><updated>2023-10-21T00:00:00+02:00</updated><id>http://localhost:4000/news/2023/10/21/publications</id><content type="html" xml:base="http://localhost:4000/news/2023/10/21/publications/"><![CDATA[<p><img src="/assets/images/vis2023-banner.png" alt="VIS2023 Header" /></p>

<p>We visited Melbourne, Australia, for <a href="https://ieeevis.org/year/2023/welcome"><em>IEEE VIS2023</em></a>, the premier forum for advances in scientific and information visualization. This week-long event convenes an international community of researchers and practitioners from academia, government, and industry to explore their shared interests in tools, techniques, and technology.</p>

<p>We presented the following journals, workshop papers, and posters:</p>

<div class="bibliography"><p>

<span id="Batch-et-al-TVCG-2024">A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “Wizualization: A ’Hard Magic’ Visualization System for Immersive and Ubiquitous Analytics,” <i>IEEE Transactions on Visualization and Computer Graphics (to appear)</i>, 2024.</span>

  <span class="toggle">
    What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (SpellBook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Batch-et-al-TVCG-2024/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Batch-et-al-TVCG-2024.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2023.3326580" class="doilink">10.1109/TVCG.2023.3326580</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2023</span>] 
</span>
<a class="details" href="/bibliography/Batch-et-al-TVCG-2024/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Shin-et-al-TVCG-2023">S. Shin, A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “The Reality of the Situation: A Survey of Situated Analytics,” <i>IEEE Transactions on Visualization and Computer Graphics (to appear)</i>, 2023.</span>

  <span class="toggle">
     The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user’s physical location. In this work, we identify prior literature in this emerging field with a focus on situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. We also assess the level which these systems support the sensemaking process. Finally, we discuss insights and design guidelines that we learned from our analysis.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Shin-et-al-TVCG-2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Shin-et-al-TVCG-2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2023.3285546" class="doilink">10.1109/TVCG.2023.3285546</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2023</span>] 
</span>
<a class="details" href="/bibliography/Shin-et-al-TVCG-2023/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Chen-et-al-Bulletin-Vis4PandemRes-VIS2023">M. Chen, A. Abdul-Rahman, D. Archambault, J. Dykes, P. D. Ritsos, A. Slingsby, T. Torsney-Weir, C. Turkay, B. Bach, R. Borgo, A. Brett, H. Fang, R. Jianu, S. Khan, R. S. Laramee, L. Matthews, P. Nguyen, R. Reeve, J. C. Roberts, F. P. Vidal, Q. Wang, J. Wood, and K. Xu, “RAMPVIS: Answering the Challenges of Building Visualization Capabilities for Large-scale Emergency Responses,” in <i>Bulletins presented at the IEEE VIS Workshop on Visualization for Pandemic and Emergency Responses 2023 (Vis4PandEmRes), IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia</i>, 2023.</span>

  <span class="toggle">
    In this bulletin video, we summarize the volunteering activities of  a group of visualization researchers who provided support to  epidemiological modeling during the COVID-19 pandemic. Epidemiological modeling during a pandemic is a complex and continuous process. The intraoperative workflow entails different visualization tasks at four different levels, i.e., disseminative, observational, analytical, and model-developmental visualization. The visualization volunteers were organized into seven teams, including a generic support team, an analytical support team, a disseminative visualization team, and four modeling support teams. During the volunteering activities, we encountered a few major challenges. We made an effort to address these challenges and gained useful experience.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Chen-et-al-Bulletin-Vis4PandemRes-VIS2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Chen-et-al-Bulletin-Vis4PandemRes-VIS2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Chen-et-al-Bulletin-Vis4PandemRes-VIS2023/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Ritsos-et-al-Bulletin-Vis4PandemRes-VIS2023">P. D. Ritsos, S. Khan, S. Jones, B. Bach, J. Meikle, J. C. Roberts, J. Wood, and M. Chen, “Creating storytelling visualizations for the Covid-19 pandemic using Feature-Action Design Patterns,” in <i>Bulletins presented at the IEEE VIS Workshop on Visualization for Pandemic and Emergency Responses 2023 (Vis4PandEmRes), IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia</i>, 2023.</span>

  <span class="toggle">
    In this bulletin video, we summarize a novel technique for authoring storytelling visualization. The technique was developed by one of the teams in the RAMPVIS project, which provided visualization support to epidemiological modeling during the COVID-19 pandemic. The team explored the prevailing approaches, in the UK and internationally, for creating public-facing visualizations related to the pandemic. This ranged from those produced by a number of governments (e.g., the four home nations in the UK), organizations (e.g., WHO, UK ONS), universities (e.g., Johns Hopkins dashboards), media outlets (e.g., FT Coronavirus tracker), and non-commercial web services (e.g., Worldometers). The team concluded that we should complement, but not duplicate, the existing effort, and defined our goal as to inform the public through advanced storytelling visualization.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-Bulletin-Vis4PandemRes-VIS2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-Bulletin-Vis4PandemRes-VIS2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-Bulletin-Vis4PandemRes-VIS2023/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Rigby-et-al-Poster-VIS2023">A. Rigby, P. W. S. Butcher, R. Bellini, P. Coughlan, A. Mc Nabola, and P. D. Ritsos, “DUVis: A visual analytics tool for supporting a trans-disciplinary project,” in <i>Posters presented at the IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia</i>, 2023.</span>

  <span class="toggle">
    We present DUVis, a visual analytics application developed to support the analysis and appraisal, of the transdiciplinary project Dŵr Uisce, from internal project managers and external stakeholders. DUVis provides a number of visualizations and additional features to facilitate data exploration of a project’s progress. It presents a map of stakeholders’ activities, and their engagement with each other, as well as outputs, workpackages, their completion status and potential impact. We present our preliminary design and provide a blueprint for further development.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Rigby-et-al-Poster-VIS2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Rigby-et-al-Poster-VIS2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Rigby-et-al-Poster-VIS2023/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Roberts-et-al-Poster-VIS2023">J. C. Roberts, H. Alnjar, A. E. Owen, and P. D. Ritsos, “A method for Critical and Creative Visualisation Design-Thinking,” in <i>Posters presented at the IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia</i>, 2023.</span>

  <span class="toggle">
    Visualisation design requires critical thought: to understand important facets, investigate design suitability and explore alternatives. But, especially for learners, it can be difficult to structure a critical reflection of creative solutions. We introduce the Critical Design Survey (CDS): structured method that facilitates visualisation design analysis through reflective and critical thought. Applying the CDS helps someone to structure critical thought, provides a unified method that can be readily taught, learners can actively engage with the process and directly use it to write a critical-thinking report of their design ideas. The CDS contains three steps: Step 1, summarise and write down the essence of the idea. Step 2, perform an in-depth critique (we define 30 questions structured in six perspectives). Step 3, synthesise the ideas, implications, and decide on the next steps. We present the CDS, describe our design process (critical thinking workshops, talk aloud, and student use), and describe our use in teaching visualisation to undergraduate and postgraduate students.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-Poster-VIS2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-Poster-VIS2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-Poster-VIS2023/"><span hidden=""></span></a></p></div>

<p>You can find more information on the conference at <a href="http://ieeevis.org/">IEEE VIS2023</a>.</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="IEEE" /><category term="conference" /><category term="journal" /><category term="TVCG" /><category term="Situated Analytics" /><category term="Immersive Analytics" /><category term="Ubiquitous Analytics" /><category term="XR" /><category term="WebXR" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">[“Workshop paper accepted in HUI / ISMAR2023”]</title><link href="http://localhost:4000/news/2023/08/07/publications/" rel="alternate" type="text/html" title="[“Workshop paper accepted in HUI / ISMAR2023”]" /><published>2023-08-07T00:00:00+02:00</published><updated>2023-08-07T00:00:00+02:00</updated><id>http://localhost:4000/news/2023/08/07/publications</id><content type="html" xml:base="http://localhost:4000/news/2023/08/07/publications/"><![CDATA[<p><img src="/assets/images/wiz_hui_ismar.png" class="prjImgFlt" alt="Wizualization System Overview" /></p>

<p>Our paper <a href="/publications#Butcher-et-al-HUI-ISMAR2023">“Don’t Pull the Balrog — Lessons Learned from Designing Wizualization: a Magic-inspired Data Analytics System in XR”</a> has been accepted for publication in <a href="https://hybrid-ui-workshop.io/">HybridUI: 1st Workshop on Hybrid User Interfaces: Complementary Interfaces for Mixed Reality Interaction</a>, taking place during <a href="https://ismar23.org/">ISMAR 2023</a>.</p>

<p>In this paper, we review lessons learned during the development of <a href="/bibliography/Batch-et-al-TVCG-2024/">Wizualization</a>, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using a ‘hard-magic’ metaphor. Wizualization is based on a fundamentally hybrid and multimodal approach utilizing AR/XR, gestures, sound, and speech to support the mobile setting.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="Butcher-et-al-HUI-ISMAR2023">P. W. S. Butcher, A. Batch, P. D. Ritsos, and N. Elmqvist, “Don’t Pull the Balrog — Lessons Learned from Designing Wizualization: a Magic-inspired Data Analytics System in XR,” in <i>HybridUI: 1st Workshop on Hybrid User Interfaces: Complementary Interfaces for Mixed Reality Interaction</i>, 2023.</span>

  <span class="toggle">
    This paper presents lessons learned in the design and development of Wizualization, a ubiquitous analytics system for authoring visualizations in WebXR using a magic metaphor. The system is based on a fundamentally hybrid and multimodal approach utilizing AR/XR, gestures, sound, and speech to support the mobile setting. Our lessons include how to overcome mostly technical challenges, such as view management and combining multiple sessions in the same analytical 3D space, but also user-based, design-oriented, and even social ones. Our intention in sharing these teachings is to help fellow travellers navigate the same troubled waters we have traversed.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Butcher-et-al-HUI-ISMAR2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Butcher-et-al-HUI-ISMAR2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Butcher-et-al-HUI-ISMAR2023/"><span hidden=""></span></a></p></div>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="XR" /><category term="WebXR" /><category term="IEEE" /><category term="ISMAR" /><category term="hybrid user interfaces" /><category term="situated analytics" /><summary type="html"><![CDATA[Our paper “Don’t Pull the Balrog — Lessons Learned from Designing Wizualization: a Magic-inspired Data Analytics System in XR” has been accepted for publication in HybridUI: 1st Workshop on Hybrid User Interfaces: Complementary Interfaces for Mixed Reality Interaction, taking place during ISMAR 2023. In this paper, we review lessons learned during the development of Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using a ‘hard-magic’ metaphor. Wizualization is based on a fundamentally hybrid and multimodal approach utilizing AR/XR, gestures, sound, and speech to support the mobile setting. Reference P. W. S. Butcher, A. Batch, P. D. Ritsos, and N. Elmqvist, “Don’t Pull the Balrog — Lessons Learned from Designing Wizualization: a Magic-inspired Data Analytics System in XR,” in HybridUI: 1st Workshop on Hybrid User Interfaces: Complementary Interfaces for Mixed Reality Interaction, 2023. This paper presents lessons learned in the design and development of Wizualization, a ubiquitous analytics system for authoring visualizations in WebXR using a magic metaphor. The system is based on a fundamentally hybrid and multimodal approach utilizing AR/XR, gestures, sound, and speech to support the mobile setting. Our lessons include how to overcome mostly technical challenges, such as view management and combining multiple sessions in the same analytical 3D space, but also user-based, design-oriented, and even social ones. Our intention in sharing these teachings is to help fellow travellers navigate the same troubled waters we have traversed. [Abstract]&nbsp;&nbsp; [Details]&nbsp;&nbsp; [PDF]&nbsp;&nbsp;]]></summary></entry></feed>