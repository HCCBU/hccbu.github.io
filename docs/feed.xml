<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-09-25T13:49:24+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Human-Centred Computing Research Group, Bangor University</title><subtitle>Human-Centred Computing research group,&lt;br/&gt; School of Computer Science and Engineering, &lt;br/&gt; College of Environmental Sciences and Engineering, Bangor University, &lt;/br&gt;, Dean Street, Bangor, Gwynedd, UK, LL57 1UT&lt;span&gt;</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><entry><title type="html">[“XRVA at IEEE VIS 2023”]</title><link href="http://localhost:4000/news/2023/10/21/publications/" rel="alternate" type="text/html" title="[“XRVA at IEEE VIS 2023”]" /><published>2023-10-21T00:00:00+01:00</published><updated>2023-10-21T00:00:00+01:00</updated><id>http://localhost:4000/news/2023/10/21/publications</id><content type="html" xml:base="http://localhost:4000/news/2023/10/21/publications/"><![CDATA[<p><img src="/assets/images/vis2023-banner.png" alt="VIS2023 Header" /></p>

<p>We visited Melbourne, Australia, for <a href="https://ieeevis.org/year/2023/welcome"><em>IEEE VIS2023</em></a>, the premier forum for advances in scientific and information visualization. This week-long event convenes an international community of researchers and practitioners from academia, government, and industry to explore their shared interests in tools, techniques, and technology.</p>

<p>We presented the following journals, workshop papers, and posters:</p>

<div class="bibliography"><p>

<span id="Batch-et-al-TVCG-2024">A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “Wizualization: A ’Hard Magic’ Visualization System for Immersive and Ubiquitous Analytics,” <i>IEEE Transactions on Visualization and Computer Graphics (to appear)</i>, 2024.</span>

  <span class="toggle">
    What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (SpellBook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Batch-et-al-TVCG-2024/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Batch-et-al-TVCG-2024.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2023.3326580" class="doilink">10.1109/TVCG.2023.3326580</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2023</span>] 
</span>
<a class="details" href="/bibliography/Batch-et-al-TVCG-2024/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Shin-et-al-TVCG-2023">S. Shin, A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “The Reality of the Situation: A Survey of Situated Analytics,” <i>IEEE Transactions on Visualization and Computer Graphics (to appear)</i>, 2023.</span>

  <span class="toggle">
     The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user’s physical location. In this work, we identify prior literature in this emerging field with a focus on situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. We also assess the level which these systems support the sensemaking process. Finally, we discuss insights and design guidelines that we learned from our analysis.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Shin-et-al-TVCG-2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Shin-et-al-TVCG-2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2023.3285546" class="doilink">10.1109/TVCG.2023.3285546</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2023</span>] 
</span>
<a class="details" href="/bibliography/Shin-et-al-TVCG-2023/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Chen-et-al-Bulletin-Vis4PandemRes-VIS2023">M. Chen, A. Abdul-Rahman, D. Archambault, J. Dykes, P. D. Ritsos, A. Slingsby, T. Torsney-Weir, C. Turkay, B. Bach, R. Borgo, A. Brett, H. Fang, R. Jianu, S. Khan, R. S. Laramee, L. Matthews, P. Nguyen, R. Reeve, J. C. Roberts, F. P. Vidal, Q. Wang, J. Wood, and K. Xu, “RAMPVIS: Answering the Challenges of Building Visualization Capabilities for Large-scale Emergency Responses,” in <i>Bulletins presented at the IEEE VIS Workshop on Visualization for Pandemic and Emergency Responses 2023 (Vis4PandEmRes), IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia</i>, 2023.</span>

  <span class="toggle">
    In this bulletin video, we summarize the volunteering activities of  a group of visualization researchers who provided support to  epidemiological modeling during the COVID-19 pandemic. Epidemiological modeling during a pandemic is a complex and continuous process. The intraoperative workflow entails different visualization tasks at four different levels, i.e., disseminative, observational, analytical, and model-developmental visualization. The visualization volunteers were organized into seven teams, including a generic support team, an analytical support team, a disseminative visualization team, and four modeling support teams. During the volunteering activities, we encountered a few major challenges. We made an effort to address these challenges and gained useful experience.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Chen-et-al-Bulletin-Vis4PandemRes-VIS2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Chen-et-al-Bulletin-Vis4PandemRes-VIS2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Chen-et-al-Bulletin-Vis4PandemRes-VIS2023/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Ritsos-et-al-Bulletin-Vis4PandemRes-VIS2023">P. D. Ritsos, S. Khan, S. Jones, B. Bach, J. Meikle, J. C. Roberts, J. Wood, and M. Chen, “Creating storytelling visualizations for the Covid-19 pandemic using Feature-Action Design Patterns,” in <i>Bulletins presented at the IEEE VIS Workshop on Visualization for Pandemic and Emergency Responses 2023 (Vis4PandEmRes), IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia</i>, 2023.</span>

  <span class="toggle">
    In this bulletin video, we summarize a novel technique for authoring storytelling visualization. The technique was developed by one of the teams in the RAMPVIS project, which provided visualization support to epidemiological modeling during the COVID-19 pandemic. The team explored the prevailing approaches, in the UK and internationally, for creating public-facing visualizations related to the pandemic. This ranged from those produced by a number of governments (e.g., the four home nations in the UK), organizations (e.g., WHO, UK ONS), universities (e.g., Johns Hopkins dashboards), media outlets (e.g., FT Coronavirus tracker), and non-commercial web services (e.g., Worldometers). The team concluded that we should complement, but not duplicate, the existing effort, and defined our goal as to inform the public through advanced storytelling visualization.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Ritsos-et-al-Bulletin-Vis4PandemRes-VIS2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Ritsos-et-al-Bulletin-Vis4PandemRes-VIS2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Ritsos-et-al-Bulletin-Vis4PandemRes-VIS2023/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Rigby-et-al-Poster-VIS2023">A. Rigby, P. W. S. Butcher, R. Bellini, P. Coughlan, A. Mc Nabola, and P. D. Ritsos, “DUVis: A visual analytics tool for supporting a trans-disciplinary project,” in <i>Posters presented at the IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia</i>, 2023.</span>

  <span class="toggle">
    We present DUVis, a visual analytics application developed to support the analysis and appraisal, of the transdiciplinary project Dŵr Uisce, from internal project managers and external stakeholders. DUVis provides a number of visualizations and additional features to facilitate data exploration of a project’s progress. It presents a map of stakeholders’ activities, and their engagement with each other, as well as outputs, workpackages, their completion status and potential impact. We present our preliminary design and provide a blueprint for further development.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Rigby-et-al-Poster-VIS2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Rigby-et-al-Poster-VIS2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Rigby-et-al-Poster-VIS2023/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Roberts-et-al-Poster-VIS2023">J. C. Roberts, H. Alnjar, A. E. Owen, and P. D. Ritsos, “A method for Critical and Creative Visualisation Design-Thinking,” in <i>Posters presented at the IEEE Conference on Visualization (IEEE VIS 2023), Melbourne, Australia</i>, 2023.</span>

  <span class="toggle">
    Visualisation design requires critical thought: to understand important facets, investigate design suitability and explore alternatives. But, especially for learners, it can be difficult to structure a critical reflection of creative solutions. We introduce the Critical Design Survey (CDS): structured method that facilitates visualisation design analysis through reflective and critical thought. Applying the CDS helps someone to structure critical thought, provides a unified method that can be readily taught, learners can actively engage with the process and directly use it to write a critical-thinking report of their design ideas. The CDS contains three steps: Step 1, summarise and write down the essence of the idea. Step 2, perform an in-depth critique (we define 30 questions structured in six perspectives). Step 3, synthesise the ideas, implications, and decide on the next steps. We present the CDS, describe our design process (critical thinking workshops, talk aloud, and student use), and describe our use in teaching visualisation to undergraduate and postgraduate students.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-Poster-VIS2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-Poster-VIS2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-Poster-VIS2023/"><span hidden=""></span></a></p></div>

<p>You can find more information on the conference at <a href="http://ieeevis.org/">IEEE VIS2023</a>.</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="IEEE" /><category term="conference" /><category term="journal" /><category term="TVCG" /><category term="Situated Analytics" /><category term="Immersive Analytics" /><category term="Ubiquitous Analytics" /><category term="XR" /><category term="WebXR" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">[“Workshop paper accepted in HUI / ISMAR2023”]</title><link href="http://localhost:4000/news/2023/08/07/publications/" rel="alternate" type="text/html" title="[“Workshop paper accepted in HUI / ISMAR2023”]" /><published>2023-08-07T00:00:00+01:00</published><updated>2023-08-07T00:00:00+01:00</updated><id>http://localhost:4000/news/2023/08/07/publications</id><content type="html" xml:base="http://localhost:4000/news/2023/08/07/publications/"><![CDATA[<p><img src="/assets/images/wiz_hui_ismar.png" class="prjImgFlt" alt="Wizualization System Overview" /></p>

<p>Our paper <a href="/publications#Butcher-et-al-HUI-ISMAR2023">“Don’t Pull the Balrog — Lessons Learned from Designing Wizualization: a Magic-inspired Data Analytics System in XR”</a> has been accepted for publication in <a href="https://hybrid-ui-workshop.io/">HybridUI: 1st Workshop on Hybrid User Interfaces: Complementary Interfaces for Mixed Reality Interaction</a>, taking place during <a href="https://ismar23.org/">ISMAR 2023</a>.</p>

<p>In this paper, we review lessons learned during the development of <a href="/bibliography/Batch-et-al-TVCG-2024/">Wizualization</a>, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using a ‘hard-magic’ metaphor. Wizualization is based on a fundamentally hybrid and multimodal approach utilizing AR/XR, gestures, sound, and speech to support the mobile setting.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="Butcher-et-al-HUI-ISMAR2023">P. W. S. Butcher, A. Batch, P. D. Ritsos, and N. Elmqvist, “Don’t Pull the Balrog — Lessons Learned from Designing Wizualization: a Magic-inspired Data Analytics System in XR,” in <i>HybridUI: 1st Workshop on Hybrid User Interfaces: Complementary Interfaces for Mixed Reality Interaction</i>, 2023.</span>

  <span class="toggle">
    This paper presents lessons learned in the design and development of Wizualization, a ubiquitous analytics system for authoring visualizations in WebXR using a magic metaphor. The system is based on a fundamentally hybrid and multimodal approach utilizing AR/XR, gestures, sound, and speech to support the mobile setting. Our lessons include how to overcome mostly technical challenges, such as view management and combining multiple sessions in the same analytical 3D space, but also user-based, design-oriented, and even social ones. Our intention in sharing these teachings is to help fellow travellers navigate the same troubled waters we have traversed.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Butcher-et-al-HUI-ISMAR2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Butcher-et-al-HUI-ISMAR2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  
   
</span>
<a class="details" href="/bibliography/Butcher-et-al-HUI-ISMAR2023/"><span hidden=""></span></a></p></div>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="XR" /><category term="WebXR" /><category term="IEEE" /><category term="ISMAR" /><category term="hybrid user interfaces" /><category term="situated analytics" /><summary type="html"><![CDATA[Our paper “Don’t Pull the Balrog — Lessons Learned from Designing Wizualization: a Magic-inspired Data Analytics System in XR” has been accepted for publication in HybridUI: 1st Workshop on Hybrid User Interfaces: Complementary Interfaces for Mixed Reality Interaction, taking place during ISMAR 2023. In this paper, we review lessons learned during the development of Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using a ‘hard-magic’ metaphor. Wizualization is based on a fundamentally hybrid and multimodal approach utilizing AR/XR, gestures, sound, and speech to support the mobile setting. Reference P. W. S. Butcher, A. Batch, P. D. Ritsos, and N. Elmqvist, “Don’t Pull the Balrog — Lessons Learned from Designing Wizualization: a Magic-inspired Data Analytics System in XR,” in HybridUI: 1st Workshop on Hybrid User Interfaces: Complementary Interfaces for Mixed Reality Interaction, 2023. This paper presents lessons learned in the design and development of Wizualization, a ubiquitous analytics system for authoring visualizations in WebXR using a magic metaphor. The system is based on a fundamentally hybrid and multimodal approach utilizing AR/XR, gestures, sound, and speech to support the mobile setting. Our lessons include how to overcome mostly technical challenges, such as view management and combining multiple sessions in the same analytical 3D space, but also user-based, design-oriented, and even social ones. Our intention in sharing these teachings is to help fellow travellers navigate the same troubled waters we have traversed. [Abstract]&nbsp;&nbsp; [Details]&nbsp;&nbsp; [PDF]&nbsp;&nbsp;]]></summary></entry><entry><title type="html">[“Journal paper accepted in IEEE TVCG / VIS2023”]</title><link href="http://localhost:4000/news/2023/07/17/publications/" rel="alternate" type="text/html" title="[“Journal paper accepted in IEEE TVCG / VIS2023”]" /><published>2023-07-17T00:00:00+01:00</published><updated>2023-07-17T00:00:00+01:00</updated><id>http://localhost:4000/news/2023/07/17/publications</id><content type="html" xml:base="http://localhost:4000/news/2023/07/17/publications/"><![CDATA[<p><img src="/assets/images/wiz.png" class="prjImgFlt" alt="Motivating scenario for Wizualization" /></p>

<p>Our paper <a href="/publications#Batch-et-al-TVCG-2024">“Wizualization: A ``Hard Magic’’ Visualization System for Immersive and Ubiquitous Analytics”</a> has been accepted for publication at <a href="https://ieeevis.org/year/2023/welcome">IEEE VIS 2023</a> and will appear in <a href="https://www.computer.org/csdl/journal/tg">IEEE Transactions on Visualization and Computer Graphics</a>.</p>

<p>In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using a system based on a ‘hard-magic’ metaphor, through gestures, speech commands, and touch interaction. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data, across time and space. We describe our system using a viginette-based scenario, the actors of which are shown in our teaser, above.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="Batch-et-al-TVCG-2024">A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “Wizualization: A ’Hard Magic’ Visualization System for Immersive and Ubiquitous Analytics,” <i>IEEE Transactions on Visualization and Computer Graphics (to appear)</i>, 2024.</span>

  <span class="toggle">
    What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (SpellBook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Batch-et-al-TVCG-2024/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Batch-et-al-TVCG-2024.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2023.3326580" class="doilink">10.1109/TVCG.2023.3326580</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2023</span>] 
</span>
<a class="details" href="/bibliography/Batch-et-al-TVCG-2024/"><span hidden=""></span></a></p></div>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="journal" /><category term="TVCG" /><category term="XR" /><category term="infoVis" /><category term="WebXR" /><category term="IEEE" /><category term="VIS" /><category term="situated analytics" /><summary type="html"><![CDATA[Our paper “Wizualization: A ``Hard Magic’’ Visualization System for Immersive and Ubiquitous Analytics” has been accepted for publication at IEEE VIS 2023 and will appear in IEEE Transactions on Visualization and Computer Graphics. In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using a system based on a ‘hard-magic’ metaphor, through gestures, speech commands, and touch interaction. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data, across time and space. We describe our system using a viginette-based scenario, the actors of which are shown in our teaser, above. Reference A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “Wizualization: A ’Hard Magic’ Visualization System for Immersive and Ubiquitous Analytics,” IEEE Transactions on Visualization and Computer Graphics (to appear), 2024. What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (SpellBook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space. [Abstract]&nbsp;&nbsp; [Details]&nbsp;&nbsp; [PDF]&nbsp;&nbsp; [doi:10.1109/TVCG.2023.3326580]&nbsp;&nbsp; [Presented at IEEE VIS 2023]]]></summary></entry><entry><title type="html">[“Journal paper accepted in IEEE TVCG”]</title><link href="http://localhost:4000/news/2023/06/08/publications/" rel="alternate" type="text/html" title="[“Journal paper accepted in IEEE TVCG”]" /><published>2023-06-08T00:00:00+01:00</published><updated>2023-06-08T00:00:00+01:00</updated><id>http://localhost:4000/news/2023/06/08/publications</id><content type="html" xml:base="http://localhost:4000/news/2023/06/08/publications/"><![CDATA[<p><img src="/assets/images/situated_survey.png" class="prjImgFlt" alt="Example of a situated analytics tool that uses Augmented Reality to enable sensemaking in-situ" /></p>

<p>Our paper <a href="/publications#Shin-et-al-TVCG-2023">“The Reality of the Situation: A Survey of Situated Analytics”</a> has been accepted for publication in <a href="https://www.computer.org/csdl/journal/tg">IEEE Transactions on Visualization and Computer Graphics</a>, and will be presented, later this year, at <a href="https://ieeevis.org/year/2023/welcome">IEEE VIS 2023</a>. In this work, we identify prior literature in this emerging field with a focus on the technologies enabling such situated analytics. We identify archetypical patterns in our classification using an ensemble cluster analysis, gauge the level of support for sensemaking and discuss several insights and design guidelines that we learned from our analysis.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="Shin-et-al-TVCG-2023">S. Shin, A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “The Reality of the Situation: A Survey of Situated Analytics,” <i>IEEE Transactions on Visualization and Computer Graphics (to appear)</i>, 2023.</span>

  <span class="toggle">
     The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user’s physical location. In this work, we identify prior literature in this emerging field with a focus on situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. We also assess the level which these systems support the sensemaking process. Finally, we discuss insights and design guidelines that we learned from our analysis.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Shin-et-al-TVCG-2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Shin-et-al-TVCG-2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1109/TVCG.2023.3285546" class="doilink">10.1109/TVCG.2023.3285546</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at IEEE VIS 2023</span>] 
</span>
<a class="details" href="/bibliography/Shin-et-al-TVCG-2023/"><span hidden=""></span></a></p></div>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="journal" /><category term="TVCG" /><category term="XR" /><category term="infoVis" /><category term="WebXR" /><category term="IEEE" /><category term="situated analytics" /><summary type="html"><![CDATA[Our paper “The Reality of the Situation: A Survey of Situated Analytics” has been accepted for publication in IEEE Transactions on Visualization and Computer Graphics, and will be presented, later this year, at IEEE VIS 2023. In this work, we identify prior literature in this emerging field with a focus on the technologies enabling such situated analytics. We identify archetypical patterns in our classification using an ensemble cluster analysis, gauge the level of support for sensemaking and discuss several insights and design guidelines that we learned from our analysis. Reference S. Shin, A. Batch, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “The Reality of the Situation: A Survey of Situated Analytics,” IEEE Transactions on Visualization and Computer Graphics (to appear), 2023. The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user’s physical location. In this work, we identify prior literature in this emerging field with a focus on situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. We also assess the level which these systems support the sensemaking process. Finally, we discuss insights and design guidelines that we learned from our analysis. [Abstract]&nbsp;&nbsp; [Details]&nbsp;&nbsp; [PDF]&nbsp;&nbsp; [doi:10.1109/TVCG.2023.3285546]&nbsp;&nbsp; [Presented at IEEE VIS 2023]]]></summary></entry><entry><title type="html">[“Journal paper accepted in Computer Graphics Forum”]</title><link href="http://localhost:4000/news/2023/03/30/publications/" rel="alternate" type="text/html" title="[“Journal paper accepted in Computer Graphics Forum”]" /><published>2023-03-30T00:00:00+01:00</published><updated>2023-03-30T00:00:00+01:00</updated><id>http://localhost:4000/news/2023/03/30/publications</id><content type="html" xml:base="http://localhost:4000/news/2023/03/30/publications/"><![CDATA[<p><img src="/assets/images/holodeck-teaser.png" class="prjImgFlt" alt="Situated Analytics with Handheld AR" /></p>

<p>Our paper <a href="/publications#Batch-et-al-EuroVis-2023">“Evaluating View Management for Situated Visualization in Web-based Handheld AR”</a> has been accepted for publication in <a href="https://onlinelibrary.wiley.com/journal/14678659">Computer Graphics Forum</a>, and was presented at <a href="https://www.eurovis.org/">EuroVis 2023</a>. In this paper, we (1) present an analysis of the properties and challenges of view management for situated visualizations in Augmented Reality (AR), (2) revisit existing techniques from the domains of computer graphics, Virtual Reality (VR), and visualization, and (3) evaluate implementations of interaction, layout, and presentation of some of those techniques, accessible in web-based handheld AR.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="Batch-et-al-CGF-2023">A. Batch, S. Shin, J. Liu, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “Evaluating View Management for Situated Visualization in Web-based Handheld AR,” <i>Computer Graphics Forum</i>, vol. 42, no. 3, pp. 349–360, Jun. 2023.</span>

  <span class="toggle">
    As visualization makes the leap to mobile and situated settings, where data is increasingly integrated with the physical world using mixed reality, there is a corresponding need for effectively managing the immersed user’s view of situated visualizations. In this paper we present an analysis of view management techniques for situated 3D visualizations in handheld augmented reality: a shadowbox, a world-in-miniature metaphor, and an interactive tour. We validate these view management solutions through a concrete implementation of all techniques within a situated visualization framework built using a web-based augmented reality visualization toolkit, and present results from a user study in augmented reality accessed using handheld mobile devices.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Batch-et-al-CGF-2023/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Batch-et-al-CGF-2023.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1111/cgf.14835" class="doilink">10.1111/cgf.14835</a>]&nbsp;&nbsp;
  
   [<span class="addedInfo">Presented at EG EuroVis 2023</span>] 
</span>
<a class="details" href="/bibliography/Batch-et-al-CGF-2023/"><span hidden=""></span></a></p></div>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="journal" /><category term="Eurographics" /><category term="XR" /><category term="infoVis" /><category term="WebXR" /><category term="EuroVis" /><category term="situated analytics" /><summary type="html"><![CDATA[Our paper “Evaluating View Management for Situated Visualization in Web-based Handheld AR” has been accepted for publication in Computer Graphics Forum, and was presented at EuroVis 2023. In this paper, we (1) present an analysis of the properties and challenges of view management for situated visualizations in Augmented Reality (AR), (2) revisit existing techniques from the domains of computer graphics, Virtual Reality (VR), and visualization, and (3) evaluate implementations of interaction, layout, and presentation of some of those techniques, accessible in web-based handheld AR. Reference A. Batch, S. Shin, J. Liu, P. W. S. Butcher, P. D. Ritsos, and N. Elmqvist, “Evaluating View Management for Situated Visualization in Web-based Handheld AR,” Computer Graphics Forum, vol. 42, no. 3, pp. 349–360, Jun. 2023. As visualization makes the leap to mobile and situated settings, where data is increasingly integrated with the physical world using mixed reality, there is a corresponding need for effectively managing the immersed user’s view of situated visualizations. In this paper we present an analysis of view management techniques for situated 3D visualizations in handheld augmented reality: a shadowbox, a world-in-miniature metaphor, and an interactive tour. We validate these view management solutions through a concrete implementation of all techniques within a situated visualization framework built using a web-based augmented reality visualization toolkit, and present results from a user study in augmented reality accessed using handheld mobile devices. [Abstract]&nbsp;&nbsp; [Details]&nbsp;&nbsp; [PDF]&nbsp;&nbsp; [doi:10.1111/cgf.14835]&nbsp;&nbsp; [Presented at EG EuroVis 2023]]]></summary></entry><entry><title type="html">[“Journal paper accepted in Environmental Modelling &amp;amp; Software”]</title><link href="http://localhost:4000/news/2022/07/21/publications/" rel="alternate" type="text/html" title="[“Journal paper accepted in Environmental Modelling &amp;amp; Software”]" /><published>2022-07-21T00:00:00+01:00</published><updated>2022-07-21T00:00:00+01:00</updated><id>http://localhost:4000/news/2022/07/21/publications</id><content type="html" xml:base="http://localhost:4000/news/2022/07/21/publications/"><![CDATA[<p><img src="/assets/images/LUCSTc.png" class="prjImgFlt" alt="LUCST" /></p>

<p>Our paper <a href="/publications#Rigby-et-al-EnvModSoft-2022">“LUCST: A novel toolkit for Land Use Land Cover change assessment in SWAT+ to support flood management decisions”</a> has been accepted for publication in Elsevier’s <a href="https://www.sciencedirect.com/journal/environmental-modelling-and-software">Environmental Modelling &amp; Software</a>. LUCST is an open-source, locally hosted, JavaScript application that allows for editing and interpreting <a href="https://swat.tamu.edu/">SWAT</a>+ files relevant to land-use land-cover changes. LUCST employs visual analytics, to facilitate easier change detection in catchments, and enhances SWAT+ accessibility by reducing scenario modelling complexity.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="Rigby-et-al-EnvModSoft-2022">A. M. F. Rigby, P. W. S. Butcher, P. D. Ritsos, and S. D. Patil, “LUCST: A novel toolkit for Land Use Land Cover change assessment in SWAT+ to support flood management decisions,” <i>Environmental Modelling &amp; Software</i>, vol. 156, no. 105469, Aug. 2022.</span>

  <span class="toggle">
    Land Use Land Cover (LULC) change is widely recognised as one of the most important factors impacting the hydrological response of river basins. SWAT +, the latest version of the Soil and Water Assessment Tool, has been used extensively to assess the hydrological impacts of LULC change. However, the process of making and assessing such changes in SWAT+ is often cumbersome and non-intuitive, thereby reducing its usability amongst a wider pool of applied users. We address this issue by developing a user-friendly toolkit, Land Use Change SWAT+ Toolkit (LUCST), that will: (1) allow the end-user to define various LULC change scenarios in their study catchment, (2) run the SWAT+ model with the specified LULC changes, and (3) enable interactive visualisation of the different SWAT+ output variables. A good System Usability Score (79.8) and positive feedback from end-users promises the potential for adopting LUCST in future LULC change studies.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Rigby-et-al-EnvModSoft-2022/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Rigby-et-al-EnvModSoft-2022.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.1016/j.envsoft.2022.105469" class="doilink">10.1016/j.envsoft.2022.105469</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Rigby-et-al-EnvModSoft-2022/"><span hidden=""></span></a></p></div>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="journal" /><category term="analytics" /><category term="infoVis" /><category term="hydrology" /><category term="environmental visualisation" /><category term="flooding modelling" /><summary type="html"><![CDATA[Our paper “LUCST: A novel toolkit for Land Use Land Cover change assessment in SWAT+ to support flood management decisions” has been accepted for publication in Elsevier’s Environmental Modelling &amp; Software. LUCST is an open-source, locally hosted, JavaScript application that allows for editing and interpreting SWAT+ files relevant to land-use land-cover changes. LUCST employs visual analytics, to facilitate easier change detection in catchments, and enhances SWAT+ accessibility by reducing scenario modelling complexity. Reference A. M. F. Rigby, P. W. S. Butcher, P. D. Ritsos, and S. D. Patil, “LUCST: A novel toolkit for Land Use Land Cover change assessment in SWAT+ to support flood management decisions,” Environmental Modelling &amp; Software, vol. 156, no. 105469, Aug. 2022. Land Use Land Cover (LULC) change is widely recognised as one of the most important factors impacting the hydrological response of river basins. SWAT +, the latest version of the Soil and Water Assessment Tool, has been used extensively to assess the hydrological impacts of LULC change. However, the process of making and assessing such changes in SWAT+ is often cumbersome and non-intuitive, thereby reducing its usability amongst a wider pool of applied users. We address this issue by developing a user-friendly toolkit, Land Use Change SWAT+ Toolkit (LUCST), that will: (1) allow the end-user to define various LULC change scenarios in their study catchment, (2) run the SWAT+ model with the specified LULC changes, and (3) enable interactive visualisation of the different SWAT+ output variables. A good System Usability Score (79.8) and positive feedback from end-users promises the potential for adopting LUCST in future LULC change studies. [Abstract]&nbsp;&nbsp; [Details]&nbsp;&nbsp; [PDF]&nbsp;&nbsp; [doi:10.1016/j.envsoft.2022.105469]&nbsp;&nbsp;]]></summary></entry><entry><title type="html">[“Journal papers accepted in Epidemics and in Phil. Trans. R. Soc. A.”]</title><link href="http://localhost:4000/news/2022/04/25/publications/" rel="alternate" type="text/html" title="[“Journal papers accepted in Epidemics and in Phil. Trans. R. Soc. A.”]" /><published>2022-04-25T00:00:00+01:00</published><updated>2022-04-25T00:00:00+01:00</updated><id>http://localhost:4000/news/2022/04/25/publications</id><content type="html" xml:base="http://localhost:4000/news/2022/04/25/publications/"><![CDATA[<p><img src="/assets/images/ramvisII_teaser.png" alt="RAMPVIS Project pipeline" style="margin: 0 auto; width: 8S00px;" /></p>

<p>Two journal papers from the <a href="/projects/RAMPVIS/">RAMPVIS project</a> have been accepted for publication in Elsevier’s <a href="https://www.journals.elsevier.com/epidemics">Epidemics</a> and in the <a href="https://royalsocietypublishing.org/journal/rsta">Philosophical Transactions of the Royal Society A</a> (Special issue on ’Technical challenges of modelling real-life epidemics and examples of overcoming these’). Bangor’s contribution in both works revolves around public engagement and disseminative visualisation for the Covid-19 pandemic.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="Dykes-et-al-PhilTransA-2022">J. Dykes, A. Abdul-Rahman, D. Archambault, B. Bach, R. Borgo, M. Chen, J. Enright, H. Fang, E. E. Firat, E. Freeman, T. Gönen, C. Harris, R. Jianu, N. W. John, S. Khan, A. Lahiff, R. S. Laramee, L. Matthews, S. Mohr, P. H. Nguyen, A. A. M. Rahat, R. Reeve, P. D. Ritsos, J. C. Roberts, A. Slingsby, B. Swallow, T. Torsney-Weir, C. Turkay, R. Turner, F. P. Vidal, Q. Wang, J. Wood, and K. Xu, “Visualization for Epidemiological Modelling: Challenges, Solutions, Reflections &amp; Recommendations,” <i>Philosophical Transactions of the Royal Society A (Special issue on ’Technical challenges of modelling real-life epidemics and examples of overcoming these’) </i>, vol. 380, no. 2233, p. 20210299, Aug. 2022.</span>

  <span class="toggle">
    We report on an ongoing collaboration between epidemiological modellers and visualization researchers by documenting and reflecting upon knowledge constructs - a series of ideas, approaches and methods taken from existing visualization research and practice – deployed and developed to support modelling of the COVID-19 pandemic. Structured independent commentary on these efforts is synthesized through iterative reflection to develop: evidence of the effectiveness and value of visualization in this context; open problems upon which the research communities may focus; guidance for future activity of this type; and recommendations to safeguard the achievements and promote, advance, secure and prepare for future collaborations of this kind. In describing and comparing a series of related projects that were undertaken in unprecedented conditions, our hope is that this unique report, and its rich interactive supplementary materials, will guide the scientific community in embracing visualization in its observation, analysis and modelling of data as well as in disseminating findings. Equally we hope to encourage the visualization community to engage with impactful science in addressing its emerging data challenges. If we are successful, this showcase of activity may stimulate mutually beneficial engagement between communities with complementary expertise to address problems of significance in epidemiology and beyond.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Dykes-et-al-PhilTransA-2022/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Dykes-et-al-PhilTransA-2022.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
   [<a href="https://arxiv.org/abs/2204.06946" class="doilink">Preprint</a>]&nbsp;&nbsp;
  
  [doi:<a href="http://dx.doi.org/10.1098/rsta.2021.0299" class="doilink">10.1098/rsta.2021.0299</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Dykes-et-al-PhilTransA-2022/"><span hidden=""></span></a></p></div>

<div class="bibliography"><p>

<span id="Chen-et-al-Epidemics-2022">M. Chen, A. Abdul-Rahman, D. Archambault, J. Dykes, A. Slingsby, P. D. Ritsos, T. Torsney-Weir, C. Turkay, B. Bach, R. Borgo, A. Brett, H. Fang, R. Jianu, S. Khan, R. S. Laramee, P. H. Nguyen, R. Reeve, J. C. Robert, F. Vidal, Q. Wang, J. Wood, and K. Xu, “RAMPVIS: Answering the Challenges of Building Visualisation Capabilities for Large-scale Emergency Responses,” <i>Epidemics</i>, vol. 39, no. 100569, Jun. 2022.</span>

  <span class="toggle">
    The effort for combating the COVID-19 pandemic around the world has resulted in a huge amount of data, e.g., from testing, contact tracing, modelling, treatment, vaccine trials, and more. In addition to numerous challenges in epidemiology, healthcare, biosciences, and social sciences, there has been an urgent need to develop and provide visualisation and visual analytics (VIS) capacities to support emergency responses under difficult operational conditions. In this paper, we report the experience of a group of VIS volunteers who have been working in a large research and development consortium and providing VIS support to various observational, analytical, model-developmental, and disseminative tasks. In particular, we describe our approaches to the challenges that we have encountered in requirements analysis, data acquisition, visual design, software design, system development, team organisation, and resource planning. By reflecting on our experience, we propose a set of recommendations as the first step towards a methodology for developing and providing rapid VIS capacities to support emergency responses.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Chen-et-al-Epidemics-2022/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Chen-et-al-Epidemics-2022.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
   [<a href="https://arxiv.org/abs/2012.04757" class="doilink">Preprint</a>]&nbsp;&nbsp;
  
  [doi:<a href="http://dx.doi.org/10.1016/j.epidem.2022.100569" class="doilink">10.1016/j.epidem.2022.100569</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Chen-et-al-Epidemics-2022/"><span hidden=""></span></a></p></div>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="journal" /><category term="Covid-19" /><category term="infoVis" /><category term="disseminative visualisation" /><category term="public engagement" /><summary type="html"><![CDATA[Two journal papers from the RAMPVIS project have been accepted for publication in Elsevier’s Epidemics and in the Philosophical Transactions of the Royal Society A (Special issue on ’Technical challenges of modelling real-life epidemics and examples of overcoming these’). Bangor’s contribution in both works revolves around public engagement and disseminative visualisation for the Covid-19 pandemic. Reference J. Dykes, A. Abdul-Rahman, D. Archambault, B. Bach, R. Borgo, M. Chen, J. Enright, H. Fang, E. E. Firat, E. Freeman, T. Gönen, C. Harris, R. Jianu, N. W. John, S. Khan, A. Lahiff, R. S. Laramee, L. Matthews, S. Mohr, P. H. Nguyen, A. A. M. Rahat, R. Reeve, P. D. Ritsos, J. C. Roberts, A. Slingsby, B. Swallow, T. Torsney-Weir, C. Turkay, R. Turner, F. P. Vidal, Q. Wang, J. Wood, and K. Xu, “Visualization for Epidemiological Modelling: Challenges, Solutions, Reflections &amp; Recommendations,” Philosophical Transactions of the Royal Society A (Special issue on ’Technical challenges of modelling real-life epidemics and examples of overcoming these’) , vol. 380, no. 2233, p. 20210299, Aug. 2022. We report on an ongoing collaboration between epidemiological modellers and visualization researchers by documenting and reflecting upon knowledge constructs - a series of ideas, approaches and methods taken from existing visualization research and practice – deployed and developed to support modelling of the COVID-19 pandemic. Structured independent commentary on these efforts is synthesized through iterative reflection to develop: evidence of the effectiveness and value of visualization in this context; open problems upon which the research communities may focus; guidance for future activity of this type; and recommendations to safeguard the achievements and promote, advance, secure and prepare for future collaborations of this kind. In describing and comparing a series of related projects that were undertaken in unprecedented conditions, our hope is that this unique report, and its rich interactive supplementary materials, will guide the scientific community in embracing visualization in its observation, analysis and modelling of data as well as in disseminating findings. Equally we hope to encourage the visualization community to engage with impactful science in addressing its emerging data challenges. If we are successful, this showcase of activity may stimulate mutually beneficial engagement between communities with complementary expertise to address problems of significance in epidemiology and beyond. [Abstract]&nbsp;&nbsp; [Details]&nbsp;&nbsp; [PDF]&nbsp;&nbsp; [Preprint]&nbsp;&nbsp; [doi:10.1098/rsta.2021.0299]&nbsp;&nbsp; M. Chen, A. Abdul-Rahman, D. Archambault, J. Dykes, A. Slingsby, P. D. Ritsos, T. Torsney-Weir, C. Turkay, B. Bach, R. Borgo, A. Brett, H. Fang, R. Jianu, S. Khan, R. S. Laramee, P. H. Nguyen, R. Reeve, J. C. Robert, F. Vidal, Q. Wang, J. Wood, and K. Xu, “RAMPVIS: Answering the Challenges of Building Visualisation Capabilities for Large-scale Emergency Responses,” Epidemics, vol. 39, no. 100569, Jun. 2022. The effort for combating the COVID-19 pandemic around the world has resulted in a huge amount of data, e.g., from testing, contact tracing, modelling, treatment, vaccine trials, and more. In addition to numerous challenges in epidemiology, healthcare, biosciences, and social sciences, there has been an urgent need to develop and provide visualisation and visual analytics (VIS) capacities to support emergency responses under difficult operational conditions. In this paper, we report the experience of a group of VIS volunteers who have been working in a large research and development consortium and providing VIS support to various observational, analytical, model-developmental, and disseminative tasks. In particular, we describe our approaches to the challenges that we have encountered in requirements analysis, data acquisition, visual design, software design, system development, team organisation, and resource planning. By reflecting on our experience, we propose a set of recommendations as the first step towards a methodology for developing and providing rapid VIS capacities to support emergency responses. [Abstract]&nbsp;&nbsp; [Details]&nbsp;&nbsp; [PDF]&nbsp;&nbsp; [Preprint]&nbsp;&nbsp; [doi:10.1016/j.epidem.2022.100569]&nbsp;&nbsp;]]></summary></entry><entry><title type="html">[“Delivered an XReality course at Airbus”]</title><link href="http://localhost:4000/news/2022/03/08/teaching/" rel="alternate" type="text/html" title="[“Delivered an XReality course at Airbus”]" /><published>2022-03-08T00:00:00+00:00</published><updated>2022-03-08T00:00:00+00:00</updated><id>http://localhost:4000/news/2022/03/08/teaching</id><content type="html" xml:base="http://localhost:4000/news/2022/03/08/teaching/"><![CDATA[<p>Between 15th of February and the 8th of March, we delivered, along with <a href="https://www.bangor.ac.uk/staff/computer-science-electronic-engineering/peter-butcher-080986/en">Dr Peter Butcher</a>, a course on XReality, to <a href="https://www.airbus.com/en">Airbus</a> employees. The course is part of the <a href="https://dsp-centre.bangor.ac.uk/">DSP Centre’s</a> Educational College and was delivered within the <a href="https://www.bangor.ac.uk/news/bangor-university-and-coleg-cambria-announce-a-new-collaborative-relationship">Skills Factory programme</a>, for which the <a href="https://www.bangor.ac.uk/computer-science-and-electronic-engineering/index.php.en">School of Computer Science and Electronic Engineering</a>, <a href="https://www.bangor.ac.uk/">Bangor University</a> is a primary contributor.</p>

<p><img src="/assets/images/XR_Airbus.jpg" alt="XReality course delivered at Airbus" style="margin: 10 auto; width: 800px;" /></p>

<p>The course provided and introduction to Virtual, Augmented and Mixed Reality, discussed some of the challenges of these paradigms, and presented some of our demos in <a href="/projects/ia/">Immersive Analytics</a> (see image above, of the VRIA framework in MR, through the Microsoft HoloLens 2). The course was delivered over four sessions.</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="XR" /><category term="future networks" /><category term="virtual environments" /><category term="digital twins" /><summary type="html"><![CDATA[Between 15th of February and the 8th of March, we delivered, along with Dr Peter Butcher, a course on XReality, to Airbus employees. The course is part of the DSP Centre’s Educational College and was delivered within the Skills Factory programme, for which the School of Computer Science and Electronic Engineering, Bangor University is a primary contributor. The course provided and introduction to Virtual, Augmented and Mixed Reality, discussed some of the challenges of these paradigms, and presented some of our demos in Immersive Analytics (see image above, of the VRIA framework in MR, through the Microsoft HoloLens 2). The course was delivered over four sessions.]]></summary></entry><entry><title type="html">[“Sprint Project on Visual Analytics for Flood Modelling funded by WDNA”]</title><link href="http://localhost:4000/news/2022/02/09/funding-updates/" rel="alternate" type="text/html" title="[“Sprint Project on Visual Analytics for Flood Modelling funded by WDNA”]" /><published>2022-02-09T00:00:00+00:00</published><updated>2022-02-09T00:00:00+00:00</updated><id>http://localhost:4000/news/2022/02/09/funding-updates</id><content type="html" xml:base="http://localhost:4000/news/2022/02/09/funding-updates/"><![CDATA[<p>We have received funding to extend our work on the development of a new decision support visual analytics toolkit, called LUCST (<a href="/projects/LUCST/">project page</a>), as a Sprint Project from the <a href="https://www.cardiff.ac.uk/__data/assets/pdf_file/0006/2520393/Wales-Data-Nation-Accelerator.pdf">Wales Data Nation Accelerator</a>. The toolkit is to help improve the planning and management of flood alleviation schemes in Gwynedd, and the funded extension is to integrate AI capabilities.</p>

<p><img src="/assets/images/LUCSTc.png" alt="The LUCST toolkit developed by Bangor University and YCG" style="margin: 0 auto; width: 500px;" /></p>

<p>This builds on work done in collaboration with the <a href="https://www.bangor.ac.uk/natural-sciences/index.php.en">School of Natural Sciences</a> and Gwynedd Council’s consultancy arm <a href="https://ygc.cymru/">Ymgynghoriaeth Gwynedd Consultancy (YGC)</a>. The work was funded by YGC and <a href="http://kess2.ac.uk/">Knowledge Economy Skills Scholarships (KESS 2)</a>, a pan-Wales higher level skills initiative led by Bangor University on behalf of the HE sector in Wales. It is part funded by the Welsh Government’s European Social Fund (ESF) convergence programme for West Wales and the Valleys.</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="infoVis" /><category term="visual analytics" /><category term="data" /><category term="AI" /><category term="flooding modelling" /><summary type="html"><![CDATA[We have received funding to extend our work on the development of a new decision support visual analytics toolkit, called LUCST (project page), as a Sprint Project from the Wales Data Nation Accelerator. The toolkit is to help improve the planning and management of flood alleviation schemes in Gwynedd, and the funded extension is to integrate AI capabilities. This builds on work done in collaboration with the School of Natural Sciences and Gwynedd Council’s consultancy arm Ymgynghoriaeth Gwynedd Consultancy (YGC). The work was funded by YGC and Knowledge Economy Skills Scholarships (KESS 2), a pan-Wales higher level skills initiative led by Bangor University on behalf of the HE sector in Wales. It is part funded by the Welsh Government’s European Social Fund (ESF) convergence programme for West Wales and the Valleys.]]></summary></entry><entry><title type="html">[“Journal paper in MDPI Computers”]</title><link href="http://localhost:4000/news/2022/02/03/publications/" rel="alternate" type="text/html" title="[“Journal paper in MDPI Computers”]" /><published>2022-02-03T00:00:00+00:00</published><updated>2022-02-03T00:00:00+00:00</updated><id>http://localhost:4000/news/2022/02/03/publications</id><content type="html" xml:base="http://localhost:4000/news/2022/02/03/publications/"><![CDATA[<p><img src="/assets/images/oneViewNotEnough_teaser.png" alt="Visualisation Data Flow for 3D" style="margin: 0 auto; width: 8S00px;" /></p>

<p>Our journal article <a href="/publications#Roberts-et-al-MDPI-2022">“One View Is Not Enough: Review of and Encouragement for Multiple and Alternative Representations in 3D and Immersive Visualisation”</a> has been accepted for publication in <a href="https://www.mdpi.com/journal/computers/special_issues/CGVC_2021"><em>MDPI Computers</em></a>, in a special issue with invited extended publications from EG CGVC 2021.</p>

<p>In this paper, using several case studies, we discuss the challenges and opportunities for using multiple views in immersive visualisation. Our aim is to provide a set of concepts that will enable developers to perform critical thinking, creative thinking and push the boundaries of what is possible with 3D and immersive visualisation.</p>

<h3 id="reference">Reference</h3>

<div class="bibliography"><p>

<span id="Roberts-et-al-MDPI-2022">J. C. Roberts, P. W. S. Butcher, and P. D. Ritsos, “One View Is Not Enough: Review of and Encouragement for Multiple and Alternative Representations in 3D and Immersive Visualisation,” <i>Computers</i>, vol. 11, no. 2, Feb. 2022.</span>

  <span class="toggle">
    The opportunities for 3D visualisations are huge. People can be immersed inside their data, interface with it in natural ways, and see it in ways that are not possible on a traditional desktop screen. Indeed, 3D visualisations, especially those that are immersed inside head-mounted displays are becoming popular. Much of this growth is driven by the availability, popularity and falling cost of head-mounted displays and other immersive technologies. However, there are also challenges. For example, data visualisation objects can be obscured, important facets missed (perhaps behind the viewer), and the interfaces may be unfamiliar. Some of these challenges are not unique to 3D immersive technologies. Indeed, developers of traditional 2D exploratory visualisation tools would use alternative views, across a multiple coordinated view (MCV) system. Coordinated view interfaces help users explore the richness of the data. For instance, an alphabetical list of people in one view shows everyone in the database, while a map view depicts where they live. Each view provides a different task or purpose. While it is possible to translate some desktop interface techniques into the 3D immersive world, it is not always clear what equivalences would be. In this paper, using several case studies, we discuss the challenges and opportunities for using multiple views in immersive visualisation. Our aim is to provide a set of concepts that will enable developers to perform critical thinking, creative thinking and push the boundaries of what is possible with 3D and immersive visualisation. In summary developers should consider how to integrate many views, techniques and presentation styles, and one view is not enough when using 3D and immersive visualisations.
  </span>
  <br />
  <span class="trigger biblinks">[<span class="abstractlink">Abstract</span>]&nbsp;&nbsp; </span>

<span class="biblinks">
  [<a href="/bibliography/Roberts-et-al-MDPI-2022/">Details</a>]&nbsp;&nbsp;
  
  
        
          [<a href="/files/Roberts-et-al-MDPI-2022.pdf" class="pdflink">PDF</a>]&nbsp;&nbsp;
         
  
   
  
  
  [doi:<a href="http://dx.doi.org/10.3390/computers11020020" class="doilink">10.3390/computers11020020</a>]&nbsp;&nbsp;
  
   
</span>
<a class="details" href="/bibliography/Roberts-et-al-MDPI-2022/"><span hidden=""></span></a></p></div>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil}</name></author><category term="news" /><category term="journal" /><category term="MDPI" /><category term="3D" /><category term="immersive visualisation" /><category term="multiple views" /><category term="alternative representation" /><category term="multivocality" /><summary type="html"><![CDATA[Our journal article “One View Is Not Enough: Review of and Encouragement for Multiple and Alternative Representations in 3D and Immersive Visualisation” has been accepted for publication in MDPI Computers, in a special issue with invited extended publications from EG CGVC 2021. In this paper, using several case studies, we discuss the challenges and opportunities for using multiple views in immersive visualisation. Our aim is to provide a set of concepts that will enable developers to perform critical thinking, creative thinking and push the boundaries of what is possible with 3D and immersive visualisation. Reference J. C. Roberts, P. W. S. Butcher, and P. D. Ritsos, “One View Is Not Enough: Review of and Encouragement for Multiple and Alternative Representations in 3D and Immersive Visualisation,” Computers, vol. 11, no. 2, Feb. 2022. The opportunities for 3D visualisations are huge. People can be immersed inside their data, interface with it in natural ways, and see it in ways that are not possible on a traditional desktop screen. Indeed, 3D visualisations, especially those that are immersed inside head-mounted displays are becoming popular. Much of this growth is driven by the availability, popularity and falling cost of head-mounted displays and other immersive technologies. However, there are also challenges. For example, data visualisation objects can be obscured, important facets missed (perhaps behind the viewer), and the interfaces may be unfamiliar. Some of these challenges are not unique to 3D immersive technologies. Indeed, developers of traditional 2D exploratory visualisation tools would use alternative views, across a multiple coordinated view (MCV) system. Coordinated view interfaces help users explore the richness of the data. For instance, an alphabetical list of people in one view shows everyone in the database, while a map view depicts where they live. Each view provides a different task or purpose. While it is possible to translate some desktop interface techniques into the 3D immersive world, it is not always clear what equivalences would be. In this paper, using several case studies, we discuss the challenges and opportunities for using multiple views in immersive visualisation. Our aim is to provide a set of concepts that will enable developers to perform critical thinking, creative thinking and push the boundaries of what is possible with 3D and immersive visualisation. In summary developers should consider how to integrate many views, techniques and presentation styles, and one view is not enough when using 3D and immersive visualisations. [Abstract]&nbsp;&nbsp; [Details]&nbsp;&nbsp; [PDF]&nbsp;&nbsp; [doi:10.3390/computers11020020]&nbsp;&nbsp;]]></summary></entry></feed>